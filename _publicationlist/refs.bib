@misc{_1511_,
  title = {[1511.03034] {{Learning}} with a {{Strong Adversary}}},
  urldate = {2022-04-29},
  howpublished = {https://arxiv.org/abs/1511.03034},
  file = {/Users/antoniohortaribeiro/Zotero/storage/859PHTM3/1511.html}
}

@misc{_anstallningsavtal_,
  title = {Anst{\"a}llningsavtal / {{Employment}} Contract: {{Ant{\^o}nio Horta Riberio}}},
  shorttitle = {Anst{\"a}llningsavtal / {{Employment}} Contract},
  journal = {Acrobat Sign},
  urldate = {2024-01-10},
  abstract = {Share with your colleagues and friends the simplicity of using Adobe Acrobat Sign to sign documents electronically. Click on the buttons below to spread the word! Learn more at {$<$}a target='\_blank' href='https://acrobat.adobe.com'{$>$}https:{\textbackslash}/{\textbackslash}/acrobat.adobe.com{$<$}/a{$>$}},
  howpublished = {https://kthsign.eu1.documents.adobe.com/public/esign?tsid=CBFCIBAACBSCTBABDUAAABACAABAAlWFtvnB7wu1XlvxJX0ziEsw7Nw\_l2I\_JkmS-Zm07tKJ0ezVLCryFV0unMwB1AEAVpeoVU8i1YhyGDsuXnGWuq\_sR90R5OQ1-AVa2\_2ssCYjwzWLLnfj1reMtHFOkcf70\&},
  langid = {british},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VYHV48IM/esign.html}
}

@misc{_antonior92_,
  title = {Antonior92/Code-Deeplearning},
  journal = {GitHub},
  urldate = {2019-12-28},
  abstract = {PyTorch deep learning projects from CODE. Contribute to antonior92/code-deeplearning development by creating an account on GitHub.},
  howpublished = {https://github.com/antonior92/code-deeplearning},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ECS7C3LA/train.html}
}

@misc{_antonior92_a,
  title = {Antonior92/Physionet-12ecg-Classification},
  journal = {GitHub},
  urldate = {2020-06-26},
  abstract = {Physionet 2020 challenge. Contribute to antonior92/physionet-12ecg-classification development by creating an account on GitHub.},
  howpublished = {https://github.com/antonior92/physionet-12ecg-classification},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9JLNL92L/physionet-12ecg-classification.html}
}

@misc{_caixa_,
  title = {Caixa de Entrada - Antonior92@gmail.Com - {{Gmail}}},
  urldate = {2020-06-28},
  howpublished = {https://mail.google.com/mail/u/0/\#inbox},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DQ2S2HWK/0.html}
}

@misc{_caixa_a,
  title = {Caixa de Entrada - Antonior92@gmail.Com - {{Gmail}}},
  urldate = {2020-06-29},
  howpublished = {https://mail.google.com/mail/u/0/\#inbox},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KPNXTXGF/0.html}
}

@misc{_changes_,
  title = {Changes},
  journal = {Google Docs},
  urldate = {2021-08-23},
  howpublished = {https://docs.google.com/document/u/0/d/1vURop52tULBv271DKzEEOFNZFr9kY1csYlWUTYI\_1OU/edit?usp=embed\_facebook},
  langid = {british},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KDAXB2PM/edit.html}
}

@misc{_cite_,
  title = {{Cit{\'e} des sciences et de l'industrie - Accueil - Expositions, conf{\'e}rences, cin{\'e}mas, activit{\'e}s culturelles et sorties touristiques pour les enfants, les parents, les familles - Paris}},
  urldate = {2023-05-28},
  abstract = {La cit{\'e} des sciences et de l'industrie est un {\'e}tablissement public de diffusion de la culture scientifique, technique et industrielle situ{\'e}e {\`a} Paris, La Villette. La Cit{\'e} propose expositions, films, conf{\'e}rences et animations pour les enfants et leurs familles.},
  howpublished = {https://www.cite-sciences.fr/fr/accueil},
  langid = {french},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QLXVBXHY/accueil.html}
}

@misc{_ctan_,
  title = {{{CTAN}}: /Tex-Archive/Macros/Latex/Contrib/Elsarticle/Doc},
  urldate = {2024-01-03},
  howpublished = {https://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle/doc},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VJYGYPM6/doc.html}
}

@misc{_fairness_,
  title = {Fairness and Machine Learning},
  urldate = {2024-06-10},
  howpublished = {https://fairmlbook.org/}
}

@misc{_fairness_a,
  title = {Fairness and Machine Learning},
  urldate = {2024-06-10},
  howpublished = {https://fairmlbook.org/}
}

@misc{_first_,
  title = {First {{M87 Event Horizon Telescope Results}}. {{III}}. {{Data Processing}} and {{Calibration}} - {{IOPscience}}},
  urldate = {2020-02-21},
  howpublished = {https://iopscience.iop.org/article/10.3847/2041-8213/ab0c57}
}

@misc{_first_a,
  title = {First {{M87 Event Horizon Telescope Results}}. {{III}}. {{Data Processing}} and {{Calibration}} - {{IOPscience}}},
  urldate = {2020-02-21},
  howpublished = {https://iopscience.iop.org/article/10.3847/2041-8213/ab0c57}
}

@misc{_folkbokforing_,
  title = {Folkbokf{\"o}ring {\textbar} {{Mina Sidor}}},
  urldate = {2024-07-29},
  howpublished = {https://sso.skatteverket.se/ms/ms\_web/page.do\#/privat/folkbokforing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JC4DYDIR/page.html}
}

@misc{_folkbokforing_a,
  title = {Folkbokf{\"o}ring {\textbar} {{Mina Sidor}}},
  urldate = {2024-07-29},
  howpublished = {https://sso.skatteverket.se/ms/ms\_web/page.do\#/privat/folkbokforing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5DXHUQ78/page.html}
}

@misc{_google_,
  title = {Google {{Agenda}} - 7 Dias, a Partir de Segunda-Feira, 25 de Maio de 2020},
  urldate = {2020-05-25},
  howpublished = {https://calendar.google.com/calendar/r},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2TJMISRU/r.html}
}

@misc{_google_a,
  title = {Google {{Calendar Hours Calculator}}},
  urldate = {2020-09-16},
  howpublished = {https://google-calendar-hours.com/},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EPV4NSAL/google-calendar-hours.com.html}
}

@misc{_hundra_,
  type = {{text}},
  title = {{Hundra m{\"o}jligheter att rekrytera utan att diskriminera}},
  urldate = {2024-06-18},
  abstract = {Du f{\aa}r v{\"a}gledning under rekryteringen med fr{\aa}gor och checklistor - 100 tips och r{\aa}d f{\"o}r att undvika fallgropar som kan leda till diskriminering.},
  howpublished = {https://www.do.se/kunskap-stod-och-vagledning/stodmaterial-forebygga-diskriminering/arbetslivet/hundra-mojligheter-att-rekrytera-utan-att-diskriminera},
  langid = {swedish},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KWM3SFWE/hundra-mojligheter-att-rekrytera-utan-att-diskriminera.html}
}

@misc{_inbox_,
  title = {Inbox (2) - Antonior92@gmail.Com - {{Gmail}}},
  urldate = {2023-10-16},
  howpublished = {https://mail.google.com/mail/u/0/\#inbox}
}

@misc{_interpolatorsunderattack_,
  title = {Interpolators-under-Attack},
  urldate = {2022-02-26},
  abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
  howpublished = {https://www.overleaf.com/project/5fabd1f02a42038b23abc59f},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SIFXMUXN/5fabd1f02a42038b23abc59f.html}
}

@misc{_jobtalk_,
  title = {{{JobTalk Uppsala Automatic Control}}},
  journal = {Google Docs},
  urldate = {2024-03-12},
  abstract = {Data-driven ECG analysis Ant{\^o}nio Horta Ribeiro On this slide: Hello my name is Antonio and I will talk about my work on data-driven analysis of the Electrocardiogram, the ECG About how my research enabled new uses of this exam. And what challenges it creates for automatic control and machine lear...},
  howpublished = {https://docs.google.com/presentation/d/1xOLtsI769Fr\_7SG-k6KVABh3vDy3ea4y7pwRiEME5j8/edit?ouid=112177853345925573315\&usp=slides\_home\&ths=true\&usp=embed\_facebook},
  langid = {british},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HFN2TE9I/edit.html}
}

@misc{_lottery_2020,
  title = {The {{Lottery Ticket Hypothesis}}: {{A Survey}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  year = {2020},
  month = jun,
  journal = {Rob's Homepage},
  urldate = {2020-06-29},
  abstract = {Metaphors are powerful tools to transfer ideas from one mind to another. Alan Kay introduced the alternative meaning of the term `desktop' at Xerox PARC in 1970. Nowadays everyone - for a glimpse of a second - has to wonder what is actually meant when referring to a desktop. Recently, Deep Learning had the pleasure to welcome a new powerful metaphor: The Lottery Ticket Hypothesis (LTH).},
  howpublished = {https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XF7ESAAF/lottery-ticket-hypothesis.html}
}

@misc{_mais_,
  title = {{Mais 1 milh{\~a}o de brasileiros passaram a trabalhar como motorista de aplicativo ou ambulante em 2018 - Economia}},
  journal = {Estad{\~a}o},
  urldate = {2020-01-23},
  abstract = {Com aumento no trabalho informal, n{\'u}mero de pessoas trabalhando com apps de transporte cresceu 30\% e nas ruas, 12\%},
  howpublished = {https://economia.estadao.com.br/noticias/geral,mais-1-milhao-de-brasileiros-passaram-a-trabalhar-como-motorista-de-aplicativo-ou-ambulante-em-2018,70003129796},
  langid = {brazilian},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BTU36966/geral,mais-1-milhao-de-brasileiros-passaram-a-trabalhar-como-motorista-de-aplicativo-ou-ambulan.html}
}

@misc{_medarbetarportalen_,
  title = {Medarbetarportalen - {{Uppsala University}}},
  urldate = {2022-11-25},
  howpublished = {https://mp.uu.se/},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VLBRBN7B/mp.uu.se.html}
}

@misc{_researcher_,
  title = {Researcher - {{Swedish Migration Agency}}},
  urldate = {2024-01-09},
  howpublished = {https://www.migrationsverket.se/formengineweb/v2/b8361dcd-7856-4b84-869d-2bfb48c6377e/complete},
  file = {/Users/antoniohortaribeiro/Zotero/storage/D35W6N83/complete.html}
}

@misc{_sponsors_,
  title = {Sponsors},
  urldate = {2019-10-28},
  howpublished = {https://www.ifac2020.org/sponsors.html},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NL2UVU3P/sponsors.html}
}

@misc{_understandingexplodinggradients_,
  title = {{understanding-exploding-gradients}},
  urldate = {2019-10-08},
  abstract = {Um editor de LaTeX online f{\'a}cil de usar. Sem instala{\c c}{\~a}o, colabora{\c c}{\~a}o em tempo real, controle de vers{\~o}es, centenas de templates LaTeX e mais.},
  howpublished = {https://pt.overleaf.com/project/5cbed10170921e14664561f1},
  langid = {portuguese},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8VRJKKP2/5cbed10170921e14664561f1.html}
}

@misc{_use_,
  title = {{{THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS}} - {{FISHER}} - 1936 - {{Annals}} of {{Eugenics}} - {{Wiley Online Library}}},
  urldate = {2024-05-21},
  howpublished = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x}
}

@article{10.1001/jamacardio.2017.3180,
  title = {Incidence of Previously Undiagnosed Atrial Fibrillation Using Insertable Cardiac Monitors in a High-Risk Population: {{The REVEAL AF}} Study},
  author = {Reiffel, James A. and Verma, Atul and Kowey, Peter R. and Halperin, Jonathan L. and Gersh, Bernard J. and Wachter, Rolf and Pouliot, Erika and Ziegler, Paul D. and Investigators, for the REVEAL AF},
  year = {2017},
  month = oct,
  journal = {JAMA Cardiology},
  volume = {2},
  number = {10},
  eprint = {https://jamanetwork.com/journals/jamacardiology/articlepdf/2650790/jamacardiology{\textbackslash}\_reiffel{\textbackslash}\_2017{\textbackslash}\_oi{\textbackslash}\_170047.pdf},
  pages = {1120--1127},
  issn = {2380-6583},
  doi = {10.1001/jamacardio.2017.3180},
  abstract = {In approximately 20 \% of atrial fibrillation (AF)--related ischemic strokes, stroke is the first clinical manifestation of AF. Strategies are needed to identify and therapeutically address previously undetected AF.To quantify the incidence of AF in patients at high risk for but without previously known AF using an insertable cardiac monitor.This prospective, single-arm, multicenter study was conducted from November 2012 to January 2017. Visits took place at 57 centers in the United States and Europe. Patients with a CHADS2 score of 3 or greater (or 2 with at least 1 additional risk factor) were enrolled. Approximately 90 \% had nonspecific symptoms potentially compatible with AF, such as fatigue, dyspnea, and/or palpitations.Patients underwent monitoring with an insertable cardiac monitor for 18 to 30 months.The primary end point was adjudicated AF lasting 6 or more minutes and was assessed at 18 months. Other analyses included detection rates at points from 30 days to 30 months and among CHADS2 score subgroups. Median time from insertion to detection and the percentage of patients subsequently prescribed oral anticoagulation therapy was also determined.A total of 446 patients were enrolled; 233 (52.2 \%) were male, and the mean (SD) age was 71.5 (9.9) years. A total of 385 patients (86.3 \%) received an insertable cardiac monitor, met the primary analysis cohort definition, and were observed for a mean (SD) period of 22.5 (7.7) months. The detection rate of AF lasting 6 or more minutes at 18 months was 29.3 \%. Detection rates at 30 days and 6, 12, 24, and 30 months were 6.2 \%, 20.4 \%, 27.1 \%, 33.6 \%, and 40.0 \%, respectively. At 18 months, AF incidence was similar among patients with CHADS2 scores of 2 (24.7 \%; 95 \% CI, 17.3-31.4), 3 (32.7 \%; 95 \% CI, 23.8-40.7), and 4 or greater (31.7 \%; 95 \% CI, 22.0-40.3) (P = .23). Median (interquartile) time from device insertion to first AF episode detection was 123 (41-330) days. Of patients meeting the primary end point, 13 (10.2 \%) had 1 or more episodes lasting 24 hours or longer, and oral anticoagulation therapy was prescribed for 72 patients (56.3 \%).The incidence of previously undiagnosed AF may be substantial in patients with risk factors for AF and stroke. Atrial fibrillation would have gone undetected in most patients had monitoring been limited to 30 days. Further trials regarding the value of detecting subclinical AF and of prophylactic therapies are warranted.clinicaltrials.gov Identifier: NCT01727297}
}

@inproceedings{10.5555/3524938.3525611,
  title = {In Defense of Uniform Convergence: {{Generalization}} via Derandomization with an Application to Interpolating Predictors},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Negrea, Jeffrey and Dziugaite, Gintare Karolina and Roy, Daniel M.},
  year = {2020},
  series = {{{ICML}}'20},
  publisher = {JMLR.org},
  abstract = {We propose to study the generalization error of a learned predictor undefined in terms of that of a surrogate (potentially randomized) predictor that is coupled to undefined and designed to trade empirical risk for control of generalization error. In the case where undefined interpolates the data, it is interesting to consider theoretical surrogate predictors that are partially derandomized or rerandomized, e.g., fit to the training data but with modified label noise. We also show that replacing undefined by its conditional distribution with respect to an arbitrary {$\sigma$}-field is a convenient way to derandomize. We study two examples, inspired by the work of Nagarajan and Kolter (2019) and Bartlett et al. (2020), where the learned predictor undefined interpolates the training data with high probability, has small risk, and, yet, does not belong to a nonrandom class with a tight uniform bound on two-sided generalization error. At the same time, we bound the risk of undefined in terms of surrogates constructed by conditioning and denoising, respectively, and shown to belong to nonrandom classes with uniformly small generalization error.},
  articleno = {673},
  file = {/Users/antoniohortaribeiro/Zotero/storage/R9P38I9D/Negrea et al. - 2020 - In defense of uniform convergence Generalization .pdf}
}

@incollection{aastrom_numerical_1966,
  title = {Numerical Identification of Linear Dynamic Systems from Normal Operating Records},
  booktitle = {{{PH Hammond}}: {{Theory}} of {{Self-Adaptive Control Systems}}},
  author = {{\textbackslash}AAstr{\"o}m, Karl Johan and Bohlin, Torsten},
  year = {1966},
  pages = {96--111},
  publisher = {Plenum Press}
}

@incollection{aastrom_numerical_1966a,
  title = {Numerical {{Identification}} of {{Linear Dynamic Systems}} from {{Normal Operating Records}}},
  booktitle = {{{PH Hammond}}: {{Theory}} of {{Self-Adaptive Control Systems}}},
  author = {{\textbackslash}AAstr{\"o}m, Karl Johan and Bohlin, Torsten},
  year = {1966},
  pages = {96--111},
  publisher = {Plenum Press}
}

@techreport{abadi_tensorflow_2015,
  title = {{{TensorFlow}}: {{Large-Scale Machine Learning}} on {{Heterogeneous Systems}}},
  author = {Abadi, Mar{\'t}{\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015},
  keywords = {â›” No DOI found}
}

@article{abdalmoaty_application_2018,
  title = {Application of a {{Linear PEM Estimator}} to a {{Stochastic Wiener-Hammerstein Benchmark Problem}}},
  author = {Abdalmoaty, Mohamed Rasheed and Hjalmarsson, H{\aa}kan},
  year = {2018},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {18th {{IFAC Symposium}} on {{System Identification SYSID}} 2018},
  volume = {51},
  number = {15},
  pages = {784--789},
  issn = {2405-8963},
  doi = {10/gfkd8g},
  urldate = {2018-11-26},
  abstract = {The estimation problem of stochastic Wiener-Hammerstein models is recognized to be challenging, mainly due to the analytical intractability of the likelihood function. In this contribution, we apply a computationally attractive prediction error method estimator to a real-data stochastic Wiener-Hammerstein benchmark problem. The estimator is defined using a deterministic predictor that is nonlinear in the input. The prediction error method results in tractable expressions, and Monte Carlo approximations are not necessary. This allows us to tackle several issues considered challenging from the perspective of the current mainstream approach. Under mild conditions, the estimator can be shown to be consistent and asymptotically normal. The results of the method applied to the benchmark data are presented and discussed.},
  keywords = {Benchmark problem,Nonlinear systems,Stochastic systems,System identification,Wiener-Hammerstein},
  file = {/Users/antoniohortaribeiro/Zotero/storage/LAGK9FXP/abdalmoaty_applicatio_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/E5WQHYW7/S2405896318317968.html}
}

@article{abdulhai_reinforcement_2003,
  title = {Reinforcement Learning: {{Introduction}} to Theory and Potential for Transport Applications},
  shorttitle = {Reinforcement Learning},
  author = {Abdulhai, Baher and Kattan, Lina},
  year = {2003},
  journal = {Canadian Journal of Civil Engineering},
  volume = {30},
  number = {6},
  pages = {981--991},
  doi = {10.1139/l03-014}
}

@article{acharya_application_2017,
  title = {Application of Deep Convolutional Neural Network for Automated Detection of Myocardial Infarction Using {{ECG}} Signals},
  author = {Acharya, U. Rajendra and Fujita, Hamido and Oh, Shu Lih and Hagiwara, Yuki and Tan, Jen Hong and Adam, Muhammad},
  year = {2017},
  month = nov,
  journal = {Information Sciences},
  volume = {415--416},
  pages = {190--198},
  issn = {00200255},
  doi = {10.1016/j.ins.2017.06.027},
  urldate = {2018-10-21},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/K4L7ZJIA/acharya_applicatio_2017.pdf}
}

@article{acharya_deep_2017,
  title = {A Deep Convolutional Neural Network Model to Classify Heartbeats},
  author = {Acharya, U. Rajendra and Oh, Shu Lih and Hagiwara, Yuki and Tan, Jen Hong and Adam, Muhammad and Gertych, Arkadiusz and Tan, Ru San},
  year = {2017},
  month = oct,
  journal = {Computers in Biology and Medicine},
  volume = {89},
  pages = {389--396},
  issn = {00104825},
  doi = {10.1016/j.compbiomed.2017.08.022},
  urldate = {2018-10-21},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/E5FJIC4W/acharya_a deep_2017.pdf}
}

@article{achille_critical_2017,
  title = {Critical {{Learning Periods}} in {{Deep Neural Networks}}},
  author = {Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.08856 [cs, q-bio, stat]},
  eprint = {1711.08856},
  primaryclass = {cs, q-bio, stat},
  urldate = {2019-06-01},
  abstract = {Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training. To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training. Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of "Information Plasticity". Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6HMGHAQ2/achille_critical_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/NYEB3LAG/1711.html}
}

@article{adiwardana_humanlike_2020,
  title = {Towards a {{Human-like Open-Domain Chatbot}}},
  author = {Adiwardana, Daniel and Luong, Minh-Thang and So, David R. and Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang, Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and Lu, Yifeng and Le, Quoc V.},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.09977 [cs, stat]},
  eprint = {2001.09977},
  primaryclass = {cs, stat},
  urldate = {2020-02-06},
  abstract = {We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72\% on multi-turn evaluation) suggests that a human-level SSA of 86\% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79\% SSA, 23\% higher in absolute SSA than the existing chatbots we evaluated.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8ZUKV8M2/Adiwardana et al. - 2020 - Towards a Human-like Open-Domain Chatbot.pdf;/Users/antoniohortaribeiro/Zotero/storage/3748DSUG/2001.html}
}

@article{adlam_neural_2020,
  title = {The {{Neural Tangent Kernel}} in {{High Dimensions}}: {{Triple Descent}} and a {{Multi-Scale Theory}} of {{Generalization}}},
  author = {Adlam, Ben and Pennington, Jeffrey},
  year = {2020},
  journal = {Proceedings of the 37 th International Conference on Machine Learning, PMLR 119},
  abstract = {Modern deep learning models employ considerably more parameters than required to fit the training data. Whereas conventional statistical wisdom suggests such models should drastically overfit, in practice these models generalize remarkably well. An emerging paradigm for describing this unexpected behavior is in terms of a double descent curve, in which increasing a model's capacity causes its test error to first decrease, then increase to a maximum near the interpolation threshold, and then decrease again in the overparameterized regime. Recent efforts to explain this phenomenon theoretically have focused on simple settings, such as linear regression or kernel regression with unstructured random features, which we argue are too coarse to reveal important nuances of actual neural networks. We provide a precise high-dimensional asymptotic analysis of generalization under kernel regression with the Neural Tangent Kernel, which characterizes the behavior of wide neural networks optimized with gradient descent. Our results reveal that the test error has non-monotonic behavior deep in the overparameterized regime and can even exhibit additional peaks and descents when the number of parameters scales quadratically with the dataset size.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2CKV499X/Adlam and Pennington - The Neural Tangent Kernel in High Dimensions Trip.pdf;/Users/antoniohortaribeiro/Zotero/storage/Y7HBXXI2/notes.pdf}
}

@article{adlam_random_2019,
  title = {A {{Random Matrix Perspective}} on {{Mixtures}} of {{Nonlinearities}} for {{Deep Learning}}},
  author = {Adlam, Ben and Levinson, Jake and Pennington, Jeffrey},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.00827 [cs, stat]},
  eprint = {1912.00827},
  primaryclass = {cs, stat},
  urldate = {2021-06-28},
  abstract = {One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features \$F=f(WX+B)\$ for a random weight matrix \$W\$ and random bias vector \$B\$, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis directly generalizes to such distributions, even those not expressible with a traditional additive bias. Intriguingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UUC75E8E/Adlam et al_2019_A Random Matrix Perspective on Mixtures of Nonlinearities for Deep Learning.pdf;/Users/antoniohortaribeiro/Zotero/storage/9FDE9LRV/1912.html}
}

@article{advani_highdimensional_2017,
  title = {High-Dimensional Dynamics of Generalization Error in Neural Networks},
  author = {Advani, Madhu S. and Saxe, Andrew M.},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.03667 [physics, q-bio, stat]},
  eprint = {1710.03667},
  primaryclass = {physics, q-bio, stat},
  urldate = {2020-11-26},
  abstract = {We perform an average case analysis of the generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant "high-dimensional" regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that naive application of worst-case theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
  archiveprefix = {arXiv},
  keywords = {{Physics - Data Analysis, Statistics and Probability},Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AR3NLGAL/1710.html}
}

@article{advani_highdimensional_2020,
  title = {High-Dimensional Dynamics of Generalization Error in Neural Networks},
  author = {Advani, Madhu S. and Saxe, Andrew M. and Sompolinsky, Haim},
  year = {2020},
  month = dec,
  journal = {Neural Networks},
  volume = {132},
  pages = {428--446},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2020.08.022},
  urldate = {2021-05-28},
  abstract = {We perform an analysis of the average generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant ``high-dimensional'' regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that standard application of theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
  keywords = {Generalization error,Neural networks,Random matrix theory},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SNEWRTRE/Advani et al. - 2020 - High-dimensional dynamics of generalization error .pdf;/Users/antoniohortaribeiro/Zotero/storage/TJNC7T7H/Advani and Saxe - 2017 - High-dimensional dynamics of generalization error .pdf;/Users/antoniohortaribeiro/Zotero/storage/95HD82LD/S0893608020303117.html}
}

@inproceedings{agrawal_differentiable_2019,
  title = {Differentiable {{Convex Optimization Layers}}},
  booktitle = {33rd {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen},
  year = {2019},
  pages = {19},
  abstract = {Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difficult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-specific languages (DSLs) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver's solution to a solution of the original problem (a new form we refer to as affine-solver-affine form). We then demonstrate how to efficiently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex optimization, and additionally implement differentiable layers for disciplined convex programs in PyTorch and TensorFlow 2.0. Our implementation significantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EM5XZ2R4/Agrawal et al. - Differentiable Convex Optimization Layers.pdf;/Users/antoniohortaribeiro/Zotero/storage/JPKKVUAG/fairmlbook.pdf}
}

@article{aguirre_bird_2019,
  title = {A {{Bird}}'s {{Eye View}} of {{Nonlinear System Identification}}},
  author = {Aguirre, Luis Antonio},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.06803 [cs, eess]},
  eprint = {1907.06803},
  primaryclass = {cs, eess},
  urldate = {2020-02-13},
  abstract = {This text aims at providing a bird's eye view of system identification with special attention to nonlinear systems. The driving force is to give a feeling for the philosophical problems facing those that build mathematical models from data. Special attention will be given to grey-box approaches in nonlinear system identification. In this text, grey-box methods use auxiliary information such as the system steady-state data, possible symmetries, some bifurcations and the presence of hysteresis. The text ends with a sample of applications. No attempt is made to be thorough nor to survey such an extensive and mature field as system identification. In most parts references will be provided for a more detailed study.},
  archiveprefix = {arXiv},
  keywords = {93-02,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XHEER4Z7/Aguirre - 2019 - A Bird's Eye View of Nonlinear System Identificati.pdf;/Users/antoniohortaribeiro/Zotero/storage/99JT92IT/1907.html}
}

@article{aguirre_development_2017,
  title = {Development of Soft Sensors for Permanent Downhole {{Gauges}} in Deepwater Oil Wells},
  author = {Aguirre, Luis A. and Teixeira, Bruno O.S. and Barbosa, Bruno H.G. and Teixeira, Alex F. and Campos, Mario C.M.M. and Mendes, Eduardo M.A.M.},
  year = {2017},
  month = aug,
  journal = {Control Engineering Practice},
  volume = {65},
  pages = {83--99},
  issn = {09670661},
  doi = {10.1016/j.conengprac.2017.06.002},
  urldate = {2020-02-17},
  abstract = {Downhole pressure is an important process variable in the operation of gas-lifted oil wells. The device installed in order to measure this variable is often called a Permanent Downhole Gauge (PDG). Replacing a faulty PDG is often not economically viable and to have an alternative estimate of the downhole pressure is an important goal. Using data from operating PDGs, this paper describes a number of issues dealt with in the development of soft sensors for several deepwater gas-lifted oil wells. Some of the tested models include nonlinear polynomials, neural networks, committee machines, unscented Kalman filters and filter banks. The variety of model classes used in addition to the diversity of oil wells considered brings to light some of the key-problems that have to be faced and reveal the strengths and weaknesses of each alternative solution. A major constraint throughout the work was the use of historical data, hence no specific tests were performed at any time. The aim of this work is to discuss the procedures, pros and cons of the tested solutions and to point to possible future directions of research.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/C8UGJ7RT/Aguirre et al. - 2017 - Development of soft sensors for permanent downhole.pdf}
}

@article{aguirre_dynamical_1995,
  title = {Dynamical Effects of Overparametrization in Nonlinear Models},
  author = {Aguirre, Luis Antonio and Billings, Stephen A},
  year = {1995},
  journal = {Physica D: Nonlinear Phenomena},
  volume = {80},
  number = {1-2},
  pages = {26--40},
  doi = {10.1016/0167-2789(95)90053-5}
}

@book{aguirre_introducao_2004,
  title = {Introdu{\c c}{\~a}o {\`a} Identifica{\c c}{\~a}o de Sistemas--{{T{\'e}cnicas}} Lineares e N{\~a}o-Lineares Aplicadas a Sistemas Reais},
  author = {Aguirre, Luis Antonio},
  year = {2004},
  publisher = {Editora UFMG},
  annotation = {00654}
}

@article{aguirre_prediction_2010,
  title = {Prediction and Simulation Errors in Parameter Estimation for Nonlinear Systems},
  author = {Aguirre, Luis A and Barbosa, Bruno HG and Braga, Ant{\^o}nio P},
  year = {2010},
  journal = {Mechanical Systems and Signal Processing},
  volume = {24},
  number = {8},
  pages = {2855--2867},
  doi = {10.1016/j.ymssp.2010.05.003},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7KIL5JI7/aguirre_prediction_2010.pdf}
}

@book{ahlfors_complex_1966,
  title = {Complex {{Analysis}}},
  author = {Ahlfors, L.V.},
  year = {1966},
  file = {/Users/antoniohortaribeiro/Zotero/storage/T8RKD3VI/ahlfors_complex_1966.pdf}
}

@article{aizenberg_multilayer_2016,
  title = {Multilayer {{Neural Network}} with {{Multi-Valued Neurons}} in Time Series Forecasting of Oil Production},
  author = {Aizenberg, Igor and Sheremetov, Leonid and {Villa-Vargas}, Luis and {Martinez-Mu{\~n}oz}, Jorge},
  year = {2016},
  journal = {Neurocomputing},
  volume = {175},
  pages = {980--989},
  doi = {10.1016/j.neucom.2015.06.092}
}

@article{akaike_new_1974,
  title = {A New Look at the Statistical Model Identification},
  author = {Akaike, H},
  year = {1974},
  month = dec,
  journal = {IEEE Transactions on Automatic Control},
  volume = {19},
  number = {6},
  pages = {716--723},
  issn = {0018-9286},
  doi = {10.1109/TAC.1974.1100705},
  keywords = {Art,Estimation theory,History,Linear systems,Maximum likelihood estimation,maximum-likelihood (ML) estimation,Parameter identification,Roundoff errors,Sampling methods,Stochastic processes,Testing,Time series,Time series analysis}
}

@article{akiyama_first_2019,
  title = {First {{M87}} Event Horizon Telescope Results. Iii. Data Processing and Calibration},
  author = {Akiyama, Kazunori and Alberdi, Antxon and Alef, Walter and Asada, Keiichi and Azulay, Rebecca and Baczko, Anne-Kathrin and Ball, David and Balokovi{\'c}, Mislav and Barrett, John and Bintley, Dan},
  year = {2019},
  journal = {The Astrophysical Journal Letters},
  volume = {875},
  number = {1},
  pages = {L3},
  issn = {2041-8205}
}

@misc{alammar_illustrated_,
  title = {The {{Illustrated BERT}}, {{ELMo}}, and Co. ({{How NLP Cracked Transfer Learning}})},
  author = {Alammar, Jay},
  urldate = {2019-06-08},
  abstract = {Discussions: Hacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments) Translations: Chinese (Simplified), Persian The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It's been referred to as NLP's ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).},
  howpublished = {http://jalammar.github.io/illustrated-bert/},
  file = {/Users/antoniohortaribeiro/Zotero/storage/STAUQRWW/illustrated-bert.html}
}

@misc{alammar_illustrated_a,
  title = {The {{Illustrated Transformer}}},
  author = {Alammar, Jay},
  urldate = {2019-06-10},
  abstract = {Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations: Chinese (Simplified), Korean Watch: MIT's Deep Learning State of the Art lecture referencing this post In the previous post, we looked at Attention -- a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer -- a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud's recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let's try to break the model apart and look at how it functions. The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard's NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter. A High-Level Look Let's begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
  howpublished = {http://jalammar.github.io/illustrated-transformer/},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FSQUH6G2/illustrated-transformer.html}
}

@misc{alammar_illustrated_b,
  title = {The {{Illustrated Transformer}}},
  author = {Alammar, Jay},
  urldate = {2019-06-26},
  abstract = {Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations: Chinese (Simplified), Korean Watch: MIT's Deep Learning State of the Art lecture referencing this post In the previous post, we looked at Attention -- a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer -- a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud's recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let's try to break the model apart and look at how it functions. The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard's NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter. A High-Level Look Let's begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
  howpublished = {http://jalammar.github.io/illustrated-transformer/},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MPLBRN5B/illustrated-transformer.html}
}

@article{alday_classification_2020,
  title = {Classification of 12-Lead {{ECGs}}: The {{PhysioNet}}/{{Computing}} in {{Cardiology Challenge}} 2020},
  shorttitle = {Classification of 12-Lead {{ECGs}}},
  author = {Alday, Erick A. Perez and Gu, Annie and Shah, Amit and Robichaux, Chad and Wong, An-Kwok Ian and Liu, Chengyu and Liu, Feifei and Rad, Ali Bahrami and Elola, Andoni and Seyedi, Salman and Li, Qiao and Sharma, Ashish and Clifford, Gari D. and Reyna, Matthew A.},
  year = {2020},
  month = aug,
  journal = {medRxiv},
  pages = {2020.08.11.20172601},
  publisher = {Cold Spring Harbor Laboratory Press},
  doi = {10.1101/2020.08.11.20172601},
  urldate = {2020-09-01},
  abstract = {{$<$}p{$>$}The subject of the PhysioNet/Computing in Cardiology Challenge 2020 was the identification of cardiac abnormalities in 12-lead electrocardiogram (ECG) recordings. A total of 66,405 recordings were sourced from hospital systems from four distinct countries and annotated with clinical diagnoses, including 43,101 annotated recordings that were posted publicly. For this Challenge, we asked participants to design working, open-source algorithms for identifying cardiac abnormalities in 12-lead ECG recordings. This Challenge provided several innovations. First, we sourced data from multiple institutions from around the world with different demographics, allowing us to assess the generalizability of the algorithms. Second, we required participants to submit both their trained models and the code for reproducing their trained models from the training data, which aids the generalizability and reproducibility of the algorithms. Third, we proposed a novel evaluation metric that considers different misclassification errors for different cardiac abnormalities, reflecting the clinical reality that some diagnoses have similar outcomes and varying risks. Over 200 teams submitted 850 algorithms (432 of which successfully ran) during the unofficial and official phases of the Challenge, representing a diversity of approaches from both academia and industry for identifying cardiac abnormalities. The official phase of the Challenge is ongoing.{$<$}/p{$>$}},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VHXH3CIK/Alday et al. - 2020 - Classification of 12-lead ECGs the PhysioNetComp.pdf;/Users/antoniohortaribeiro/Zotero/storage/GLMNZC7Z/2020.08.11.html}
}

@inproceedings{alemohammad_recurrent_2021,
  title = {The Recurrent Neural Tangent Kernel},
  booktitle = {International Conference on Learning Representations},
  author = {Alemohammad, Sina and Wang, Zichao and Balestriero, Randall and Baraniuk, Richard},
  year = {2021},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5EZ68PUI/Alemohammad et al. - 2021 - The Recurrent Neural Tangent Kernel.pdf}
}

@article{alkmim_improving_2012,
  title = {Improving Patient Access to Specialized Health Care: The {{Telehealth Network}} of {{Minas Gerais}}, {{Brazil}}},
  author = {Alkmim, Maria Beatriz and Figueira, Renato Minelli and Marcolino, Milena Soriano and Cardoso, Clareci Silva and {Pena de Abreu}, Monica and Cunha, Lemuel Rodrigues and {da Cunha}, Daniel Ferreira and Antunes, Andre Pires and Resende, Ad{\'e}lson Geraldo de A and Resende, Elmiro Santos and Ribeiro, Antonio Luiz Pinho},
  year = {2012},
  month = may,
  journal = {Bulletin of the World Health Organization},
  volume = {90},
  number = {5},
  eprint = {22589571},
  eprinttype = {pubmed},
  pages = {373--378},
  issn = {1564-0604},
  doi = {10/f3x7px},
  abstract = {PROBLEM: The Brazilian population lacks equitable access to specialized health care and diagnostic tests, especially in remote municipalities, where health professionals often feel isolated and staff turnover is high. Telehealth has the potential to improve patients' access to specialized health care, but little is known about it in terms of cost-effectiveness, access to services or user satisfaction. APPROACH: In 2005, the State Government of Minas Gerais, Brazil, funded the establishment of the Telehealth Network, intended to connect university hospitals with the state's remote municipal health departments; support professionals in providing tele-assistance; and perform tele-electrocardiography and teleconsultations. The network uses low-cost equipment and has employed various strategies to overcome the barriers to telehealth use. LOCAL SETTING: The Telehealth Network connects specialists in state university hospitals with primary health-care professionals in 608 municipalities of the large state of Minas Gerais, many of them in remote areas. RELEVANT CHANGES: From June 2006 to October 2011, 782,773 electrocardiograms and 30 883 teleconsultations were performed through the network, and 6000 health professionals were trained in its use. Most of these professionals (97\%) were satisfied with the system, which was cost-effective, economically viable and averted 81\% of potential case referrals to distant centres. LESSONS LEARNT: To succeed, a telehealth service must be part of a collaborative network, meet the real needs of local health professionals, use simple technology and have at least some face-to-face components. If applied to health problems for which care is in high demand, this type of service can be economically viable and can help to improve patient access to specialized health care.}
}

@article{allen_assessing_1996,
  title = {Assessing {{ECG}} Signal Quality on a Coronary Care Unit},
  author = {Allen, John and Murray, Alan},
  year = {1996},
  journal = {Physiological measurement},
  volume = {17},
  number = {4},
  pages = {249},
  doi = {10.1088/0967-3334/17/4/002},
  file = {/Users/antoniohortaribeiro/Zotero/storage/H4JXDGNM/allen_assessing_1996.pdf}
}

@article{allen-zhu_convergence_2019,
  title = {A {{Convergence Theory}} for {{Deep Learning}} via {{Over-Parameterization}}},
  author = {{Allen-Zhu}, Zeyuan and Li, Yuanzhi and Song, Zhao},
  year = {2019},
  journal = {Proceedings of the 36 th International Conference on Machine Learning, PMLR},
  volume = {97},
  eprint = {1811.03962},
  abstract = {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works has been focusing on training neural networks with one hidden layer. The theory of multi-layer networks remains largely unsettled. In this work, we prove why stochastic gradient descent (SGD) can find \${\textbackslash}textit\{global minima\}\$ on the training objective of DNNs in \${\textbackslash}textit\{polynomial time\}\$. We only make two assumptions: the inputs are non-degenerate and the network is over-parameterized. The latter means the network width is sufficiently large: \${\textbackslash}textit\{polynomial\}\$ in \$L\$, the number of layers and in \$n\$, the number of samples. Our key technique is to derive that, in a sufficiently large neighborhood of the random initialization, the optimization landscape is almost-convex and semi-smooth even with ReLU activations. This implies an equivalence between over-parameterized neural networks and neural tangent kernel (NTK) in the finite (and polynomial) width setting. As concrete examples, starting from randomly initialized weights, we prove that SGD can attain 100\% training accuracy in classification tasks, or minimize regression loss in linear convergence speed, with running time polynomial in \$n,L\$. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2IREYMXE/Allen-Zhu et al. - 2019 - A Convergence Theory for Deep Learning via Over-Pa.pdf;/Users/antoniohortaribeiro/Zotero/storage/HQUVDKSB/1811.html}
}

@article{alonso_simple_2013,
  title = {Simple {{Risk Model Predicts Incidence}} of {{Atrial Fibrillation}} in a {{Racially}} and {{Geographically Diverse Population}}: The {{CHARGE AF Consortium}}},
  shorttitle = {Simple {{Risk Model Predicts Incidence}} of {{Atrial Fibrillation}} in a {{Racially}} and {{Geographically Diverse Population}}},
  author = {Alonso, Alvaro and Krijthe, Bouwe P. and Aspelund, Thor and Stepas, Katherine A. and Pencina, Michael J. and Moser, Carlee B. and Sinner, Moritz F. and Sotoodehnia, Nona and Fontes, Jo{\~a}o D. and Janssens, A. Cecile J. W. and Kronmal, Richard A. and Magnani, Jared W. and Witteman, Jacqueline C. and Chamberlain, Alanna M. and Lubitz, Steven A. and Schnabel, Renate B. and Agarwal, Sunil K. and McManus, David D. and Ellinor, Patrick T. and Larson, Martin G. and Burke, Gregory L. and Launer, Lenore J. and Hofman, Albert and Levy, Daniel and Gottdiener, John S. and K{\"a}{\"a}b, Stefan and Couper, David and Harris, Tamara B. and Soliman, Elsayed Z. and Stricker, Bruno H. C. and Gudnason, Vilmundur and Heckbert, Susan R. and Benjamin, Emelia J.},
  year = {2013},
  journal = {Journal of the American Heart Association},
  volume = {2},
  number = {2},
  pages = {e000102},
  publisher = {Wiley},
  doi = {10.1161/JAHA.112.000102},
  urldate = {2024-06-10},
  abstract = {BackgroundTools for the prediction of atrial fibrillation (AF) may identify high-risk individuals more likely to benefit from preventive interventions and serve as a benchmark to test novel putative risk factors.Methods and ResultsIndividual-level data from 3 large cohorts in the United States (Atherosclerosis Risk in Communities [ARIC] study, the Cardiovascular Health Study [CHS], and the Framingham Heart Study [FHS]), including 18~556 men and women aged 46 to 94~years (19\% African Americans, 81\% whites) were pooled to derive predictive models for AF using clinical variables. Validation of the derived models was performed in 7672 participants from the Age, Gene and Environment---Reykjavik study (AGES) and the Rotterdam Study (RS). The analysis included 1186 incident AF cases in the derivation cohorts and 585 in the validation cohorts. A simple 5-year predictive model including the variables age, race, height, weight, systolic and diastolic blood pressure, current smoking, use of antihypertensive medication, diabetes, and history of myocardial infarction and heart failure had good discrimination (C-statistic, 0.765; 95\% CI, 0.748 to 0.781). Addition of variables from the electrocardiogram did not improve the overall model discrimination (C-statistic, 0.767; 95\% CI, 0.750 to 0.783; categorical net reclassification improvement, -0.0032; 95\% CI, -0.0178 to 0.0113). In the validation cohorts, discrimination was acceptable (AGES C-statistic, 0.664; 95\% CI, 0.632 to 0.697 and RS C-statistic, 0.705; 95\% CI, 0.664 to 0.747) and calibration was adequate.ConclusionA risk model including variables readily available in primary care settings adequately predicted AF in diverse populations from the United States and Europe.},
  keywords = {atrial fibrillation,epidemiology,risk factors},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GFSTYEBX/Alonso et al. - Simple Risk Model Predicts Incidence of Atrial Fib.pdf}
}

@article{amorim_mais_2019,
  title = {{Mais 1 milh{\~a}o de brasileiros passaram a trabalhar como motorista de aplicativo ou ambulante em 2018}},
  author = {Amorim, Daniela},
  year = {2019},
  month = dec,
  journal = {O Estado de S. Paulo},
  urldate = {2020-01-28},
  abstract = {Com aumento no trabalho informal, n{\'u}mero de pessoas trabalhando com apps de transporte cresceu 30\% e nas ruas, 12\%},
  langid = {brazilian},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ANF6BX4W/geral,mais-1-milhao-de-brasileiros-passaram-a-trabalhar-como-motorista-de-aplicativo-ou-ambulan.html}
}

@article{anand_risk_2021,
  title = {Risk {{Assessment}} of {{Stealthy Attacks}} on {{Uncertain Control Systems}}},
  author = {Anand, Sribalaji C. and Teixeira, Andr{\'e} M. H. and Ahl{\'e}n, Anders},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.07071 [cs, eess, math]},
  eprint = {2106.07071},
  primaryclass = {cs, eess, math},
  urldate = {2021-11-16},
  abstract = {In this article, we address the problem of risk assessment of stealthy attacks on uncertain control systems. Considering data injection attacks that aim at maximizing impact while remaining undetected, we use the recently proposed output-to-output gain to characterize the risk associated with the impact of attacks in two setups: A full system knowledge attacker and a limited system knowledge attacker. The risk in each setup is formulated using a well-established risk metric, namely the Value-at-Risk and the maximum expected loss, respectively. Under these setups, the risk assessment problem corresponds to an untractable infinite non-convex optimization problem. To address this limitation, we adopt the framework of scenario-based optimization to approximate the infinite non-convex optimization problem by a sampled non-convex optimization problem. Then, based on the framework of dissipative system theory and S-procedure, the sampled non-convex risk assessment problem is formulated as an equivalent convex semi-definite program. Additionally, we derive the necessary and sufficient conditions for the risk to be bounded. Finally, we illustrate the results through numerical simulation of a hydro-turbine power system.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BM56F7IM/Anand et al_2021_Risk Assessment of Stealthy Attacks on Uncertain Control Systems.pdf;/Users/antoniohortaribeiro/Zotero/storage/45UH9N6M/2106.html}
}

@book{anderson_introduction_2009,
  title = {An {{Introduction}} to {{Random Matrices}}},
  author = {Anderson, Greg W. and Guionnet, Alice and Zeitouni, Ofer},
  year = {2009},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KWDHNLYF/cupbook.pdf}
}

@article{andersson_deep_2019,
  title = {Deep {{Convolutional Networks}} in {{System Identification}}},
  author = {Andersson, Carl and Ribeiro, Ant{\^o}nio H. and Tiels, Koen and Wahlstr{\"o}m, Niklas and Sch{\"o}n, Thomas B.},
  year = {2019},
  month = sep,
  journal = {IEEE Conference on Decision and Control (CDC)},
  eprint = {1909.01730},
  pages = {3670--3676},
  doi = {10.1109/CDC40024.2019.9030219},
  abstract = {Recent developments within deep learning are relevant for nonlinear system identification problems. In this paper, we establish connections between the deep learning and the system identification communities. It has recently been shown that convolutional architectures are at least as capable as recurrent architectures when it comes to sequence modeling tasks. Inspired by these results we explore the explicit relationships between the recently proposed temporal convolutional network (TCN) and two classic system identification model structures; Volterra series and block-oriented models. We end the paper with an experimental study where we provide results on two real-world problems, the well-known Silverbox dataset and a newer dataset originating from ground vibration experiments on an F-16 fighter aircraft.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BIQCGKR5/andersson_deep_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/IJQDKH4X/Andersson et al. - 2019 - Deep Convolutional Networks in System Identificati.pdf;/Users/antoniohortaribeiro/Zotero/storage/QHGR7SZU/1909.html}
}

@article{andrieu_particle_2010,
  title = {Particle {{Markov}} Chain {{Monte Carlo}} Methods: {{Particle Markov Chain Monte Carlo Methods}}},
  shorttitle = {Particle {{Markov}} Chain {{Monte Carlo}} Methods},
  author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
  year = {2010},
  month = jun,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {72},
  number = {3},
  pages = {269--342},
  issn = {13697412, 14679868},
  doi = {10/dzzss3},
  urldate = {2018-11-27},
  abstract = {Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efficient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a L{\'e}vy-driven stochastic volatility model.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QNVP7M5M/Andrieu et al. - 2010 - Particle Markov chain Monte Carlo methods Particl.pdf}
}

@misc{andriushchenko_sgd_2022,
  title = {{{SGD}} with Large Step Sizes Learns Sparse Features},
  author = {Andriushchenko, Maksym and Varre, Aditya and {Pillaud-Vivien}, Loucas and Flammarion, Nicolas},
  year = {2022},
  month = oct,
  number = {arXiv:2210.05337},
  eprint = {2210.05337},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.05337},
  urldate = {2022-10-26},
  abstract = {We showcase important features of the dynamics of the Stochastic Gradient Descent (SGD) in the training of neural networks. We present empirical observations that commonly used large step sizes (i) lead the iterates to jump from one side of a valley to the other causing loss stabilization, and (ii) this stabilization induces a hidden stochastic dynamics orthogonal to the bouncing directions that biases it implicitly toward simple predictors. Furthermore, we show empirically that the longer large step sizes keep SGD high in the loss landscape valleys, the better the implicit regularization can operate and find sparse representations. Notably, no explicit regularization is used so that the regularization effect comes solely from the SGD training dynamics influenced by the step size schedule. Therefore, these observations unveil how, through the step size schedules, both gradient and noise drive together the SGD dynamics through the loss landscape of neural networks. We justify these findings theoretically through the study of simple neural network models as well as qualitative arguments inspired from stochastic processes. Finally, this analysis allows to shed a new light on some common practice and observed phenomena when training neural networks. The code of our experiments is available at https://github.com/tml-epfl/sgd-sparse-features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GIQW764P/Andriushchenko et al. - 2022 - SGD with large step sizes learns sparse features.pdf;/Users/antoniohortaribeiro/Zotero/storage/PMVJSJJI/2210.html}
}

@book{angelopoulos_conformal_2023,
  title = {Conformal {{Prediction}}: {{A Gentle Introduction}}},
  author = {Angelopoulos, Anastasios N. and Bates, Stephen},
  year = {2023},
  eprint = {2107.07511},
  primaryclass = {cs, math, stat},
  publisher = {{Now Foundations and Trends}},
  urldate = {2023-09-07},
  abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction (a.k.a. conformal inference) is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/antoniohortaribeiro/Zotero/storage/S4C7S4WT/Angelopoulos and Bates - 2022 - A Gentle Introduction to Conformal Prediction and .pdf}
}

@book{anton_elementary_2010,
  title = {Elementary {{Linear Algebra}}: {{Applications Version}}},
  author = {Anton, H. and Rorres, C.},
  year = {2010},
  publisher = {John Wiley \& Sons},
  isbn = {978-0-470-43205-1},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2FPIJ33D/anton_elementary_2010.pdf;/Users/antoniohortaribeiro/Zotero/storage/Q9F5AWXP/anton_elementary_2010.pdf}
}

@article{aquino_brazilian_2012,
  title = {Brazilian Longitudinal Study of Adult Health ({{ELSA-Brasil}}): {{Objectives}} and Design},
  author = {Aquino, Estela M. L. and Barreto, Sandhi Maria and Bensenor, Isabela M. and Carvalho, Marilia S. and Chor, D{\'o}ra and Duncan, Bruce B. and Lotufo, Paulo A. and Mill, Jos{\'e} Geraldo and Molina, Maria Del Carmen and Mota, Eduardo L. A. and Azeredo Passos, Val{\'e}ria Maria and Schmidt, Maria In{\^e}s and Szklo, Moyses},
  year = {2012},
  month = jan,
  journal = {American Journal of Epidemiology},
  volume = {175},
  number = {4},
  eprint = {https://academic.oup.com/aje/article-pdf/175/4/315/267831/kwr294.pdf},
  pages = {315--324},
  issn = {0002-9262},
  doi = {10.1093/aje/kwr294},
  abstract = {Although low- and middle-income countries still bear the burden of major infectious diseases, chronic noncommunicable diseases are becoming increasingly common due to rapid demographic, epidemiologic, and nutritional transitions. However, information is generally scant in these countries regarding chronic disease incidence, social determinants, and risk factors. The Brazilian Longitudinal Study of Adult Health (ELSA-Brasil) aims to contribute relevant information with respect to the development and progression of clinical and subclinical chronic diseases, particularly cardiovascular diseases and diabetes. In this report, the authors delineate the study's objectives, principal methodological features, and timeline. At baseline, ELSA-Brasil enrolled 15,105 civil servants from 5 universities and 1 research institute. The baseline examination (2008--2010) included detailed interviews, clinical and anthropometric examinations, an oral glucose tolerance test, overnight urine collection, a 12-lead resting electrocardiogram, measurement of carotid intima-media thickness, echocardiography, measurement of pulse wave velocity, hepatic ultrasonography, retinal fundus photography, and an analysis of heart rate variability. Long-term biologic sample storage will allow investigation of biomarkers that may predict cardiovascular diseases and diabetes. Annual telephone surveillance, initiated in 2009, will continue for the duration of the study. A follow-up examination is scheduled for 2012--2013.}
}

@phdthesis{araujo_desenvolvimento_2012,
  type = {{Projeto de Final de Curso}},
  title = {{Desenvolvimento de uma Ferramenta para Identifica{\c c}{\~a}o de Sinais Correlacionados: Aplica{\c c}{\~a}o em Dados Provenientes de uma Plataforma de Petr{\'o}leo}},
  author = {Araujo, Kim C{\^a}ndido Pereira},
  year = {2012},
  month = jun,
  langid = {portuguese},
  school = {Universidade Federal de Minas Gerais},
  file = {/Users/antoniohortaribeiro/Zotero/storage/77S98G3Z/Aguirre et al. - Desenvolvimento de uma Ferramenta para IdenticaÃ§Ã£.pdf}
}

@article{arbabi_introduction_,
  title = {Introduction to {{Koopman}} Operator Theory of Dynamical Systems},
  author = {Arbabi, Hassan},
  pages = {32},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PWAF79N2/Arbabi - Introduction to Koopman operator theory of dynamic.pdf}
}

@misc{ardeshir_support_2021,
  title = {Support Vector Machines and Linear Regression Coincide with Very High-Dimensional Features},
  author = {Ardeshir, Navid and Sanford, Clayton and Hsu, Daniel},
  year = {2021},
  month = oct,
  number = {arXiv:2105.14084},
  eprint = {2105.14084},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.14084},
  urldate = {2022-12-12},
  abstract = {The support vector machine (SVM) and minimum Euclidean norm least squares regression are two fundamentally different approaches to fitting linear models, but they have recently been connected in models for very high-dimensional data through a phenomenon of support vector proliferation, where every training example used to fit an SVM becomes a support vector. In this paper, we explore the generality of this phenomenon and make the following contributions. First, we prove a super-linear lower bound on the dimension (in terms of sample size) required for support vector proliferation in independent feature models, matching the upper bounds from previous works. We further identify a sharp phase transition in Gaussian feature models, bound the width of this transition, and give experimental support for its universality. Finally, we hypothesize that this phase transition occurs only in much higher-dimensional settings in the \${\textbackslash}ell\_1\$ variant of the SVM, and we present a new geometric characterization of the problem that may elucidate this phenomenon for the general \${\textbackslash}ell\_p\$ case.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7XCW9JBH/Ardeshir et al. - 2021 - Support vector machines and linear regression coin.pdf;/Users/antoniohortaribeiro/Zotero/storage/9W8U889C/2105.html}
}

@inproceedings{arjovsky_unitary_2016,
  title = {Unitary Evolution Recurrent Neural Networks},
  booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
  author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  year = {2016},
  series = {{{ICML}}'16},
  pages = {1120--1128},
  publisher = {JMLR.org},
  address = {New York, NY, USA},
  acmid = {3045509},
  numpages = {9},
  keywords = {â›” No DOI found}
}

@article{arora_convergence_2019,
  title = {A {{Convergence Analysis}} of {{Gradient Descent}} for {{Deep Linear Neural Networks}}},
  author = {Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
  year = {2019},
  month = oct,
  journal = {arXiv:1810.02281 [cs, stat]},
  eprint = {1810.02281},
  primaryclass = {cs, stat},
  urldate = {2020-08-27},
  abstract = {We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network (parameterized as \$x {\textbackslash}mapsto W\_N W\_\{N-1\} {\textbackslash}cdots W\_1 x\$) by minimizing the \${\textbackslash}ell\_2\$ loss over whitened data. Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution. The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme. Our results significantly extend previous analyses, e.g., of deep linear residual networks (Bartlett et al., 2018).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/A8KFSCMU/Arora et al. - 2019 - A Convergence Analysis of Gradient Descent for Dee.pdf;/Users/antoniohortaribeiro/Zotero/storage/Z8ZA8LBL/1810.html}
}

@inproceedings{arora_exact_2019,
  title = {On Exact Computation with an Infinitely Wide Neural Net},
  booktitle = {Thirty-Third Conference on Neural Information Processing Systems},
  author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
  year = {2019}
}

@article{arora_finegrained_2019,
  title = {Fine-{{Grained Analysis}} of {{Optimization}} and {{Generalization}} for {{Overparameterized Two-Layer Neural Networks}}},
  author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.08584 [cs, stat]},
  eprint = {1901.08584},
  primaryclass = {cs, stat},
  urldate = {2019-02-22},
  abstract = {Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR'17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GVQUXMPY/arora_fine-grain_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/2MJC3H4W/1901.html}
}

@misc{arora_stronger_2018,
  title = {Stronger Generalization Bounds for Deep Nets via a Compression Approach},
  author = {Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  year = {2018},
  month = nov,
  number = {arXiv:1802.05296},
  eprint = {1802.05296},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-27},
  abstract = {Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. The current paper shows generalization bounds that're orders of magnitude better in practice. These rely upon new succinct reparametrizations of the trained net --- a compression that is explicit and efficient. These yield generalization bounds via a simple compression-based framework introduced here. Our results also provide some theoretical justification for widespread empirical success in compressing deep nets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {\_tablet,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QDMKQFIG/Arora et al_2018_Stronger generalization bounds for deep nets via a compression approach.pdf}
}

@article{arreckx_regularized_2018,
  title = {A {{Regularized Factorization-Free Method}} for {{Equality-Constrained Optimization}}},
  author = {Arreckx, Sylvain and Orban, Dominique},
  year = {2018},
  month = jan,
  journal = {SIAM Journal on Optimization},
  volume = {28},
  number = {2},
  pages = {1613--1639},
  issn = {1052-6234, 1095-7189},
  doi = {10.1137/16M1088570},
  urldate = {2018-05-23},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QBCSMG5V/arreckx_a_2018.pdf}
}

@book{artin_algebra_1991,
  title = {Algebra},
  author = {Artin, M.},
  year = {1991},
  publisher = {Prentice Hall},
  isbn = {978-0-13-004763-2},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SHJUG8A5/artin_algebra_1991.pdf}
}

@book{ash_probability_2000,
  title = {Probability and {{Measure Theory}}},
  author = {Ash, Robert B. and {Dol{\'e}ans-Dale}, Catherine},
  year = {2000},
  edition = {2nd},
  publisher = {Harcourt/Academic Press},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PAQP2VQ4/Ash and DolÃ©ans-Dale - 2000 - Probability and Measure Theory.pdf}
}

@article{ashton_that_2009,
  title = {That `Internet of Things' Thing},
  author = {Ashton, Kevin},
  year = {2009},
  journal = {RFID journal},
  volume = {22},
  number = {7},
  pages = {97--114}
}

@article{astrom_maximum_1979,
  title = {Maximum Likelihood and Prediction Error Methods},
  author = {Astrom, K. J.},
  year = {1979},
  journal = {IFAC Proceedings Volumes},
  volume = {12},
  number = {8},
  pages = {551--574},
  doi = {10.1016/S1474-6670(17)53976-2},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3ICCUZJ5/astrom_maximum_1979.pdf}
}

@article{astrom_system_1971,
  title = {System {{Identification}}---{{A Survey}}},
  author = {{\AA}str{\"o}m, K.J. and Eykhoff, P.},
  year = {1971},
  month = mar,
  journal = {Automatica},
  volume = {7},
  number = {2},
  pages = {123--162},
  issn = {0005-1098},
  doi = {10/dw3ktq}
}

@article{astrom_system_1971a,
  title = {System Identification---{{A}} Survey},
  author = {{\AA}str{\"o}m, K.J. and Eykhoff, P.},
  year = {1971},
  month = mar,
  journal = {Automatica},
  volume = {7},
  number = {2},
  pages = {123--162},
  issn = {0005-1098},
  doi = {10.1016/0005-1098(71)90059-8},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CNCVR8X6/Ã¥strÃ¶m_system_1971.pdf}
}

@article{attia_artificial_2019,
  title = {An Artificial Intelligence-Enabled {{ECG}} Algorithm for the Identification of Patients with Atrial Fibrillation during Sinus Rhythm: A Retrospective Analysis of Outcome Prediction},
  shorttitle = {An Artificial Intelligence-Enabled {{ECG}} Algorithm for the Identification of Patients with Atrial Fibrillation during Sinus Rhythm},
  author = {Attia, Zachi I and Noseworthy, Peter A and {Lopez-Jimenez}, Francisco and Asirvatham, Samuel J and Deshmukh, Abhishek J and Gersh, Bernard J and Carter, Rickey E and Yao, Xiaoxi and Rabinstein, Alejandro A and Erickson, Brad J and Kapa, Suraj and Friedman, Paul A},
  year = {2019},
  month = aug,
  journal = {The Lancet},
  issn = {0140-6736},
  doi = {10/gf7d9h},
  urldate = {2019-09-02},
  abstract = {Summary Background Atrial fibrillation is frequently asymptomatic and thus underdetected but is associated with stroke, heart failure, and death. Existing screening methods require prolonged monitoring and are limited by cost and low yield. We aimed to develop a rapid, inexpensive, point-of-care means of identifying patients with atrial fibrillation using machine learning. Methods We developed an artificial intelligence (AI)-enabled electrocardiograph (ECG) using a convolutional neural network to detect the electrocardiographic signature of atrial fibrillation present during normal sinus rhythm using standard 10-second, 12-lead ECGs. We included all patients aged 18 years or older with at least one digital, normal sinus rhythm, standard 10-second, 12-lead ECG acquired in the supine position at the Mayo Clinic ECG laboratory between Dec 31, 1993, and July 21, 2017, with rhythm labels validated by trained personnel under cardiologist supervision. We classified patients with at least one ECG with a rhythm of atrial fibrillation or atrial flutter as positive for atrial fibrillation. We allocated ECGs to the training, internal validation, and testing datasets in a 7:1:2 ratio. We calculated the area under the curve (AUC) of the receiver operatoring characteristic curve for the internal validation dataset to select a probability threshold, which we applied to the testing dataset. We evaluated model performance on the testing dataset by calculating the AUC and the accuracy, sensitivity, specificity, and F1 score with two-sided 95\% CIs. Findings We included 180\hphantom{,}922 patients with 649\hphantom{,}931 normal sinus rhythm ECGs for analysis: 454\hphantom{,}789 ECGs recorded from 126\hphantom{,}526 patients in the training dataset, 64\hphantom{,}340 ECGs from 18\hphantom{,}116 patients in the internal validation dataset, and 130\hphantom{,}802 ECGs from 36\hphantom{,}280 patients in the testing dataset. 3051 (8{$\cdot$}4\%) patients in the testing dataset had verified atrial fibrillation before the normal sinus rhythm ECG tested by the model. A single AI-enabled ECG identified atrial fibrillation with an AUC of 0{$\cdot$}87 (95\% CI 0{$\cdot$}86--0{$\cdot$}88), sensitivity of 79{$\cdot$}0\% (77{$\cdot$}5--80{$\cdot$}4), specificity of 79{$\cdot$}5\% (79{$\cdot$}0--79{$\cdot$}9), F1 score of 39{$\cdot$}2\% (38{$\cdot$}1--40{$\cdot$}3), and overall accuracy of 79{$\cdot$}4\% (79{$\cdot$}0--79{$\cdot$}9). Including all ECGs acquired during the first month of each patient's window of interest (ie, the study start date or 31 days before the first recorded atrial fibrillation ECG) increased the AUC to 0{$\cdot$}90 (0{$\cdot$}90--0{$\cdot$}91), sensitivity to 82{$\cdot$}3\% (80{$\cdot$}9--83{$\cdot$}6), specificity to 83{$\cdot$}4\% (83{$\cdot$}0--83{$\cdot$}8), F1 score to 45{$\cdot$}4\% (44{$\cdot$}2--46{$\cdot$}5), and overall accuracy to 83{$\cdot$}3\% (83{$\cdot$}0--83{$\cdot$}7). Interpretation An AI-enabled ECG acquired during normal sinus rhythm permits identification at point of care of individuals with atrial fibrillation. Funding None.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XGBA9DQA/S0140673619317210.html}
}

@article{attia_screening_2019,
  title = {Screening for Cardiac Contractile Dysfunction Using an Artificial Intelligence--Enabled Electrocardiogram},
  author = {Attia, Zachi I. and Kapa, Suraj and {Lopez-Jimenez}, Francisco and McKie, Paul M. and Ladewig, Dorothy J. and Satam, Gaurav and Pellikka, Patricia A. and {Enriquez-Sarano}, Maurice and Noseworthy, Peter A. and Munger, Thomas M. and Asirvatham, Samuel J. and Scott, Christopher G. and Carter, Rickey E. and Friedman, Paul A.},
  year = {2019},
  month = jan,
  journal = {Nature Medicine},
  volume = {25},
  number = {1},
  pages = {70--74},
  issn = {1546-170X},
  doi = {10.1038/s41591-018-0240-2},
  abstract = {Asymptomatic left ventricular dysfunction (ALVD) is present in 3--6\% of the general population, is associated with reduced quality of life and longevity, and is treatable when found1--4. An inexpensive, noninvasive screening tool for ALVD in the doctor's office is not available. We tested the hypothesis that application of artificial intelligence (AI) to the electrocardiogram (ECG), a routine method of measuring the heart's electrical activity, could identify ALVD. Using paired 12-lead ECG and echocardiogram data, including the left ventricular ejection fraction (a measure of contractile function), from 44,959 patients at the Mayo Clinic, we trained a convolutional neural network to identify patients with ventricular dysfunction, defined as ejection fraction {$\leq$}35\%, using the ECG data alone. When tested on an independent set of 52,870 patients, the network model yielded values for the area under the curve, sensitivity, specificity, and accuracy of 0.93, 86.3\%, 85.7\%, and 85.7\%, respectively. In patients without ventricular dysfunction, those with a positive AI screen were at 4 times the risk (hazard ratio, 4.1; 95\% confidence interval, 3.3 to 5.0) of developing future ventricular dysfunction compared with those with a negative screen. Application of AI to the ECG---a ubiquitous, low-cost test---permits the ECG to serve as a powerful screening tool in asymptomatic individuals to identify ALVD.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DQDEL3CM/Attia et al. - 2019 - Screening for cardiac contractile dysfunction usin.pdf}
}

@article{attiazachii._age_2019,
  title = {Age and {{Sex Estimation Using Artificial Intelligence From Standard}} 12-{{Lead ECGs}}},
  author = {{Attia Zachi I.} and A., Friedman Paul and {Noseworthy Peter A.} and {Lopez-Jimenez Francisco} and {Ladewig Dorothy J.} and {Satam Gaurav} and {Pellikka Patricia A.} and {Munger Thomas M.} and {Asirvatham Samuel J.} and {Scott Christopher G.} and {Carter Rickey E.} and {Kapa Suraj}},
  year = {2019},
  month = sep,
  journal = {Circulation: Arrhythmia and Electrophysiology},
  volume = {12},
  number = {9},
  pages = {e007284},
  doi = {10/gf7d9g},
  urldate = {2019-09-02},
  abstract = {Background:Sex and age have long been known to affect the ECG. Several biologic variables and anatomic factors may contribute to sex and age-related differences on the ECG. We hypothesized that a convolutional neural network (CNN) could be trained through a process called deep learning to predict a person's age and self-reported sex using only 12-lead ECG signals. We further hypothesized that discrepancies between CNN-predicted age and chronological age may serve as a physiological measure of health.Methods:We trained CNNs using 10-second samples of 12-lead ECG signals from 499\,727 patients to predict sex and age. The networks were tested on a separate cohort of 275\,056 patients. Subsequently, 100 randomly selected patients with multiple ECGs over the course of decades were identified to assess within-individual accuracy of CNN age estimation.Results:Of 275\,056 patients tested, 52\% were males and mean age was 58.6{\textpm}16.2 years. For sex classification, the model obtained 90.4\% classification accuracy with an area under the curve of 0.97 in the independent test data. Age was estimated as a continuous variable with an average error of 6.9{\textpm}5.6 years (R-squared =0.7). Among 100 patients with multiple ECGs over the course of at least 2 decades of life, most patients (51\%) had an average error between real age and CNN-predicted age of {$<$}7 years. Major factors seen among patients with a CNN-predicted age that exceeded chronologic age by {$>$}7 years included: low ejection fraction, hypertension, and coronary disease (P{$<$}0.01). In the 27\% of patients where correlation was {$>$}0.8 between CNN-predicted and chronologic age, no incident events occurred over follow-up (33{\textpm}12 years).Conclusions:Applying artificial intelligence to the ECG allows prediction of patient sex and estimation of age. The ability of an artificial intelligence algorithm to determine physiological age, with further validation, may serve as a measure of overall health.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PEDYQT2L/attia zachi i._age and_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/K8I9BWLF/CIRCEP.119.html}
}

@book{augustyniak_equivalence_,
  title = {On {{The Equivalence Of The}} 12-{{Lead Ecg And The Vcg Representations Of The Cardiac Electrical Activity}}},
  author = {Augustyniak, Piotr},
  abstract = {The electrocardiography (ECG) and vectocardiography (VCG) both describe the same phenomena: the temporal changes of the surface potentials resulting from cardiac electrical field. The ECG uses 12 leads positioned as it was found optimal from the medical point of view during a hundred years of practice. The VCG uses 3 channels connected to the psuedoorthogonal leads that placement is determined by the orthogonality of main Cartesian axes - in consequence a three-dimensional recording is performed. This paper is devoted to the experimental verification of the likeness of the data provided by both recording techniques. Two different transforms re-mapping the ECG to the VCG domain and viceversa were studied with use of the set of 125 simultaneous ECG and VCG signals from the CSE Multilead Database. One of the possible technical interests of transforming the ECG signals to the VCG domain is reducing the data volume thanks to eliminating the information redundancy typical for ECG. Our results demonstrate that the forward and inverse transform has no perfect reconstruction property and some extent of distortion should be considered when applying this technique to the signal compression.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VPU7RN3W/augustyniak_on the_.pdf;/Users/antoniohortaribeiro/Zotero/storage/S7R7B6HD/summary.html}
}

@article{ayalasolares_modeling_2016,
  title = {Modeling and Prediction of Global Magnetic Disturbance in Near-{{Earth}} Space: {{A}} Case Study for {{Kp}} Index Using {{NARX}} Models},
  author = {Ayala Solares, Jose Roberto and Wei, Hua-Liang and Boynton, R. J. and Walker, Simon N and Billings, Stephen A},
  year = {2016},
  journal = {Space Weather},
  volume = {14},
  number = {10},
  pages = {899--916},
  keywords = {ðŸ”No DOI found}
}

@article{ayalasolares_modeling_2016a,
  title = {Modeling and {{Prediction}} of {{Global Magnetic Disturbance}} in {{Near-Earth Space}}: {{A Case Study}} for {{Kp Index Using NARX Models}}},
  author = {Ayala Solares, Jose Roberto and Wei, Hua-Liang and Boynton, R. J. and Walker, Simon N and Billings, Stephen A},
  year = {2016},
  journal = {Space Weather},
  volume = {14},
  number = {10},
  pages = {899--916},
  doi = {10/gfjwmm}
}

@article{azad_longterm_2014,
  title = {Long-Term Wind Speed Forecasting and General Pattern Recognition Using Neural Networks},
  author = {Azad, Hanieh Borhan and Mekhilef, Saad and Ganapathy, Vellapa Gounder},
  year = {2014},
  journal = {IEEE Transactions on Sustainable Energy},
  volume = {5},
  number = {2},
  pages = {546--553},
  doi = {10.1109/TSTE.2014.2300150}
}

@article{azulay_why_2019,
  title = {Why Do Deep Convolutional Networks Generalize so Poorly to Small Image Transformations?},
  author = {Azulay, Aharon and Weiss, Yair},
  year = {2019},
  journal = {Journal of Machine Learning Research},
  volume = {20},
  pages = {1--25},
  abstract = {Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the network's prediction. In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are sufficient to achieve the desired invariance. Specifically, we show that the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem, and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set. We discuss two possible solutions to this problem: (1) antialiasing the intermediate representations and (2) increasing data augmentation and show that they provide only a partial solution at best. Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DAPDL9GL/Azulay and Weiss - Why do deep convolutional networks generalize so p.pdf}
}

@article{ba_layer_2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  journal = {arXiv:1607.06450 [cs, stat]},
  eprint = {1607.06450},
  primaryclass = {cs, stat},
  urldate = {2019-05-29},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3USJAJEY/ba_layer_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/M7UJMEBN/1607.html}
}

@article{baake_fitting_1992,
  title = {Fitting Ordinary Differential Equations to Chaotic Data},
  author = {Baake, Ellen and Baake, Michael and Bock, {\relax HG} and Briggs, {\relax KM}},
  year = {1992},
  journal = {Physical Review A},
  volume = {45},
  number = {8},
  pages = {5524},
  doi = {10.1103/PhysRevA.45.5524}
}

@article{baake_modelling_1992,
  title = {Modelling the Fast Fluorescence Rise of Photosynthesis},
  author = {Baake, Ellen and Schl{\"o}der, Johannes P},
  year = {1992},
  journal = {Bulletin of mathematical biology},
  volume = {54},
  number = {6},
  pages = {999--1021},
  keywords = {â“Multiple DOI}
}

@article{bach_breaking_2017,
  title = {Breaking the Curse of Dimensionality with Convex Neural Networks},
  author = {Bach, Francis},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {19},
  pages = {1--53},
  file = {/Users/antoniohortaribeiro/Zotero/storage/47PQZMC9/Bach - Breaking the Curse of Dimensionality with Convex N.pdf}
}

@article{bach_equivalence_2017,
  title = {On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions},
  author = {Bach, Francis},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {21},
  pages = {1--38},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9SPQ2X9C/Bach - On the Equivalence between Kernel Quadrature Rules.pdf}
}

@misc{bach_etatrick_2019,
  title = {The ``Eta-Trick'' or the Effectiveness of Reweighted Least-Squares -- {{Machine Learning Research Blog}}},
  author = {Bach, Francis},
  year = {2019},
  month = jul,
  urldate = {2023-06-02},
  langid = {american},
  keywords = {\_tablet},
  file = {/Users/antoniohortaribeiro/Zotero/storage/K43F2KU9/Bach_2019_The â€œÎ·-trickâ€ or the effectiveness of reweighted least-squares â€“ Machine.pdf}
}

@article{bach_highdimensional_2023,
  title = {High-Dimensional Analysis of Double Descent for Linear Regression with Random Projections},
  author = {Bach, Francis},
  year = {2023},
  month = mar,
  journal = {arXiv:2303.01372},
  eprint = {2303.01372},
  abstract = {We consider linear regression problems with a varying number of random projections, where we provably exhibit a double descent curve for a fixed prediction problem, with a high-dimensional analysis based on random matrix theory. We first consider the ridge regression estimator and re-interpret earlier results using classical notions from non-parametric statistics, namely degrees of freedom, also known as effective dimensionality. In particular, we show that the random design performance of ridge regression with a specific regularization parameter matches the classical bias and variance expressions coming from the easier fixed design analysis but for another larger implicit regularization parameter. We then compute asymptotic equivalents of the generalization performance (in terms of bias and variance) of the minimum norm least-squares fit with random projections, providing simple expressions for the double descent phenomenon.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YXNMAHU2/Bach_2023_High-dimensional analysis of double descent for linear regression with random.pdf;/Users/antoniohortaribeiro/Zotero/storage/KHHKQ6E6/2303.html}
}

@book{bach_learning_2023,
  title = {Learning {{Theory}} from {{First Principles}}},
  author = {Bach, Francis},
  year = {2023},
  langid = {english},
  keywords = {\_tablet\_modified},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IIEKLTG7/Bach_2023_Learning Theory from First Principles.pdf}
}

@article{bach_optimization_2011,
  title = {Optimization with {{Sparsity-Inducing Penalties}}},
  author = {Bach, Francis},
  year = {2011},
  journal = {Foundations and Trends in Machine Learning},
  volume = {4},
  number = {1},
  pages = {1--106},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000015},
  urldate = {2023-06-12},
  abstract = {Sparse estimation methods are aimed at using or obtaining parsimonious representations of data or models. They were first dedicated to linear variable selection but numerous extensions have now emerged such as structured sparsity or kernel selection. It turns out that many of the related estimation problems can be cast as convex optimization problems by regularizing the empirical risk with appropriate nonsmooth norms. The goal of this monograph is to present from a general perspective optimization tools and techniques dedicated to such sparsity-inducing penalties. We cover proximal methods, block-coordinate descent, reweighted 2-penalized techniques, workingset and homotopy methods, as well as non-convex formulations and extensions, and provide an extensive set of experiments to compare various algorithms from a computational point of view.},
  langid = {english},
  keywords = {\_tablet},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HADUHLC5/Bach_2011_Optimization with Sparsity-Inducing Penalties.pdf}
}

@misc{bach_relationship_2023,
  title = {On the Relationship between Multivariate Splines and Infinitely-Wide Neural Networks},
  author = {Bach, Francis},
  year = {2023},
  month = mar,
  number = {arXiv:2302.03459},
  eprint = {2302.03459},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.03459},
  urldate = {2023-05-25},
  abstract = {We consider multivariate splines and show that they have a random feature expansion as infinitely wide neural networks with one-hidden layer and a homogeneous activation function which is the power of the rectified linear unit. We show that the associated function space is a Sobolev space on a Euclidean ball, with an explicit bound on the norms of derivatives. This link provides a new random feature expansion for multivariate splines that allow efficient algorithms. This random feature expansion is numerically better behaved than usual random Fourier features, both in theory and practice. In particular, in dimension one, we compare the associated leverage scores to compare the two random expansions and show a better scaling for the neural network expansion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XX5L6K3R/Bach_2023_On the relationship between multivariate splines and infinitely-wide neural.pdf;/Users/antoniohortaribeiro/Zotero/storage/DU3X8V5J/2302.html}
}

@article{bahdanau_neural_2014,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  month = sep,
  journal = {arXiv:1409.0473 [cs, stat]},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/C39DHWE9/bahdanau_neural_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/M3477GFU/1409.html}
}

@article{bai_asymptotics_2007,
  title = {On Asymptotics of Eigenvectors of Large Sample Covariance Matrix},
  author = {Bai, Z. D. and Miao, B. Q. and Pan, G. M.},
  year = {2007},
  month = jul,
  journal = {The Annals of Probability},
  volume = {35},
  number = {4},
  eprint = {0708.1720},
  pages = {1532--1572},
  issn = {0091-1798},
  doi = {10.1214/009117906000001079},
  urldate = {2020-12-22},
  abstract = {Let {\textbackslash}\{\$X\_\{ij\}\${\textbackslash}\}, \$i,j=...,\$ be a double array of i.i.d. complex random variables with \$EX\_\{11\}=0,E{\textbar}X\_\{11\}{\textbar}{\textasciicircum}2=1\$ and \$E{\textbar}X\_\{11\}{\textbar}{\textasciicircum}4{$<\backslash$}infty\$, and let \$A\_n={\textbackslash}frac\{1\}\{N\}T\_n{\textasciicircum}\{\{1\}/\{2\}\}X\_nX\_n{\textasciicircum}*T\_n{\textasciicircum}\{\{1\}/\{2\}\}\$, where \$T\_n{\textasciicircum}\{\{1\}/\{2\}\}\$ is the square root of a nonnegative definite matrix \$T\_n\$ and \$X\_n\$ is the \$n{\textbackslash}times N\$ matrix of the upper-left corner of the double array. The matrix \$A\_n\$ can be considered as a sample covariance matrix of an i.i.d. sample from a population with mean zero and covariance matrix \$T\_n\$, or as a multivariate \$F\$ matrix if \$T\_n\$ is the inverse of another sample covariance matrix. To investigate the limiting behavior of the eigenvectors of \$A\_n\$, a new form of empirical spectral distribution is defined with weights defined by eigenvectors and it is then shown that this has the same limiting spectral distribution as the empirical spectral distribution defined by equal weights. Moreover, if {\textbackslash}\{\$X\_\{ij\}\${\textbackslash}\} and \$T\_n\$ are either real or complex and some additional moment assumptions are made then linear spectral statistics defined by the eigenvectors of \$A\_n\$ are proved to have Gaussian limits, which suggests that the eigenvector matrix of \$A\_n\$ is nearly Haar distributed when \$T\_n\$ is a multiple of the identity matrix, an easy consequence for a Wishart matrix.},
  archiveprefix = {arXiv},
  keywords = {{15A52, 60F15, 62E20 (Primary)},{60F17, 62H99 (Secondary)},Mathematics - Probability},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QYT53LTB/Bai et al_2007_On asymptotics of eigenvectors of large sample covariance matrix.pdf;/Users/antoniohortaribeiro/Zotero/storage/N6WQQD4E/0708.html}
}

@article{bai_deep_2019,
  title = {Deep {{Equilibrium Models}}},
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.01377 [cs, stat]},
  eprint = {1909.01377},
  primaryclass = {cs, stat},
  urldate = {2019-10-14},
  abstract = {We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective "depth" of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these state-of-the-art models (for similar parameter counts); 2) have similar computational requirements as existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88\% memory reduction in our experiments. The code is available at https://github. com/locuslab/deq .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q4J5I34Q/Bai et al. - 2019 - Deep Equilibrium Models.pdf;/Users/antoniohortaribeiro/Zotero/storage/J7457MT6/1909.html}
}

@article{bai_empirical_2018,
  title = {An {{Empirical Evaluation}} of {{Generic Convolutional}} and {{Recurrent Networks}} for {{Sequence Modeling}}},
  author = {Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  year = {2018},
  journal = {arXiv:1803.01271},
  eprint = {1803.01271},
  abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UVNBZTPY/Bai et al. - An Empirical Evaluation of Generic Convolutional a.pdf}
}

@article{bai_recent_2021,
  title = {Recent {{Advances}} in {{Adversarial Training}} for {{Adversarial Robustness}}},
  author = {Bai, Tao and Luo, Jinqi and Zhao, Jun and Wen, Bihan and Wang, Qian},
  year = {2021},
  month = apr,
  journal = {International Joint Conference on Artificial Intelligence (IJCAI)},
  eprint = {2102.01356},
  abstract = {Adversarial training is one of the most effective approaches to defending deep learning models against adversarial examples. Unlike other defense strategies, adversarial training aims to enhance the robustness of models intrinsically. During the last few years, adversarial training has been studied and discussed from various aspects. A variety of improvements and developments of adversarial training are proposed, which were, however, neglected in existing surveys. For the first time in this survey, we systematically review the recent progress on adversarial training for adversarial robustness with a novel taxonomy. Then we discuss the generalization problems in adversarial training from three perspectives and highlight the challenges which are not fully tackled. Finally, we present potential future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NUQV984N/Bai et al_2021_Recent Advances in Adversarial Training for Adversarial Robustness.pdf}
}

@book{bai_spectral_2010,
  title = {Spectral Analysis of Large Dimensional Random Matrices},
  author = {Bai, Zhidong and Silverstein, Jack W},
  year = {2010},
  series = {Springer {{Series}} in {{Statistics}}},
  volume = {20},
  publisher = {Springer},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SVNBQJR5/Bai and Silverstein - 2010 - Spectral Analysis of Large Dimensional Random Matr.pdf}
}

@article{bai_trellis_2018,
  title = {Trellis {{Networks}} for {{Sequence Modeling}}},
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.06682 [cs, stat]},
  eprint = {1810.06682},
  primaryclass = {cs, stat},
  urldate = {2019-06-06},
  abstract = {We present trellis networks, a new architecture for sequence modeling. On the one hand, a trellis network is a temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. On the other hand, we show that truncated recurrent networks are equivalent to trellis networks with special sparsity structure in their weight matrices. Thus trellis networks with general weight matrices generalize truncated recurrent networks. We leverage these connections to design high-performing trellis networks that absorb structural and algorithmic elements from both recurrent and convolutional models. Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling tasks, and stress tests designed to evaluate long-term memory retention. The code is available at https://github.com/locuslab/trellisnet .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q6C2TAGK/bai_trellis_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/A3GCRJH8/1810.html}
}

@incollection{bailey_sizenoise_2018,
  title = {Size-{{Noise Tradeoffs}} in {{Generative Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Bailey, Bolton and Telgarsky, Matus J},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {6489--6499},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-04},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CGUEMK9T/Bailey and Telgarsky - Size-Noise Tradeoffs in Generative Networks.pdf;/Users/antoniohortaribeiro/Zotero/storage/PINR64IM/bailey_size-noise_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/7VIEGTNL/7884-size-noise-tradeoffs-in-generative-networks.html}
}

@incollection{baldi_neuronal_2018,
  title = {On {{Neuronal Capacity}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Baldi, Pierre and Vershynin, Roman},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {7739--7748},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-04},
  file = {/Users/antoniohortaribeiro/Zotero/storage/M7Q6GP95/baldi_on_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/ZBNE5H87/7999-on-neuronal-capacity.html}
}

@article{banerjee_diagnosis_2017,
  title = {Diagnosis of Prostate Cancer by Desorption Electrospray Ionization Mass Spectrometric Imaging of Small Metabolites and Lipids},
  author = {Banerjee, Shibdas and Zare, Richard N. and Tibshirani, Robert J. and Kunder, Christian A. and Nolley, Rosalie and Fan, Richard and Brooks, James D. and Sonn, Geoffrey A.},
  year = {2017},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {13},
  pages = {3334--3339},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1700677114},
  urldate = {2017-09-18},
  abstract = {Accurate identification of prostate cancer in frozen sections at the time of surgery can be challenging, limiting the surgeon's ability to best determine resection margins during prostatectomy. We performed desorption electrospray ionization mass spectrometry imaging (DESI-MSI) on 54 banked human cancerous and normal prostate tissue specimens to investigate the spatial distribution of a wide variety of small metabolites, carbohydrates, and lipids. In contrast to several previous studies, our method included Krebs cycle intermediates (m/z {$<$}200), which we found to be highly informative in distinguishing cancer from benign tissue. Malignant prostate cells showed marked metabolic derangements compared with their benign counterparts. Using the ``Least absolute shrinkage and selection operator'' (Lasso), we analyzed all metabolites from the DESI-MS data and identified parsimonious sets of metabolic profiles for distinguishing between cancer and normal tissue. In an independent set of samples, we could use these models to classify prostate cancer from benign specimens with nearly 90\% accuracy per patient. Based on previous work in prostate cancer showing that glucose levels are high while citrate is low, we found that measurement of the glucose/citrate ion signal ratio accurately predicted cancer when this ratio exceeds 1.0 and normal prostate when the ratio is less than 0.5. After brief tissue preparation, the glucose/citrate ratio can be recorded on a tissue sample in 1 min or less, which is in sharp contrast to the 20 min or more required by histopathological examination of frozen tissue specimens.},
  langid = {english},
  pmid = {28292895},
  keywords = {desorption electrospray ionization,Krebs cycle,mass spectrometry,metabolism,prostate cancer},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZPVZRB96/banerjee_diagnosis_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/2U2V3W5A/3334.html}
}

@article{banerjee_largest_2017,
  title = {Largest Eigenvalue of Large Random Block Matrices: {{A}} Combinatorial Approach},
  shorttitle = {Largest Eigenvalue of Large Random Block Matrices},
  author = {Banerjee, Debapratim and Bose, Arup},
  year = {2017},
  month = apr,
  journal = {Random Matrices: Theory and Applications},
  volume = {06},
  number = {02},
  pages = {1750008},
  issn = {2010-3263, 2010-3271},
  doi = {10.1142/S2010326317500083},
  urldate = {2020-11-23},
  abstract = {We study the largest eigenvalue of certain block matrices where the number of blocks and the block size both increase with suitable conditions on their relative growth. In one of them, we employ a symmetric block structure with large independent Wigner blocks and in the other we have the Wigner block structure with large independent symmetric blocks. The entries are assumed to be independent and identically distributed with mean [Formula: see text] variance [Formula: see text] with an appropriate growth condition on the moments. Under our conditions the limit spectral distribution of these matrices is the standard semi-circle law. It is natural to ask if the extreme eigenvalues converge to the extreme points of its support, namely [Formula: see text]. We exhibit models where this indeed happens as well as models where the spectral norm converges to [Formula: see text]. Our proofs are based on combinatorial analysis of the behavior of the trace of large powers of the matrix.},
  langid = {english}
}

@article{barakat_simultaneous_1999,
  title = {Simultaneous Determination of the Modulus and Phase of a Coherently Illuminated Object from Its Measured Diffraction Image},
  author = {Barakat, Richard and Sandler, Barbara H.},
  year = {1999},
  journal = {Journal of Optics A: Pure and Applied Optics},
  volume = {1},
  number = {5},
  pages = {629},
  doi = {10.1088/1464-4258/1/5/309}
}

@book{barber_bayesian_2012,
  title = {Bayesian Reasoning and Machine Learning},
  author = {Barber, David},
  year = {2012},
  publisher = {Cambridge University Press},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NP35QFSH/barber_bayesian_2012.pdf}
}

@article{barbosa_downhole_2015,
  title = {Downhole {{Pressure Estimation Using Committee Machines}} and {{Neural Networks}}},
  author = {Barbosa, Bruno HG and Gomes, Lucas P and Teixeira, Alex F and Aguirre, Luis A},
  year = {2015},
  journal = {IFAC-PapersOnLine},
  volume = {48},
  number = {6},
  pages = {286--291},
  doi = {10.1016/j.ifacol.2015.08.045}
}

@article{barbosa_downhole_2015a,
  title = {Downhole {{Pressure Estimation Using Committee Machines}} and {{Neural Networks}}},
  author = {Barbosa, Bruno HG and Gomes, Lucas P and Teixeira, Alex F and Aguirre, Luis A},
  year = {2015},
  journal = {IFAC-PapersOnLine},
  volume = {48},
  number = {6},
  pages = {286--291},
  doi = {10/gfjwq5},
  annotation = {00003}
}

@book{barocas_fairness_2023,
  title = {Fairness and {{Machine Learning}}},
  author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  year = {2023},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/V4HWH74K/Barocas et al. - Fairness and Machine Learning.pdf}
}

@misc{barsbey_heavy_2021,
  title = {Heavy {{Tails}} in {{SGD}} and {{Compressibility}} of {{Overparametrized Neural Networks}}},
  author = {Barsbey, Melih and Sefidgaran, Milad and Erdogdu, Murat A. and Richard, Ga{\"e}l and {\c S}im{\c s}ekli, Umut},
  year = {2021},
  month = jun,
  number = {arXiv:2106.03795},
  eprint = {2106.03795},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.03795},
  urldate = {2023-06-15},
  abstract = {Neural network compression techniques have become increasingly popular as they can drastically reduce the storage and computation requirements for very large networks. Recent empirical studies have illustrated that even simple pruning strategies can be surprisingly effective, and several theoretical studies have shown that compressible networks (in specific senses) should achieve a low generalization error. Yet, a theoretical characterization of the underlying cause that makes the networks amenable to such simple compression schemes is still missing. In this study, we address this fundamental question and reveal that the dynamics of the training algorithm has a key role in obtaining such compressible networks. Focusing our attention on stochastic gradient descent (SGD), our main contribution is to link compressibility to two recently established properties of SGD: (i) as the network size goes to infinity, the system can converge to a mean-field limit, where the network weights behave independently, (ii) for a large step-size/batch-size ratio, the SGD iterates can converge to a heavy-tailed stationary distribution. In the case where these two phenomena occur simultaneously, we prove that the networks are guaranteed to be '\${\textbackslash}ell\_p\$-compressible', and the compression errors of different pruning techniques (magnitude, singular value, or node pruning) become arbitrarily small as the network size increases. We further prove generalization bounds adapted to our theoretical framework, which indeed confirm that the generalization error will be lower for more compressible networks. Our theory and numerical study on various neural networks show that large step-size/batch-size ratios introduce heavy-tails, which, in combination with overparametrization, result in compressibility.},
  archiveprefix = {arXiv},
  keywords = {\_tablet\_modified,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/W2HJPA84/Barsbey et al_2021_Heavy Tails in SGD and Compressibility of Overparametrized Neural Networks.pdf;/Users/antoniohortaribeiro/Zotero/storage/PLIENTBS/2106.html}
}

@article{bartlett_adversarial_2021,
  title = {Adversarial {{Examples}} in {{Multi-Layer Random ReLU Networks}}},
  author = {Bartlett, Peter L and Bubeck, S{\'e}bastien and Cherapanamjeri, Yeshwanth},
  year = {2021},
  journal = {Neural Information Processing Systems  (NeurIPS)},
  abstract = {We consider the phenomenon of adversarial examples in ReLU networks with independent Gaussian parameters. For networks of constant depth and with a large range of widths (for instance, it suffices if the width of each layer is polynomial in that of any other layer), small perturbations of input vectors lead to large changes of outputs. This generalizes results of Daniely and Schacham (2020) for networks of rapidly decreasing width and of Bubeck et al (2021) for two-layer networks. Our proof shows that adversarial examples arise in these networks because the functions they compute are locally very similar to random linear functions. Bottleneck layers play a key role: the minimal width up to some point in the network determines scales and sensitivities of mappings computed up to that point. The main result is for networks with constant depth, but we also show that some constraint on depth is necessary for a result of this kind, because there are suitably deep networks that, with constant probability, compute a function that is close to constant.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/C8DKHQTU/Bartlett et al_2021_Adversarial Examples in Multi-Layer Random ReLU Networks.pdf}
}

@article{bartlett_benign_2020,
  title = {Benign Overfitting in Linear Regression},
  author = {Bartlett, Peter L. and Long, Philip M. and Lugosi, G{\'a}bor and Tsigler, Alexander},
  year = {2020},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {48},
  pages = {30063--30070},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1907378117},
  abstract = {The phenomenon of benign overfitting is one of the key mysteries uncovered by deep learning methodology: deep neural networks seem to predict well, even with a perfect fit to noisy training data. Motivated by this phenomenon, we consider when a perfect fit to training data in linear regression is compatible with accurate prediction. We give a characterization of linear regression problems for which the minimum norm interpolating prediction rule has near-optimal prediction accuracy. The characterization is in terms of two notions of the effective rank of the data covariance. It shows that overparameterization is essential for benign overfitting in this setting: the number of directions in parameter space that are unimportant for prediction must significantly exceed the sample size. By studying examples of data covariance properties that this characterization shows are required for benign overfitting, we find an important role for finite-dimensional data: the accuracy of the minimum norm interpolating prediction rule approaches the best possible accuracy for a much narrower range of properties of the data distribution when the data lie in an infinite-dimensional space vs. when the data lie in a finite-dimensional space with dimension that grows faster than the sample size.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7W8FBA7F/Bartlett et al. - 2020 - Benign Overfitting in Linear Regression.pdf;/Users/antoniohortaribeiro/Zotero/storage/FUJX4WLK/1906.11300.pdf}
}

@article{bartlett_deep_2021,
  title = {Deep Learning: A Statistical Viewpoint},
  shorttitle = {Deep Learning},
  author = {Bartlett, Peter L. and Montanari, Andrea and Rakhlin, Alexander},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.09177},
  eprint = {2103.09177},
  urldate = {2021-03-22},
  abstract = {The remarkable practical success of deep learning has revealed some major surprises from a theoretical perspective. In particular, simple gradient methods easily find near-optimal solutions to non-convex optimization problems, and despite giving a near-perfect fit to training data without any explicit effort to control model complexity, these methods exhibit excellent predictive accuracy. We conjecture that specific principles underlie these phenomena: that overparametrization allows gradient methods to find interpolating solutions, that these methods implicitly impose regularization, and that overparametrization leads to benign overfitting. We survey recent theoretical progress that provides examples illustrating these principles in simpler settings. We first review classical uniform convergence results and why they fall short of explaining aspects of the behavior of deep learning methods. We give examples of implicit regularization in simple settings, where gradient methods lead to minimal norm functions that perfectly fit the training data. Then we review prediction methods that exhibit benign overfitting, focusing on regression problems with quadratic loss. For these methods, we can decompose the prediction rule into a simple component that is useful for prediction and a spiky component that is useful for overfitting but, in a favorable setting, does not harm prediction accuracy. We focus specifically on the linear regime for neural networks, where the network can be approximated by a linear model. In this regime, we demonstrate the success of gradient flow, and we consider benign overfitting with two-layer networks, giving an exact asymptotic analysis that precisely demonstrates the impact of overparametrization. We conclude by highlighting the key challenges that arise in extending these insights to realistic deep learning settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ALVWCRPZ/Bartlett et al. - 2021 - Deep learning a statistical viewpoint.pdf;/Users/antoniohortaribeiro/Zotero/storage/6JBV8IIG/2103.html}
}

@inproceedings{bartlett_spectrallynormalized_2017,
  title = {Spectrally-Normalized Margin Bounds for Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.}
}

@article{basu_regularized_2015,
  title = {Regularized Estimation in Sparse High-Dimensional Time Series Models},
  author = {Basu, Sumanta and Michailidis, George},
  year = {2015},
  month = aug,
  journal = {The Annals of Statistics},
  volume = {43},
  number = {4},
  eprint = {1311.4175},
  pages = {1535--1567},
  issn = {0090-5364},
  doi = {10.1214/15-AOS1315},
  abstract = {Many scientific and economic problems involve the analysis of high-dimensional time series datasets. However, theoretical studies in high-dimensional statistics to date rely primarily on the assumption of independent and identically distributed (i.i.d.) samples. In this work, we focus on stable Gaussian processes and investigate the theoretical properties of \${\textbackslash}ell \_1\$-regularized estimates in two important statistical problems in the context of high-dimensional time series: (a) stochastic regression with serially correlated errors and (b) transition matrix estimation in vector autoregressive (VAR) models. We derive nonasymptotic upper bounds on the estimation errors of the regularized estimates and establish that consistent estimation under high-dimensional scaling is possible via \${\textbackslash}ell\_1\$-regularization for a large class of stable processes under sparsity constraints. A key technical contribution of the work is to introduce a measure of stability for stationary processes using their spectral properties that provides insight into the effect of dependence on the accuracy of the regularized estimates. With this proposed stability measure, we establish some useful deviation bounds for dependent data, which can be used to study several important regularized estimates in a time series setting.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RDIDVH2K/basu_regularize_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/GR2U6A4S/1311.html}
}

@article{batselier_canonical_2014,
  title = {The {{Canonical Decomposition}} of \${\textbackslash}mathcal\{\vphantom\}{{C}}\vphantom\{\}{\textasciicircum}n\_d\$ and {{Numerical Gr{\"o}bner}} and {{Border Bases}}},
  author = {Batselier, K. and Dreesen, P. and De Moor, B.},
  year = {2014},
  month = jan,
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {35},
  number = {4},
  pages = {1242--1264},
  issn = {0895-4798},
  doi = {10.1137/130927176},
  abstract = {This article introduces the canonical decomposition of the vector space of multivariate polynomials for a given monomial ordering. Its importance lies in solving multivariate polynomial systems, computing Gr{\"o}bner bases, and solving the ideal membership problem. An SVD-based algorithm is presented that numerically computes the canonical decomposition. It is then shown how, by introducing the notion of divisibility into this algorithm, a numerical Gr{\"o}bner basis can also be computed. In addition, we demonstrate how the canonical decomposition can be used to decide whether the affine solution set of a multivariate polynomial system is zero-dimensional and to solve the ideal membership problem numerically. The SVD-based canonical decomposition algorithm is also extended to numerically compute border bases. A tolerance for each of the algorithms is derived using perturbation theory of principal angles. This derivation shows that the condition number of computing the canonical decomposition and numerical Gr{\"o}bner basis is essentially the condition number of the Macaulay matrix. Numerical experiments with both exact and noisy coefficients are presented and discussed.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NY4YFYJH/130927176.html;/Users/antoniohortaribeiro/Zotero/storage/WVQGCFFG/130927176.html}
}

@article{batselier_geometry_2013,
  title = {The {{Geometry}} of {{Multivariate Polynomial Division}} and {{Elimination}}},
  author = {Batselier, K. and Dreesen, P. and Moor, B.},
  year = {2013},
  month = jan,
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {34},
  number = {1},
  pages = {102--125},
  issn = {0895-4798},
  doi = {10.1137/120863782},
  abstract = {Multivariate polynomials are usually discussed in the framework of algebraic geometry. Solving problems in algebraic geometry usually involves the use of a Gr{\"o}bner basis. This article shows that linear algebra without any Gr{\"o}bner basis computation suffices to solve basic problems from algebraic geometry by describing three operations: multiplication, division, and elimination. This linear algebra framework will also allow us to give a geometric interpretation. Multivariate division will involve oblique projections, and a link between elimination and principal angles between subspaces (CS decomposition) is revealed. The main computational tool in this approach is the QR decomposition.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8V45CBB4/120863782.html;/Users/antoniohortaribeiro/Zotero/storage/9JGQICKX/120863782.html}
}

@article{battiti_first_1992,
  title = {First- and {{Second-Order Methods}} for {{Learning}}: {{Between Steepest Descent}} and {{Newton}}'s {{Method}}},
  shorttitle = {First- and {{Second-Order Methods}} for {{Learning}}},
  author = {Battiti, R.},
  year = {1992},
  month = mar,
  journal = {Neural Computation},
  volume = {4},
  number = {2},
  pages = {141--166},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.2.141},
  abstract = {On-line first-order backpropagation is sufficiently fast and effective for many large-scale classification problems but for very high precision mappings, batch processing may be the method of choice. This paper reviews first- and second-order optimization methods for learning in feedforward neural networks. The viewpoint is that of optimization: many methods can be cast in the language of optimization techniques, allowing the transfer to neural nets of detailed results about computational complexity and safety procedures to ensure convergence and to avoid numerical problems. The review is not intended to deliver detailed prescriptions for the most appropriate methods in specific applications, but to illustrate the main characteristics of the different methods and their mutual relations.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WAICJQE3/battiti_first-_1992.pdf;/Users/antoniohortaribeiro/Zotero/storage/SNGDRXWQ/6796060.html;/Users/antoniohortaribeiro/Zotero/storage/XXHUEQVJ/6796060.html}
}

@book{bauschke_convex_2011,
  title = {Convex {{Analysis}} and {{Monotone Operator Theory}} in {{Hilbert Spaces}}},
  author = {Bauschke, Heinz H. and Combettes, Patrick L.},
  year = {2011},
  series = {{{CMS Books}} in {{Mathematics}}},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4419-9467-7},
  urldate = {2023-03-21},
  isbn = {978-1-4419-9466-0 978-1-4419-9467-7},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9WQ6UIYQ/Bauschke_Combettes_2011_Convex Analysis and Monotone Operator Theory in Hilbert Spaces.pdf}
}

@techreport{beale_neural_2017,
  title = {Neural Network Toolbox for Use with {{MATLAB}}},
  author = {Beale, Mark Hudson and Hagan, Martin T. and Demuth, Howard B.},
  year = {2017},
  institution = {Mathworks}
}

@article{beck_fast_2009,
  title = {A {{Fast Iterative Shrinkage-Thresholding Algorithm}} for {{Linear Inverse Problems}}},
  author = {Beck, Amir and Teboulle, Marc},
  year = {2009},
  month = jan,
  journal = {SIAM Journal on Imaging Sciences},
  volume = {2},
  number = {1},
  pages = {183--202},
  issn = {1936-4954},
  doi = {10.1137/080716542},
  urldate = {2024-01-16},
  abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KV6I465K/Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf}
}

@article{beck_protecting_2016,
  title = {Protecting the Confidentiality and Security of Personal Health Information in Low- and Middle-Income Countries in the Era of {{SDGs}} and {{Big Data}}},
  author = {Beck, Eduard J. and Gill, Wayne and De Lay, Paul R.},
  year = {2016},
  journal = {Global Health Action},
  volume = {9},
  pages = {32089},
  issn = {1654-9880},
  abstract = {BACKGROUND: As increasing amounts of personal information are being collected through a plethora of electronic modalities by statutory and non-statutory organizations, ensuring the confidentiality and security of such information has become a major issue globally. While the use of many of these media can be beneficial to individuals or populations, they can also be open to abuse by individuals or statutory and non-statutory organizations. Recent examples include collection of personal information by national security systems and the development of national programs like the Chinese Social Credit System. In many low- and middle-income countries, an increasing amount of personal health information is being collected. The collection of personal health information is necessary, in order to develop longitudinal medical records and to monitor and evaluate the use, cost, outcome, and impact of health services at facility, sub-national, and national levels. However, if personal health information is not held confidentially and securely, individuals with communicable or non-communicable diseases (NCDs) may be reluctant to use preventive or therapeutic health services, due to fear of being stigmatized or discriminated against. While policymakers and other stakeholders in these countries recognize the need to develop and implement policies for protecting the privacy, confidentiality and security of personal health information, to date few of these countries have developed, let alone implemented, coherent policies. The global HIV response continues to emphasize the importance of collecting HIV-health information, recently re-iterated by the Fast Track to End AIDS by 2030 program and the recent changes in the Guidelines on When to Start Antiretroviral Therapy and on Pre-exposure Prophylaxis for HIV. The success of developing HIV treatment cascades in low- and middle-income countries will require the development of National Health Identification Systems. The success of programs like Universal Health Coverage, under the recently ratified Sustainable Development Goals is also contingent on the availability of personal health information for communicable and non-communicable diseases. DESIGN: Guidance for countries to develop and implement their own guidelines for protecting HIV-information formed the basis of identifying a number of fundamental principles, governing the areas of privacy, confidentiality and security. The use of individual-level data must balance maximizing the benefits from their most effective and fullest use, and minimizing harm resulting from their malicious or inadvertent release. DISCUSSION: These general principles are described in this paper, as along with a bibliography referring to more detailed technical information. A country assessment tool and user's manual, based on these principles, have been developed to support countries to assess the privacy, confidentiality, and security of personal health information at facility, data warehouse/repository, and national levels. The successful development and implementation of national guidance will require strong collaboration at local, regional, and national levels, and this is a pre-condition for the successful implementation of a range of national and global programs. CONCLUSION: This paper is a call for action for stakeholders in low- and middle-income countries to develop and implement such coherent policies and provides fundamental principles governing the areas of privacy, confidentiality, and security of personal health information being collected in low- and middle-income countries.},
  langid = {english},
  pmcid = {PMC5123209},
  pmid = {27885972},
  keywords = {Big Data,confidentiality,discrimination,ethics,personal health information,privacy laws,SDGs,security,stigma}
}

@book{beekmans_linux_2012,
  title = {Linux {{From Scratch}}},
  author = {Beekmans, G.},
  year = {2012},
  publisher = {Lulu Press, Incorporated},
  isbn = {978-1-300-01983-1},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BXXZTDQN/beekmans_linux_2012.pdf}
}

@article{bejnordi_diagnostic_2017,
  title = {Diagnostic {{Assessment}} of {{Deep Learning Algorithms}} for {{Detection}} of {{Lymph Node Metastases}} in {{Women With Breast Cancer}}},
  author = {Bejnordi, Babak Ehteshami and Veta, Mitko and {Johannes van Diest}, Paul and {van Ginneken}, Bram and Karssemeijer, Nico and Litjens, Geert and {van der Laak}, Jeroen A. W. M. and {and the CAMELYON16 Consortium} and Hermsen, Meyke and Manson, Quirine F and Balkenhol, Maschenka and Geessink, Oscar and Stathonikos, Nikolaos and {van Dijk}, Marcory CRF and Bult, Peter and Beca, Francisco and Beck, Andrew H and Wang, Dayong and Khosla, Aditya and Gargeya, Rishab and Irshad, Humayun and Zhong, Aoxiao and Dou, Qi and Li, Quanzheng and Chen, Hao and Lin, Huang-Jing and Heng, Pheng-Ann and Ha{\ss}, Christian and Bruni, Elia and Wong, Quincy and Halici, Ugur and {\"O}ner, Mustafa {\"U}mit and {Cetin-Atalay}, Rengul and Berseth, Matt and Khvatkov, Vitali and Vylegzhanin, Alexei and Kraus, Oren and Shaban, Muhammad and Rajpoot, Nasir and Awan, Ruqayya and Sirinukunwattana, Korsuk and Qaiser, Talha and Tsang, Yee-Wah and Tellez, David and Annuscheit, Jonas and Hufnagl, Peter and Valkonen, Mira and Kartasalo, Kimmo and Latonen, Leena and Ruusuvuori, Pekka and Liimatainen, Kaisa and Albarqouni, Shadi and Mungal, Bharti and George, Ami and Demirci, Stefanie and Navab, Nassir and Watanabe, Seiryo and Seno, Shigeto and Takenaka, Yoichi and Matsuda, Hideo and Ahmady Phoulady, Hady and Kovalev, Vassili and Kalinovsky, Alexander and Liauchuk, Vitali and Bueno, Gloria and {Fernandez-Carrobles}, M. Milagro and Serrano, Ismael and Deniz, Oscar and Racoceanu, Daniel and Ven{\^a}ncio, Rui},
  year = {2017},
  month = dec,
  journal = {JAMA},
  volume = {318},
  number = {22},
  pages = {2199},
  issn = {0098-7484},
  doi = {10.1001/jama.2017.14585},
  urldate = {2017-12-13},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BACNA43B/ehteshami bejnordi_diagnostic_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/LWYNHURU/bejnordi_diagnostic_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/5AAGUWZT/2665774.html}
}

@article{belkin_reconciling_2019,
  title = {Reconciling Modern Machine-Learning Practice and the Classical Bias--Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year = {2019},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {32},
  pages = {15849--15854},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1903070116},
  urldate = {2020-08-07},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias--variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias--variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This ``double-descent'' curve subsumes the textbook U-shaped bias--variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7F6EFE3P/Belkin et al. - 2019 - Reconciling modern machine-learning practice and t.pdf;/Users/antoniohortaribeiro/Zotero/storage/HBUZP4F9/pnas.1903070116.sapp.pdf;/Users/antoniohortaribeiro/Zotero/storage/VKTT2276/Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf}
}

@article{belkin_two_2020,
  title = {Two {{Models}} of {{Double Descent}} for {{Weak Features}}},
  author = {Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  year = {2020},
  month = jan,
  journal = {SIAM Journal on Mathematics of Data Science},
  volume = {2},
  number = {4},
  eprint = {1903.07571},
  pages = {1167--1180},
  doi = {10.1137/20M1336072},
  abstract = {The ``double descent'' risk curve was proposed to qualitatively describe the out-of-sample prediction accuracy of variably parameterized machine learning models. This article provides a precise mathematical analysis for the shape of this curve in two simple data models with the least squares/least norm predictor. Specifically, it is shown that the risk peaks when the number of features \$p\$ is close to the sample size \$n\$ but also that the risk sometimes decreases toward its minimum as \$p\$ increases beyond \$n\$. This behavior parallels some key patterns observed in large models, including modern neural networks, and is contrasted with that of ``prescient'' models that select features in an a priori optimal order.},
  archiveprefix = {arXiv},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BK268TXU/Belkin et al. - 2020 - Two Models of Double Descent for Weak Features.pdf;/Users/antoniohortaribeiro/Zotero/storage/JJVK7QWV/Belkin et al. - 2020 - Two models of double descent for weak features.pdf;/Users/antoniohortaribeiro/Zotero/storage/ZDEFDAG6/20M1336072.html}
}

@article{belkin_understand_2018,
  title = {To Understand Deep Learning We Need to Understand Kernel Learning},
  author = {Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  year = {2018},
  month = jun,
  journal = {arXiv:1802.01396 [cs, stat]},
  eprint = {1802.01396},
  primaryclass = {cs, stat},
  urldate = {2020-08-07},
  abstract = {Generalization performance of classifiers in deep learning has recently become a subject of intense study. Deep models, typically over-parametrized, tend to fit the training data exactly. Despite this "overfitting", they perform well on test data, a phenomenon not yet fully understood. The first point of our paper is that strong performance of overfitted classifiers is not a unique feature of deep learning. Using six real-world and two synthetic datasets, we establish experimentally that kernel machines trained to have zero classification or near zero regression error perform very well on test data, even when the labels are corrupted with a high level of noise. We proceed to give a lower bound on the norm of zero loss solutions for smooth kernels, showing that they increase nearly exponentially with data size. We point out that this is difficult to reconcile with the existing generalization bounds. Moreover, none of the bounds produce non-trivial results for interpolating solutions. Second, we show experimentally that (non-smooth) Laplacian kernels easily fit random labels, a finding that parallels results for ReLU neural networks. In contrast, fitting noisy data requires many more epochs for smooth Gaussian kernels. Similar performance of overfitted Laplacian and Gaussian classifiers on test, suggests that generalization is tied to the properties of the kernel function rather than the optimization process. Certain key phenomena of deep learning are manifested similarly in kernel methods in the modern "overfitted" regime. The combination of the experimental and theoretical results presented in this paper indicates a need for new theoretical ideas for understanding properties of classical kernel methods. We argue that progress on understanding deep learning will be difficult until more tractable "shallow" kernel methods are better understood.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8FLVDSPD/Belkin et al. - 2018 - To understand deep learning we need to understand .pdf;/Users/antoniohortaribeiro/Zotero/storage/PH7MXX5Y/1802.html}
}

@article{belloni_squareroot_2011,
  title = {Square-{{Root Lasso}}: {{Pivotal Recovery}} of {{Sparse Signals}} via {{Conic Programming}}},
  shorttitle = {Square-{{Root Lasso}}},
  author = {Belloni, Alexandre and Chernozhukov, Victor and Wang, Lie},
  year = {2011},
  month = dec,
  journal = {Biometrika},
  volume = {98},
  number = {4},
  eprint = {1009.5689},
  primaryclass = {math, stat},
  pages = {791--806},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asr043},
  urldate = {2022-10-10},
  abstract = {We propose a pivotal method for estimating high-dimensional sparse linear regression models, where the overall number of regressors \$p\$ is large, possibly much larger than \$n\$, but only \$s\$ regressors are significant. The method is a modification of the lasso, called the square-root lasso. The method is pivotal in that it neither relies on the knowledge of the standard deviation \${\textbackslash}sigma\$ or nor does it need to pre-estimate \${\textbackslash}sigma\$. Moreover, the method does not rely on normality or sub-Gaussianity of noise. It achieves near-oracle performance, attaining the convergence rate \${\textbackslash}sigma {\textbackslash}\{(s/n){\textbackslash}log p{\textbackslash}\}{\textasciicircum}\{1/2\}\$ in the prediction norm, and thus matching the performance of the lasso with known \${\textbackslash}sigma\$. These performance results are valid for both Gaussian and non-Gaussian errors, under some mild moment restrictions. We formulate the square-root lasso as a solution to a convex conic programming problem, which allows us to implement the estimator using efficient algorithmic methods, such as interior-point and first-order methods.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KCIVXVNG/Belloni et al_2011_Square-Root Lasso.pdf;/Users/antoniohortaribeiro/Zotero/storage/R29L7UPB/1009.html}
}

@article{bengio_learning_1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  author = {Bengio, Y. and Simard, P. and Frasconi, P.},
  year = {1994},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {2},
  pages = {157--166},
  issn = {1045-9227},
  doi = {10.1109/72.279181},
  abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered},
  keywords = {Computer networks,Cost function,Delay effects,Discrete transforms,Displays,efficient learning,gradient descent,input/output sequence mapping,Intelligent networks,learning (artificial intelligence),long-term dependencies,Neural networks,Neurofeedback,numerical analysis,prediction problems,Production,production problems,recognition,recurrent neural nets,recurrent neural network training,Recurrent neural networks,temporal contingencies},
  file = {/Users/antoniohortaribeiro/Zotero/storage/STQ8AISP/bengio_learning_1994.pdf;/Users/antoniohortaribeiro/Zotero/storage/QTPUS9GW/279181.html}
}

@article{bengio_neural_2003,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  year = {2003},
  journal = {Journal of machine learning research},
  volume = {3},
  number = {Feb},
  pages = {1137--1155},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/T2XE5ETG/bengio_a neural_2003.pdf}
}

@inproceedings{bengio_problem_1993,
  title = {The Problem of Learning Long-Term Dependencies in Recurrent Networks},
  booktitle = {{{IEEE International Conference}} on {{Neural Networks}}},
  author = {Bengio, Y. and Frasconi, P. and Simard, P.},
  year = {1993},
  month = mar,
  pages = {1183-1188 vol.3},
  doi = {10/d7zs24},
  abstract = {The authors seek to train recurrent neural networks in order to map input sequences to output sequences, for applications in sequence recognition or production. Results are presented showing that learning long-term dependencies in such recurrent networks using gradient descent is a very difficult task. It is shown how this difficulty arises when robustly latching bits of information with certain attractors. The derivatives of the output at time t with respect to the unit activations at time zero tend rapidly to zero as t increases for most input values. In such a situation, simple gradient descent techniques appear inappropriate. The consideration of alternative optimization methods and architectures is suggested.{$<<$}ETX{$>>$}},
  keywords = {Background noise,Discrete transforms,gradient descent,input sequences,Intelligent networks,learning (artificial intelligence),long-term dependencies,neural networks,Neural networks,optimization methods,Optimization methods,output sequences,Production,recurrent networks,recurrent neural nets,Recurrent neural networks,Robustness,sequence recognition,Speech,Text recognition,unit activations},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UUCD2TGC/bengio_the_1993.pdf;/Users/antoniohortaribeiro/Zotero/storage/2ATQIMYL/298725.html}
}

@inproceedings{bensrhair_stereo_2002,
  title = {Stereo Vision-Based Feature Extraction for Vehicle Detection},
  booktitle = {Intelligent {{Vehicle Symposium}}, 2002. {{IEEE}}},
  author = {Bensrhair, Abdelaziz and Bertozzi, Massimo and Broggi, Alberto and Fascioli, Alessandra and Mousset, Stephane and Toulminet, Gwenadelle},
  year = {2002},
  volume = {2},
  pages = {465--470},
  publisher = {IEEE}
}

@article{berger_formal_2009,
  title = {The Formal Definition of Reference Priors},
  author = {Berger, James O. and Bernardo, Jos{\'e} M. and Sun, Dongchu},
  year = {2009},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {37},
  number = {2},
  eprint = {0904.0156},
  pages = {905--938},
  issn = {0090-5364},
  doi = {10.1214/07-AOS587},
  urldate = {2018-10-07},
  abstract = {Reference analysis produces objective Bayesian inference, in the sense that inferential statements depend only on the assumed model and the available data, and the prior distribution used to make an inference is least informative in a certain information-theoretic sense. Reference priors have been rigorously defined in specific contexts and heuristically defined in general, but a rigorous general definition has been lacking. We produce a rigorous general definition here and then show how an explicit expression for the reference prior can be obtained under very weak regularity conditions. The explicit expression can be used to derive new reference priors both analytically and numerically.},
  archiveprefix = {arXiv},
  keywords = {{62F15 (Primary) 62A01, 62B10 (Secondary)},Mathematics - Statistics Theory},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Y26ACAXG/berger_the_2009.pdf;/Users/antoniohortaribeiro/Zotero/storage/MK2ECVY8/0904.html}
}

@article{bern_chagas_2015,
  title = {Chagas' {{Disease}}},
  author = {Bern, Caryn},
  year = {2015},
  month = jul,
  journal = {The New England Journal of Medicine},
  volume = {373},
  number = {5},
  pages = {456--466},
  issn = {1533-4406},
  doi = {10.1056/NEJMra1410150},
  langid = {english},
  pmid = {26222561},
  keywords = {Chagas Disease,Humans,Life Cycle Stages,Prevalence,Trypanocidal Agents,Trypanosoma cruzi}
}

@article{berrevoets_causal_2024,
  title = {Causal {{Deep Learning}}: {{Encouraging Impact}} on {{Real-world Problems Through Causality}}},
  shorttitle = {Causal {{Deep Learning}}},
  author = {Berrevoets, Jeroen and Kacprzyk, Krzysztof and Qian, Zhaozhi and van der Schaar, Mihaela},
  year = {2024},
  month = jul,
  journal = {Foundations and Trends{\textregistered} in Signal Processing},
  volume = {18},
  number = {3},
  pages = {200--309},
  publisher = {Now Publishers, Inc.},
  issn = {1932-8346, 1932-8354},
  doi = {10.1561/2000000123},
  urldate = {2024-08-18},
  abstract = {Causal Deep Learning: Encouraging Impact on Real-world Problems Through Causality},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7UCBMG9C/2303.02186v2.pdf;/Users/antoniohortaribeiro/Zotero/storage/TWKNGEM5/Berrevoets et al. - 2024 - Causal Deep Learning Encouraging Impact on Real-w.pdf}
}

@article{bert_phantom_2005,
  title = {A Phantom Evaluation of a Stereo-Vision Surface Imaging System for Radiotherapy Patient Setup},
  author = {Bert, Christoph and Metheany, Katherine G and Doppke, Karen and Chen, George TY},
  year = {2005},
  journal = {Medical physics},
  volume = {32},
  number = {9},
  pages = {2753--2762},
  doi = {10.1118/1.1984263}
}

@article{bertozzi_gold_1998,
  title = {{{GOLD}}: {{A}} Parallel Real-Time Stereo Vision System for Generic Obstacle and Lane Detection},
  author = {Bertozzi, Massimo and Broggi, Alberto},
  year = {1998},
  journal = {Image Processing, IEEE Transactions on},
  volume = {7},
  number = {1},
  pages = {62--81},
  doi = {10.1109/83.650851}
}

@book{bertsekas_convex_2003,
  title = {Convex {{Analysis}} and {{Optimization}}},
  author = {Bertsekas, Dimitri P and Nedi, Angelia and Ozdaglar, Asuman E},
  year = {2003},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HCB7H9EA/bertsekas_convex_2003.pdf}
}

@book{bertsekas_nonlinear_1999,
  title = {Nonlinear Programming},
  author = {Bertsekas, Dimitri P},
  year = {1999},
  publisher = {Athena scientific Belmont},
  isbn = {1-886529-00-0},
  file = {/Users/antoniohortaribeiro/Zotero/storage/R5XJETRG/bertsekas_nonlinear_1999.pdf}
}

@book{bertsimas_introduction_1997,
  title = {Introduction to Linear Optimization},
  author = {Bertsimas, Dimitris and Tsitsiklis, John N},
  year = {1997},
  volume = {6},
  publisher = {Athena Scientific Belmont, MA},
  file = {/Users/antoniohortaribeiro/Zotero/storage/623RIAKI/bertsimas_introducti_1997.pdf}
}

@article{betancourt_conceptual_2017,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2017},
  month = jan,
  journal = {arXiv:1701.02434 [stat]},
  eprint = {1701.02434},
  primaryclass = {stat},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Statistics - Methodology},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KNVGPEV3/betancourt_a_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/CGHFI3P7/1701.html}
}

@article{bhagoji_lower_2019,
  title = {Lower {{Bounds}} on {{Adversarial Robustness}} from {{Optimal Transport}}},
  author = {Bhagoji, Arjun Nitin and Cullina, Daniel and Mittal, Prateek},
  year = {2019},
  journal = {Advances in Neural Information Processing Systems},
  volume = {32},
  urldate = {2021-05-16},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/J6CMH72Z/Bhagoji et al. - 2019 - Lower Bounds on Adversarial Robustness from Optima.pdf;/Users/antoniohortaribeiro/Zotero/storage/Y7M4D2AY/02bf86214e264535e3412283e817deaa-Abstract.html}
}

@inproceedings{bhojanapalli_lowrank_2020,
  title = {Low-{{Rank Bottleneck}} in {{Multi-head Attention Models}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Bhojanapalli, Srinadh and Yun, Chulhee and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  year = {2020},
  month = nov,
  pages = {864--873},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2021-11-11},
  abstract = {Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. Unfortunately, this leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the current architecture gives rise to a low-rank bottleneck in attention heads, causing this limitation. We further validate this in our experiments. As a solution we propose to set the head size of an attention unit to input sequence length, and independent of the number of heads, resulting in multi-head attention layers with provably more expressive power. We empirically show that this allows us to train models with a relatively smaller embedding dimension and with better performance scaling.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3CZJSBDF/Bhojanapalli et al_2020_Low-Rank Bottleneck in Multi-head Attention Models.pdf;/Users/antoniohortaribeiro/Zotero/storage/YEK7XA6M/Bhojanapalli et al. - 2020 - Low-Rank Bottleneck in Multi-head Attention Models.pdf}
}

@incollection{biau_highdimensional_2015,
  title = {High-{{Dimensional}} p-{{Norms}}},
  booktitle = {Mathematical Statistics and Limit Theorems},
  author = {Biau, G{\'e}rard and Mason, David M},
  year = {2015},
  pages = {21--40},
  publisher = {Springer},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RQ4WNKII/b-dm.pdf}
}

@article{biggio_wild_2018,
  title = {Wild Patterns: {{Ten}} Years after the Rise of Adversarial Machine Learning},
  shorttitle = {Wild Patterns},
  author = {Biggio, Battista and Roli, Fabio},
  year = {2018},
  month = dec,
  journal = {Pattern Recognition},
  volume = {84},
  pages = {317--331},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2018.07.023},
  urldate = {2021-05-15},
  abstract = {Learning-based pattern classifiers, including deep networks, have shown impressive performance in several application domains, ranging from computer vision to cybersecurity. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to such wild patterns (also referred to as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this research area over the last ten years and beyond, starting from pioneering, earlier work on the security of non-deep learning algorithms up to more recent work aimed to understand the security properties of deep learning algorithms, in the context of computer vision and cybersecurity tasks. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the security evaluation of machine-learning algorithms. We review the main threat models and attacks defined to this end, and discuss the main limitations of current work, along with the corresponding future challenges towards the design of more secure learning algorithms.},
  langid = {english},
  keywords = {Adversarial examples,Adversarial machine learning,Deep learning,Evasion attacks,Poisoning attacks,Secure learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8N9GKNTQ/Biggio and Roli - 2018 - Wild patterns Ten years after the rise of adversa.pdf;/Users/antoniohortaribeiro/Zotero/storage/GCUTJK7Y/S0031320318302565.html}
}

@article{billings_correlation_1986,
  title = {Correlation Based Model Validity Tests for Non-Linear Models},
  author = {Billings, S. A. and Voon, W. S. F.},
  year = {1986},
  journal = {International journal of Control},
  volume = {44},
  number = {1},
  pages = {235--244},
  doi = {10.1080/00207179108934155}
}

@article{billings_correlation_1986a,
  title = {Correlation {{Based Model Validity Tests}} for {{Non-Linear Models}}},
  author = {Billings, S. A. and Voon, W. S. F.},
  year = {1986},
  journal = {International journal of Control},
  volume = {44},
  number = {1},
  pages = {235--244},
  doi = {10/dkj53r},
  annotation = {00002}
}

@article{billings_identification_1989,
  title = {Identification of {{MIMO}} Non-Linear Systems Using a Forward-Regression Orthogonal Estimator},
  author = {Billings, S. A. and Chen, S and Korenberg, M. J.},
  year = {1989},
  journal = {International Journal of Control},
  volume = {49},
  number = {6},
  pages = {2157--2189},
  doi = {10.1080/00207178908559767},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2C7EHU54/billings_identifica_1989.pdf;/Users/antoniohortaribeiro/Zotero/storage/DSDARTUC/billings_identifica_1989.pdf}
}

@article{billings_nonlinear_1994,
  title = {Nonlinear Model Validation Using Correlation Tests},
  author = {Billings, S. A. and Zhu, Q. M.},
  year = {1994},
  journal = {International Journal of Control},
  volume = {60},
  number = {6},
  pages = {1107--1120},
  doi = {10.1080/00207179408921513}
}

@book{billings_nonlinear_2013,
  title = {Nonlinear System Identification: {{NARMAX}} Methods in the Time, Frequency, and Spatio-Temporal Domains},
  shorttitle = {Nonlinear System Identification},
  author = {Billings, S. A.},
  year = {2013},
  publisher = {Wiley},
  address = {Chichester, West Sussex, United Kingdom},
  isbn = {978-1-119-94359-4},
  lccn = {QA402.5 .B55 2013},
  keywords = {Mathematical models,nonlinear systems,Nonlinear theories,Systems engineering},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XG8XDNEF/billings_nonlinear_2013.pdf}
}

@article{birchfield_depth_1999,
  title = {Depth Discontinuities by Pixel-to-Pixel Stereo},
  author = {Birchfield, Stan and Tomasi, Carlo},
  year = {1999},
  journal = {International Journal of Computer Vision},
  volume = {35},
  number = {3},
  pages = {269--293},
  doi = {10.1023/A:1008160311296}
}

@article{birodkar_closedform_2019,
  title = {A {{Closed-Form Learned Pooling}} for {{Deep Classification Networks}}},
  author = {Birodkar, Vighnesh and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.03808 [cs, stat]},
  eprint = {1906.03808},
  primaryclass = {cs, stat},
  urldate = {2020-03-23},
  abstract = {In modern computer vision tasks, convolutional neural networks (CNNs) are indispensable for image classification tasks due to their efficiency and effectiveness. Part of their superiority compared to other architectures, comes from the fact that a single, local filter is shared across the entire image. However, there are scenarios where we may need to treat spatial locations in non-uniform manner. We see this in nature when considering how humans have evolved foveation to process different areas in their field of vision with varying levels of detail. In this paper we propose a way to enable CNNs to learn different pooling weights for each pixel location. We do so by introducing an extended definition of a pooling operator. This operator can learn a strict super-set of what can be learned by average pooling or convolutions. It has the benefit of being shared across feature maps and can be encouraged to be local or diffuse depending on the data. We show that for fixed network weights, our pooling operator can be computed in closed-form by spectral decomposition of matrices associated with class separability. Through experiments, we show that this operator benefits generalization for ResNets and CNNs on the CIFAR-10, CIFAR-100 and SVHN datasets and improves robustness to geometric corruptions and perturbations on the CIFAR-10-C and CIFAR-10-P test sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KBIN93S9/Birodkar et al. - 2019 - A Closed-Form Learned Pooling for Deep Classificat.pdf;/Users/antoniohortaribeiro/Zotero/storage/5D8H7GII/1906.html}
}

@book{bishop_pattern_2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-31073-2},
  lccn = {Q327 .B52 2006},
  keywords = {machine learning,Pattern perception},
  file = {/Users/antoniohortaribeiro/Zotero/storage/I389ZEKB/bishop_pattern_2006.pdf}
}

@article{biton_atrial_2021,
  title = {Atrial Fibrillation Risk Prediction from the 12-Lead {{ECG}} Using Digital Biomarkers and Deep Representation Learning},
  author = {Biton, Shany and Gendelman, Sheina and Ribeiro, Ant{\^o}nio H and Miana, Gabriela and Moreira, Carla and Ribeiro, Antonio Luiz P and Behar, Joachim A},
  year = {2021},
  journal = {European Heart Journal - Digital Health},
  issn = {2634-3916},
  doi = {10.1093/ehjdh/ztab071},
  urldate = {2021-08-10},
  abstract = {This study aims to assess whether information derived from the raw 12-lead electrocardiogram (ECG) combined with clinical information is predictive of atrial fibrillation (AF) development.We use a subset of the Telehealth Network of Minas Gerais (TNMG) database consisting of patients that had repeated 12-lead ECG measurements between 2010-2017 that is 1,130,404 recordings from 415,389 unique patients. Median and interquartile of age for the recordings were 58 (46-69) and 38\% of the patients were males. Recordings were assigned to train-validation and test sets in an 80:20\% split which was stratified by class, age and gender. A random forest classifier was trained to predict, for a given recording, the risk of AF development within 5-years. We use features obtained from different modalities, namely demographics, clinical information, engineered features, and features from deep representation learning.The best model performance on the test set was obtained for the model combining features from all modalities with an AUROC=0.909 against the best single modality model which had an AUROC=0.839.Our study has important clinical implications for AF management. It is the first study integrating feature engineering, deep learning and EMR metadata to create a risk prediction tool for the management of patients at risk of AF. The best model that includes features from all modalities demonstrates that human knowledge in electrophysiology combined with deep learning outperforms any single modality approach. The high performance obtained suggest that structural changes in the 12-lead ECG are associated with existing or impending AF.},
  copyright = {All rights reserved}
}

@article{bjorck_understanding_2018,
  title = {Understanding {{Batch Normalization}}},
  author = {Bjorck, Johan and Gomes, Carla and Selman, Bart and Weinberger, Kilian Q.},
  year = {2018},
  month = may,
  journal = {arXiv:1806.02375 [cs, stat]},
  eprint = {1806.02375},
  primaryclass = {cs, stat},
  urldate = {2018-12-13},
  abstract = {Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. Yet, despite its enormous success, there remains little consensus on the exact reason and mechanism behind these improvements. In this paper we take a step towards a better understanding of BN, following an empirical approach. We conduct several experiments, and show that BN primarily enables training with larger learning rates, which is the cause for faster convergence and better generalization. For networks without BN we demonstrate how large gradient updates can result in diverging loss and activations growing uncontrollably with network depth, which limits possible learning rates. BN avoids this problem by constantly correcting activations to be zero-mean and of unit standard deviation, which enables larger gradient steps, yields faster convergence and may help bypass sharp local minima. We further show various ways in which gradients and activations of deep unnormalized networks are ill-behaved. We contrast our results against recent findings in random matrix theory, shedding new light on classical initialization schemes and their consequences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XIAG97JU/bjorck_understand_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/YI2PRIK6/1806.html}
}

@article{blei_variational_2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  eprint = {1601.00670},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10/gb2dc6},
  urldate = {2019-01-10},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CDR68TA5/blei_variationa_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/KEZI2VJG/1601.html}
}

@misc{blog_understanding_,
  title = {Understanding the {{Neural Tangent Kernel}}},
  author = {Blog, Rajat's},
  urldate = {2020-08-09},
  abstract = {My attempt at distilling the ideas behind the neural tangent kernel that is making waves in recent theoretical deep learning research.},
  howpublished = {https://rajatvd.github.io/NTK/},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TLGZVRAT/NTK.html}
}

@article{bock_multiple_1984,
  title = {A Multiple Shooting Algorithm for Direct Solution of Optimal Control Problems},
  author = {Bock, Hans Georg and Plitt, Karl-Josef},
  year = {1984},
  journal = {IFAC Proceedings Volumes},
  volume = {17},
  number = {2},
  pages = {1603--1608},
  issn = {1474-6670},
  doi = {10.1016/S1474-6670(17)61205-9}
}

@article{bock_multiple_1984a,
  title = {A {{Multiple Shooting Algorithm}} for {{Direct Solution}} of {{Optimal Control Problems}}},
  author = {Bock, Hans Georg and Plitt, Karl-Josef},
  year = {1984},
  journal = {IFAC Proceedings Volumes},
  volume = {17},
  number = {2},
  pages = {1603--1608},
  issn = {1474-6670},
  doi = {10/gfjwmq}
}

@incollection{bock_numerical_1981,
  title = {Numerical Treatment of Inverse Problems in Chemical Reaction Kinetics},
  booktitle = {Modelling of Chemical Reaction Systems},
  author = {Bock, Hans Georg},
  year = {1981},
  pages = {102--125},
  publisher = {Springer}
}

@incollection{bock_numerical_1981a,
  title = {Numerical {{Treatment}} of {{Inverse Problems}} in {{Chemical Reaction Kinetics}}},
  booktitle = {Modelling of {{Chemical Reaction Systems}}},
  author = {Bock, Hans Georg},
  year = {1981},
  pages = {102--125},
  publisher = {Springer}
}

@article{bock_recent_1983,
  title = {Recent Advances in Parameter Identification Problems for {{ODE}}},
  author = {Bock, {\relax HG}},
  year = {1983},
  journal = {Numerical Treatment of Inverse Problems in Differential and Integral Equations},
  pages = {95--121},
  doi = {10.1007/978-1-4684-7324-7_7}
}

@article{bock_recent_1983a,
  title = {Recent {{Advances}} in {{Parameter Identification Problems}} for {{ODE}}},
  author = {Bock, {\relax HG}},
  year = {1983},
  journal = {Numerical Treatment of Inverse Problems in Differential and Integral Equations},
  pages = {95--121},
  doi = {10/gfjwmg}
}

@article{bohlin_case_1994,
  title = {A Case Study of Grey Box Identification},
  author = {Bohlin, Torsten},
  year = {1994},
  journal = {Automatica},
  volume = {30},
  number = {2},
  pages = {307--318},
  issn = {0005-1098},
  doi = {10.1016/0005-1098(94)90032-9}
}

@article{bollapragada_exact_2016,
  title = {Exact and {{Inexact Subsampled Newton Methods}} for {{Optimization}}},
  author = {Bollapragada, Raghu and Byrd, Richard and Nocedal, Jorge},
  year = {2016},
  month = sep,
  journal = {arXiv:1609.08502 [math, stat]},
  eprint = {1609.08502},
  primaryclass = {math, stat},
  abstract = {The paper studies the solution of stochastic optimization problems in which approximations to the gradient and Hessian are obtained through subsampling. We first consider Newton-like methods that employ these approximations and discuss how to coordinate the accuracy in the gradient and Hessian to yield a superlinear rate of convergence in expectation. The second part of the paper analyzes an inexact Newton method that solves linear systems approximately using the conjugate gradient (CG) method, and that samples the Hessian and not the gradient (the gradient is assumed to be exact). We provide a complexity analysis for this method based on the properties of the CG iteration and the quality of the Hessian approximation, and compare it with a method that employs a stochastic gradient iteration instead of the CG method. We report preliminary numerical results that illustrate the performance of inexact subsampled Newton methods on machine learning applications based on logistic regression.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/U778ERRI/bollapragada_exact and_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/3NKNMVQG/1609.html}
}

@inproceedings{bolukbasi_man_2016,
  title = {Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  year = {2016},
  pages = {4349--4357},
  file = {/Users/antoniohortaribeiro/Zotero/storage/K8ZGD75S/bolukbasi_man is to_2016.pdf}
}

@article{bond_assessing_2014,
  title = {Assessing Computerized Eye Tracking Technology for Gaining Insight into Expert Interpretation of the 12-Lead Electrocardiogram: An Objective Quantitative Approach},
  shorttitle = {Assessing Computerized Eye Tracking Technology for Gaining Insight into Expert Interpretation of the 12-Lead Electrocardiogram},
  author = {Bond, R. R. and Zhu, T. and Finlay, D. D. and Drew, B. and Kligfield, P. D. and Guldenring, D. and Breen, C. and Gallagher, A. G. and Daly, M. J. and Clifford, G. D.},
  year = {2014 Nov-Dec},
  journal = {Journal of Electrocardiology},
  volume = {47},
  number = {6},
  pages = {895--906},
  issn = {1532-8430},
  doi = {10.1016/j.jelectrocard.2014.07.011},
  abstract = {INTRODUCTION: It is well known that accurate interpretation of the 12-lead electrocardiogram (ECG) requires a high degree of skill. There is also a moderate degree of variability among those who interpret the ECG. While this is the case, there are no best practice guidelines for the actual ECG interpretation process. Hence, this study adopts computerized eye tracking technology to investigate whether eye-gaze can be used to gain a deeper insight into how expert annotators interpret the ECG. Annotators were recruited in San Jose, California at the 2013 International Society of Computerised Electrocardiology (ISCE). METHODS: Each annotator was recruited to interpret a number of 12-lead ECGs (N=12) while their eye gaze was recorded using a Tobii X60 eye tracker. The device is based on corneal reflection and is non-intrusive. With a sampling rate of 60Hz, eye gaze coordinates were acquired every 16.7ms. Fixations were determined using a predefined computerized classification algorithm, which was then used to generate heat maps of where the annotators looked. The ECGs used in this study form four groups (3=ST elevation myocardial infarction [STEMI], 3=hypertrophy, 3=arrhythmias and 3=exhibiting unique artefacts). There was also an equal distribution of difficulty levels (3=easy to interpret, 3=average and 3=difficult). ECGs were displayed using the 4x3+1 display format and computerized annotations were concealed. RESULTS: Precisely 252 expert ECG interpretations (21 annotators{\texttimes}12 ECGs) were recorded. Average duration for ECG interpretation was 58s (SD=23). Fleiss' generalized kappa coefficient (Pa=0.56) indicated a moderate inter-rater reliability among the annotators. There was a 79\% inter-rater agreement for STEMI cases, 71\% agreement for arrhythmia cases, 65\% for the lead misplacement and dextrocardia cases and only 37\% agreement for the hypertrophy cases. In analyzing the total fixation duration, it was found that on average annotators study lead V1 the most (4.29s), followed by leads V2 (3.83s), the rhythm strip (3.47s), II (2.74s), V3 (2.63s), I (2.53s), aVL (2.45s), V5 (2.27s), aVF (1.74s), aVR (1.63s), V6 (1.39s), III (1.32s) and V4 (1.19s). It was also found that on average the annotator spends an equal amount of time studying leads in the frontal plane (15.89s) when compared to leads in the transverse plane (15.70s). It was found that on average the annotators fixated on lead I first followed by leads V2, aVL, V1, II, aVR, V3, rhythm strip, III, aVF, V5, V4 and V6. We found a strong correlation (r=0.67) between time to first fixation on a lead and the total fixation duration on each lead. This indicates that leads studied first are studied the longest. There was a weak negative correlation between duration and accuracy (r=-0.2) and a strong correlation between age and accuracy (r=0.67). CONCLUSIONS: Eye tracking facilitated a deeper insight into how expert annotators interpret the 12-lead ECG. As a result, the authors recommend ECG annotators to adopt an initial first impression/pattern recognition approach followed by a conventional systematic protocol to ECG interpretation. This recommendation is based on observing misdiagnoses given due to first impression only. In summary, this research presents eye gaze results from expert ECG annotators and provides scope for future work that involves exploiting computerized eye tracking technology to further the science of ECG interpretation.},
  langid = {english},
  pmid = {25110276},
  keywords = {{Arrhythmias, Cardiac},{Fixation, Ocular},Adult,Artificial Intelligence,Clinical Competence,ECG interpretation,Electrocardiography,Eye Movements,eye-tracking,Female,Humans,Male,Reading,Visual Perception}
}

@inproceedings{bonin_lassoenhanced_2010,
  title = {{{LASSO-enhanced}} Simulation Error Minimization Method for {{NARX}} Model Selection},
  booktitle = {American {{Control Conference}} ({{ACC}}), 2010},
  author = {Bonin, Mariangela and Seghezza, Valerio and Piroddi, Luigi},
  year = {2010},
  pages = {4522--4527},
  publisher = {IEEE},
  urldate = {2017-09-13},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CTUFMGBI/bonin_lasso-enha_2010.pdf;/Users/antoniohortaribeiro/Zotero/storage/KJCTTU4E/bonin_lasso-enha_2010.pdf}
}

@article{bonin_narx_2010,
  title = {{{NARX}} Model Selection Based on Simulation Error Minimisation and {{LASSO}}},
  author = {Bonin, M and Seghezza, V and Piroddi, L},
  year = {2010},
  journal = {IET Control Theory \& Applications},
  volume = {4},
  number = {7},
  pages = {1157--1168},
  doi = {10.1049/iet-cta.2009.0217},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HXI8JNAV/bonin_narx_2010.pdf}
}

@book{boolos_computability_2002,
  title = {Computability and Logic},
  author = {Boolos, George S. and Burgess, John P. and Jeffrey, Richard C.},
  year = {2002},
  publisher = {Cambridge university press},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GBMBMFEU/boolos_computabil_2002.pdf}
}

@article{bottou_optimization_2018,
  title = {Optimization {{Methods}} for {{Large-Scale Machine Learning}}},
  author = {Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
  year = {2018},
  month = jan,
  journal = {SIAM Review},
  volume = {60},
  number = {2},
  pages = {223--311},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/16M1080173},
  urldate = {2018-05-08},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9WMXJBN9/bottou_optimizati_2018.pdf}
}

@article{bouc_mathematical_1971,
  title = {A Mathematical Model for Hysteresis},
  author = {Bouc, R},
  year = {1971},
  journal = {Acta Acustica united with Acustica},
  volume = {24},
  number = {1},
  pages = {16--25},
  publisher = {S. Hirzel Verlag},
  issn = {1610-1928}
}

@inproceedings{bousquet_tradeoffs_2008,
  title = {The Tradeoffs of Large Scale Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bousquet, Olivier and Bottou, L{\'e}on},
  year = {2008},
  pages = {161--168},
  urldate = {2017-09-11},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BQNC2TGI/bousquet_the_2008.pdf}
}

@article{bousseljot_nutzung_1995,
  title = {Nutzung Der {{EKG-Signaldatenbank CARDIODAT}} Der {{PTB}} {\"U}ber Das Internet},
  author = {Bousseljot, R and Kreiseler, D and Schnabel, A},
  year = {1995},
  journal = {Biomedizinische Technik/Biomedical Engineering},
  volume = {40},
  number = {s1},
  pages = {317--318},
  publisher = {Walter de Gruyter, Berlin/New York}
}

@book{box_time_2015,
  title = {Time Series Analysis: Forecasting and Control},
  shorttitle = {Time Series Analysis},
  author = {Box, George EP and Jenkins, Gwilym M. and Reinsel, Gregory C. and Ljung, Greta M.},
  year = {2015},
  publisher = {John Wiley \& Sons},
  file = {/Users/antoniohortaribeiro/Zotero/storage/D6QCIQMX/box_time_2015.pdf}
}

@book{boyd_convex_2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2004},
  publisher = {Cambridge University Press},
  isbn = {978-0-521-83378-3},
  lccn = {QA402.5 .B69 2004},
  keywords = {Convex functions,Mathematical optimization},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9P5IUMEX/boyd_convex_2004.pdf;/Users/antoniohortaribeiro/Zotero/storage/AFS6STKK/boyd_convex_2004.pdf}
}

@article{boyd_fading_1985,
  title = {Fading Memory and the Problem of Approximating Nonlinear Operators with {{Volterra}} Series},
  author = {Boyd, S. and Chua, L.},
  year = {1985},
  month = nov,
  journal = {IEEE Transactions on Circuits and Systems},
  volume = {32},
  number = {11},
  pages = {1150--1161},
  issn = {0098-4094},
  doi = {10/fkqmc5},
  abstract = {Using the notion of fading memory we prove very strong versions of two folk theorems. The first is that any time-invariant (TI) continuous nonlinear operator can be approximated by a Volterra series operator, and the second is that the approximating operator can be realized as a finite-dimensional linear dynamical system with a nonlinear readout map. While previous approximation results are valid over finite time intervals and for signals in compact sets, the approximations presented here hold for all time and for signals in useful (noncompact) sets. The discretetime analog of the second theorem asserts that any TI operator with fading memory can be approximated (in our strong sense) by a nonlinear moving- average operator. Some further discussion of the notion of fading memory is given.},
  keywords = {Approximation methods,Control systems,Convolution,Fading,Fasteners,Mathematics,Nonlinear circuits and systems,Nonlinear control systems,Nonlinear equations,Nonlinear systems,Operational amplifiers,Operator theory,Polynomials,Volterra series},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GMURV899/boyd_fading_1985.pdf;/Users/antoniohortaribeiro/Zotero/storage/ETRSVKMR/1085649.html}
}

@techreport{boyd_subgradients_2022,
  title = {Subgradients ({{Lecture Notes}})},
  author = {Boyd, S. and Duchi, J. and Pilanci, M. and Vandenberghe, L.},
  year = {2022},
  month = apr,
  number = {Notes for EE364b, Stanford University, Spring 2021-22},
  file = {/Users/antoniohortaribeiro/Zotero/storage/U5ITZIQE/subgradients_notes.pdf}
}

@article{boynton_analysis_2013,
  title = {The Analysis of Electron Fluxes at Geosynchronous Orbit Employing a {{NARMAX}} Approach},
  author = {Boynton, R. J. and Balikhin, M. A. and Billings, S. A. and Reeves, G. D. and Ganushkina, N and Gedalin, M and Amariutei, {\relax OA} and Borovsky, J. E. and Walker, S. N.},
  year = {2013},
  journal = {Journal of Geophysical Research: Space Physics},
  volume = {118},
  number = {4},
  pages = {1500--1513},
  doi = {10.1002/jgra.50192}
}

@article{boynton_analysis_2013a,
  title = {The {{Analysis}} of {{Electron Fluxes}} at {{Geosynchronous Orbit Employing}} a {{NARMAX Approach}}},
  author = {Boynton, R. J. and Balikhin, M. A. and Billings, S. A. and Reeves, G. D. and Ganushkina, N and Gedalin, M and Amariutei, {\relax OA} and Borovsky, J. E. and Walker, S. N.},
  year = {2013},
  journal = {Journal of Geophysical Research: Space Physics},
  volume = {118},
  number = {4},
  pages = {1500--1513},
  doi = {10/f42b3m}
}

@book{bradski_learning_2008,
  title = {Learning {{OpenCV}}: {{Computer}} Vision with the {{OpenCV}} Library},
  author = {Bradski, Gary and Kaehler, Adrian},
  year = {2008},
  publisher = {" O'Reilly Media, Inc."}
}

@book{bradski_learning_2011,
  title = {Learning {{OpenCV}}: Computer Vision with the {{OpenCV}} Library},
  shorttitle = {Learning {{OpenCV}}},
  author = {Bradski, Gary R. and Kaehler, Adrian},
  year = {2011},
  series = {Software That Sees},
  edition = {1. ed., [Nachdr.]},
  publisher = {O'Reilly},
  address = {Beijing},
  isbn = {978-0-596-51613-0},
  langid = {english},
  annotation = {OCLC: 838472784},
  file = {/Users/antoniohortaribeiro/Zotero/storage/79BQPV2Z/bradski_learning_2011.pdf}
}

@article{branch_subspace_1999,
  title = {A Subspace, Interior, and Conjugate Gradient Method for Large-Scale Bound-Constrained Minimization Problems},
  author = {Branch, Mary Ann and Coleman, Thomas F and Li, Yuying},
  year = {1999},
  journal = {SIAM Journal on Scientific Computing},
  volume = {21},
  number = {1},
  pages = {1--23},
  doi = {10.1137/S1064827595289108}
}

@article{brant_electrocardiographic_2023,
  title = {Electrocardiographic {{Age Predicts Cardiovascular Events}} in {{Community}}: {{The Framingham Heart Study}}},
  author = {Brant, Luisa C C and Ribeiro, Ant{\^o}nio H and {Pinto-Filho}, Marcelo M and Kornej, Jelena and Preis, Sarah R. and Eromosele, Benjamin and Magnani, Jared W. and Murabito, Joanne M. and Larson, Martin G and Benjamin, Emelia J and Ribeiro, Antonio L P and Lin, Honghuang},
  year = {2023},
  journal = {Circulation: Cardiovascular Quality and Outcomes},
  doi = {10.1161/CIRCOUTCOMES.122.009821},
  copyright = {All rights reserved}
}

@article{brant_reproducibility_2013,
  title = {Reproducibility of Peripheral Arterial Tonometry for the Assessment of Endothelial Function in Adults},
  author = {Brant, Luisa C.C. and Barreto, Sandhi M. and Passos, Val{\'e}ria M.A. and Ribeiro, Ant{\^o}nio L.P.},
  year = {2013},
  month = oct,
  journal = {Journal of Hypertension},
  volume = {31},
  number = {10},
  pages = {1984--1990},
  issn = {0263-6352},
  doi = {10.1097/HJH.0b013e328362d913},
  urldate = {2023-08-30},
  abstract = {Objectives: Endothelial dysfunction is associated to cardiovascular risk factors and predicts cardiovascular events. Peripheral arterial tonometry (PAT) is a novel noninvasive method to assess endothelial function. However, there is a paucity of data about its reproducibility. The aim of this study was to assess the feasibility and reproducibility of PAT in adults. Methods: PAT exams were performed twice in the same day in 123 participants of a cohort about the determinants of diabetes and cardiovascular diseases (Brazilian Longitudinal Study of Adult Health -- ELSA-Brasil). The interval between the exams was 2--6 h (mean {$\frac{1}{4}$} 4 h). Endothelial function in PAT method is measured by reactive hyperemia index (RHI), which evaluates arterial pulsatile volume changes in response to hyperemia. Agreement of RHI values was compared by Bland--Altman method, coefficient of variation and coefficient of repeatability. Reliability was assessed by intraclass correlation coefficient (ICC). Results: Mean values of RHI did not differ significantly between the exams of each participant (1.92 {\AE} 0.56 vs. 1.96 {\AE} 0.58, P {$\frac{1}{4}$} 0.48). There were no systematic errors between the exams (mean of differences {$\frac{1}{4}$} {\`A}0.03 {\AE} 0.5). Measurement error was 0.35, coefficient of variation was 18.0\% and ICC was 0.61. Sex, age or the presence of obesity did not have a considerable influence on the reproducibility of PAT. Conclusion: PAT exam is feasible and has acceptable reproducibility in adults when compared with other noninvasive methods for endothelial function assessment. This performance makes PAT a promising method for future clinical and epidemiological studies.},
  langid = {english},
  keywords = {\_tablet},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZMVKPD4R/Brant et al_2013_Reproducibility of peripheral arterial tonometry for the assessment of.pdf}
}

@inproceedings{braun_enzoiia_1994,
  title = {{{ENZO-II-A}} Powerful Design Tool to Evolve Multilayer Feed Forward Networks},
  booktitle = {Evolutionary {{Computation}}, 1994. {{IEEE World Congress}} on {{Computational Intelligence}}., {{Proceedings}} of the {{First IEEE Conference}} On},
  author = {Braun, Heinrich and Zagorski, Peter},
  year = {1994},
  pages = {278--283},
  publisher = {IEEE}
}

@article{breiman_bagging_1996,
  title = {Bagging Predictors},
  author = {Breiman, Leo},
  year = {1996},
  month = aug,
  journal = {Machine Learning},
  volume = {24},
  number = {2},
  pages = {123--140},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00058655},
  urldate = {2020-11-26},
  abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7AWNKHGQ/Breiman - 1996 - Bagging predictors.pdf}
}

@article{breiman_heuristics_1996,
  title = {Heuristics of Instability and Stabilization in Model Selection},
  author = {Breiman, Leo},
  year = {1996},
  journal = {The annals of statistics},
  volume = {24},
  number = {6},
  pages = {2350--2383},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3R8WCID6/breiman_heuristics_1996.pdf}
}

@article{breiman_random_2001,
  title = {Random Forests},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Machine learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  publisher = {Springer}
}

@article{breiman_statistical_2001,
  title = {Statistical Modeling: {{The}} Two Cultures (with Comments and a Rejoinder by the Author)},
  shorttitle = {Statistical Modeling},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Statistical science},
  volume = {16},
  number = {3},
  pages = {199--231},
  doi = {10.1214/ss/1009213726},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QMNQCIQ7/breiman_statistica_2001.pdf}
}

@book{bronson_matrix_1991,
  title = {Matrix Methods: An Introduction},
  shorttitle = {Matrix Methods},
  author = {Bronson, Richard},
  year = {1991},
  edition = {2nd ed},
  publisher = {Academic Press},
  address = {Boston},
  isbn = {978-0-12-135251-6},
  lccn = {QA188 .B758 1990},
  keywords = {Matrices},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TX9W9JWX/bronson_matrix_1991.pdf}
}

@book{brown_complex_2009,
  title = {Complex Variables and Applications},
  author = {Brown, James Ward and Churchill, Ruel V.},
  year = {2009},
  series = {Brown and {{Churchill}} Series},
  edition = {8th ed},
  publisher = {McGraw-Hill Higher Education},
  address = {Boston},
  isbn = {978-0-07-305194-9},
  lccn = {QA331.7 .C524 2009},
  keywords = {Functions of complex variables},
  annotation = {OCLC: ocn176648981},
  file = {/Users/antoniohortaribeiro/Zotero/storage/T38J5RU4/brown_complex_2009.pdf;/Users/antoniohortaribeiro/Zotero/storage/THVB9N5T/brown_complex_2009.pdf}
}

@article{broyden_convergence_1970,
  title = {The Convergence of a Class of Double-Rank Minimization Algorithms 1. General Considerations},
  author = {Broyden, Charles George},
  year = {1970},
  journal = {IMA Journal of Applied Mathematics},
  volume = {6},
  number = {1},
  pages = {76--90},
  doi = {10.1093/imamat/6.1.76}
}

@inproceedings{bruna_intriguing_2014,
  title = {Intriguing Properties of Neural Networks},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Bruna, Joan and Szegedy, Christian and Sutskever, Ilya and Goodfellow, Ian and Zaremba, Wojciech and Fergus, Rob and Erhan, Dumitru},
  year = {2014},
  file = {/Users/antoniohortaribeiro/Zotero/storage/S6MU6243/forum.html}
}

@article{bryc_spectral_2006,
  title = {Spectral Measure of Large Random {{Hankel}}, {{Markov}} and {{Toeplitz}} Matrices},
  author = {Bryc, W{\l}odzimierz and Dembo, Amir and Jiang, Tiefeng},
  year = {2006},
  month = feb,
  journal = {arXiv:math/0307330},
  eprint = {math/0307330},
  doi = {10.1214/009117905000000495},
  urldate = {2020-12-14},
  abstract = {We study the limiting spectral measure of large symmetric random matrices of linear algebraic structure. For Hankel and Toeplitz matrices generated by i.i.d. random variables \${\textbackslash}\{X\_k{\textbackslash}\}\$ of unit variance, and for symmetric Markov matrices generated by i.i.d. random variables \${\textbackslash}\{X\_\{ij\}{\textbackslash}\}\_\{j{$>$}i\}\$ of zero mean and unit variance, scaling the eigenvalues by \${\textbackslash}sqrt\{n\}\$ we prove the almost sure, weak convergence of the spectral measures to universal, nonrandom, symmetric distributions \${\textbackslash}gamma\_H\$, \${\textbackslash}gamma\_M\$ and \${\textbackslash}gamma\_T\$ of unbounded support. The moments of \${\textbackslash}gamma\_H\$ and \${\textbackslash}gamma\_T\$ are the sum of volumes of solids related to Eulerian numbers, whereas \${\textbackslash}gamma\_M\$ has a bounded smooth density given by the free convolution of the semicircle and normal densities. For symmetric Markov matrices generated by i.i.d. random variables \${\textbackslash}\{X\_\{ij\}{\textbackslash}\}\_\{j{$>$}i\}\$ of mean \$m\$ and finite variance, scaling the eigenvalues by \$\{n\}\$ we prove the almost sure, weak convergence of the spectral measures to the atomic measure at \$-m\$. If \$m=0\$, and the fourth moment is finite, we prove that the spectral norm of \${\textbackslash}mathbf \{M\}\_n\$ scaled by \${\textbackslash}sqrt\{2n{\textbackslash}log n\}\$ converges almost surely to 1.},
  archiveprefix = {arXiv},
  keywords = {{15A52 (Primary) 60F99, 62H10, 60F10 (Secondary)},Mathematics - Combinatorics,Mathematics - Probability,Mathematics - Statistics Theory},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WKZUFBPW/Bryc et al. - 2006 - Spectral measure of large random Hankel, Markov an.pdf;/Users/antoniohortaribeiro/Zotero/storage/N9M6VGWA/0307330.html}
}

@article{bubeck_convex_2015,
  title = {Convex {{Optimization}}: {{Algorithms}} and {{Complexity}}},
  shorttitle = {Convex {{Optimization}}},
  author = {Bubeck, S{\'e}bastien},
  year = {2015},
  journal = {Foundations and Trends in Machine Learning},
  volume = {8},
  number = {3-4},
  eprint = {1405.4980},
  primaryclass = {cs, math, stat},
  pages = {231--357},
  urldate = {2024-01-16},
  abstract = {This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3N3BCDUZ/Bubeck - 2015 - Convex Optimization Algorithms and Complexity.pdf}
}

@article{bubeck_law_2021,
  title = {A Law of Robustness for Two-Layers Neural Networks},
  author = {Bubeck, S{\'e}bastien and Li, Yuanzhi and Nagaraj, Dheeraj},
  year = {2021},
  journal = {134 of Proceedings of Machine Learning Research, Conference on Learning Theory (COLT)},
  volume = {134},
  eprint = {2009.14444},
  pages = {804--820},
  abstract = {We initiate the study of the inherent tradeoffs between the size of a neural network and its robustness, as measured by its Lipschitz constant. We make a precise conjecture that, for any Lipschitz activation function and for most datasets, any two-layers neural network with \$k\$ neurons that perfectly fit the data must have its Lipschitz constant larger (up to a constant) than \${\textbackslash}sqrt\{n/k\}\$ where \$n\$ is the number of datapoints. In particular, this conjecture implies that overparametrization is necessary for robustness, since it means that one needs roughly one neuron per datapoint to ensure a \$O(1)\$-Lipschitz network, while mere data fitting of \$d\$-dimensional data requires only one neuron per \$d\$ datapoints. We prove a weaker version of this conjecture when the Lipschitz constant is replaced by an upper bound on it based on the spectral norm of the weight matrix. We also prove the conjecture in the high-dimensional regime \$n {\textbackslash}approx d\$ (which we also refer to as the undercomplete case, since only \$k {\textbackslash}leq d\$ is relevant here). Finally we prove the conjecture for polynomial activation functions of degree \$p\$ when \$n {\textbackslash}approx d{\textasciicircum}p\$. We complement these findings with experimental evidence supporting the conjecture.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PRHH43V2/Bubeck et al_2021_A law of robustness for two-layers neural networks.pdf;/Users/antoniohortaribeiro/Zotero/storage/D37PJ65N/2009.html}
}

@article{bubeck_universal_2021,
  title = {A {{Universal Law}} of {{Robustness}} via {{Isoperimetry}}},
  author = {Bubeck, S{\'e}bastien and Sellke, Mark},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  eprint = {2105.12806},
  urldate = {2021-08-26},
  abstract = {Classically, data interpolation with a parametrized model class is possible as long as the number of parameters is larger than the number of equations to be satisfied. A puzzling phenomenon in deep learning is that models are trained with many more parameters than what this classical theory would suggest. We propose a theoretical explanation for this phenomenon. We prove that for a broad class of data distributions and model classes, overparametrization is necessary if one wants to interpolate the data smoothly. Namely we show that smooth interpolation requires \$d\$ times more parameters than mere interpolation, where \$d\$ is the ambient data dimension. We prove this universal law of robustness for any smoothly parametrized function class with polynomial size weights, and any covariate distribution verifying isoperimetry. In the case of two-layers neural networks and Gaussian covariates, this law was conjectured in prior work by Bubeck, Li and Nagaraj. We also give an interpretation of our result as an improved generalization bound for model classes consisting of smooth functions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XQGGG4X8/Bubeck and Sellke - 2021 - A Universal Law of Robustness via Isoperimetry.pdf;/Users/antoniohortaribeiro/Zotero/storage/UAXPZ9ZG/2105.html}
}

@article{buda_systematic_2018,
  title = {A Systematic Study of the Class Imbalance Problem in Convolutional Neural Networks},
  author = {Buda, Mateusz and Maki, Atsuto and Mazurowski, Maciej A.},
  year = {2018},
  month = oct,
  journal = {Neural Networks},
  volume = {106},
  eprint = {1710.05381},
  pages = {249--259},
  issn = {08936080},
  doi = {10/gfbfz3},
  urldate = {2019-01-28},
  abstract = {In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve (ROC AUC) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that completely eliminates the imbalance, whereas the optimal undersampling ratio depends on the extent of imbalance; (iv) as opposed to some classical machine learning models, oversampling does not cause overfitting of CNNs; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7ZUP72WW/buda_a_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/DCJSQJXI/1710.html}
}

@book{burden_numerical_2011,
  title = {Numerical Analysis},
  author = {Burden, Richard L. and Faires, J. Douglas},
  year = {2011},
  edition = {9th ed},
  publisher = {Brooks/Cole, Cengage Learning},
  address = {Boston, MA},
  isbn = {978-0-538-73351-9},
  lccn = {QA297 .B84 2011},
  keywords = {numerical analysis},
  annotation = {OCLC: ocn496962633},
  file = {/Users/antoniohortaribeiro/Zotero/storage/W2WN7A6H/burden_numerical_2011.pdf}
}

@article{byrd_approximate_1988,
  title = {Approximate Solution of the Trust Region Problem by Minimization over Two-Dimensional Subspaces},
  author = {Byrd, Richard H and Schnabel, Robert B and Shultz, Gerald A},
  year = {1988},
  journal = {Mathematical Programming},
  volume = {40},
  number = {1-3},
  pages = {247--263},
  doi = {10.1007/BF01580735}
}

@article{byrd_interior_1999,
  title = {An Interior Point Algorithm for Large-Scale Nonlinear Programming},
  author = {Byrd, Richard H. and Hribar, Mary E. and Nocedal, Jorge},
  year = {1999},
  journal = {SIAM Journal on Optimization},
  volume = {9},
  number = {4},
  pages = {877--900},
  doi = {10.1137/S1052623497325107},
  urldate = {2017-08-20},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VE35ZEJ9/byrd_an_1999.pdf}
}

@article{byrd_interior_1999a,
  title = {An {{Interior Point Algorithm}} for {{Large-Scale Nonlinear Programming}}},
  author = {Byrd, Richard H. and Hribar, Mary E. and Nocedal, Jorge},
  year = {1999},
  journal = {SIAM Journal on Optimization},
  volume = {9},
  number = {4},
  pages = {877--900},
  doi = {10/b4r783},
  urldate = {2017-08-20}
}

@incollection{byrd_knitro_2006,
  title = {{{KNITRO}}: {{An}} Integrated Package for Nonlinear Optimization},
  booktitle = {Large-Scale Nonlinear Optimization},
  author = {Byrd, Richard H and Nocedal, Jorge and Waltz, Richard A},
  year = {2006},
  pages = {35--59},
  publisher = {Springer},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YLQIBR3U/byrd_knitro_2006.pdf}
}

@incollection{byrd_knitro_2006a,
  title = {{{KNITRO}}: {{An Integrated Package}} for {{Nonlinear Optimization}}},
  booktitle = {Large-{{Scale Nonlinear Optimization}}},
  author = {Byrd, Richard H and Nocedal, Jorge and Waltz, Richard A},
  year = {2006},
  pages = {35--59},
  publisher = {Springer}
}

@article{byrd_limited_1995,
  title = {A Limited Memory Algorithm for Bound Constrained Optimization},
  author = {Byrd, Richard H and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
  year = {1995},
  journal = {SIAM Journal on Scientific Computing},
  volume = {16},
  number = {5},
  pages = {1190--1208},
  issn = {1064-8275},
  doi = {10.1137/0916069},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CWGWJK7H/Byrd et al_1995_A limited memory algorithm for bound constrained optimization.pdf}
}

@article{byrd_local_1997,
  title = {On the Local Behavior of an Interior Point Method for Nonlinear Programming},
  author = {Byrd, Richard H and Liu, Guanghui and Nocedal, Jorge},
  year = {1997},
  journal = {Numerical analysis},
  volume = {1997},
  pages = {37--56},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6EN3Q9NT/byrd_on the_1997.pdf}
}

@article{byrd_local_1997a,
  title = {On the {{Local Behavior}} of an {{Interior Point Method}} for {{Nonlinear Programming}}},
  author = {Byrd, Richard H and Liu, Guanghui and Nocedal, Jorge},
  year = {1997},
  journal = {Numerical analysis},
  volume = {1997},
  pages = {37--56},
  keywords = {ðŸ”No DOI found}
}

@article{byrd_representations_1994,
  title = {Representations of Quasi-{{Newton}} Matrices and Their Use in Limited Memory Methods},
  author = {Byrd, Richard H. and Nocedal, Jorge and Schnabel, Robert B.},
  year = {1994},
  month = jan,
  journal = {Mathematical Programming},
  volume = {63},
  number = {1-3},
  pages = {129--156},
  issn = {0025-5610, 1436-4646},
  doi = {10.1007/BF01582063},
  urldate = {2017-10-30},
  abstract = {We derive compact representations of BFGS and symmetric rank-one matrices for optimization. These representations allow us to efficiently implement limited memory methods for large constrained optimization problems. In particular, we discuss how to compute projections of limited memory matrices onto subspaces. We also present a compact representation of the matrices generated by Broyden's update for solving systems of nonlinear equations.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TZHS6F5G/byrd_representa_1994.pdf;/Users/antoniohortaribeiro/Zotero/storage/ESW3TBFK/10.html}
}

@article{byrd_trust_1987,
  title = {A Trust Region Algorithm for Nonlinearly Constrained Optimization},
  author = {Byrd, Richard H and Schnabel, Robert B and Shultz, Gerald A},
  year = {1987},
  journal = {SIAM Journal on Numerical Analysis},
  volume = {24},
  number = {5},
  pages = {1152--1170},
  doi = {10.1137/0724076}
}

@article{byrd_trust_2000,
  title = {A Trust Region Method Based on Interior Point Techniques for Nonlinear Programming},
  author = {Byrd, Richard H and Gilbert, Jean Charles and Nocedal, Jorge},
  year = {2000},
  journal = {Mathematical Programming},
  volume = {89},
  number = {1},
  pages = {149--185},
  issn = {0025-5610},
  doi = {10.1007/PL00011391},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZD3CCHQ4/byrd_a trust_2000.pdf}
}

@article{byrd_trust_2000a,
  title = {A {{Trust Region Method Based}} on {{Interior Point Techniques}} for {{Nonlinear Programming}}},
  author = {Byrd, Richard H and Gilbert, Jean Charles and Nocedal, Jorge},
  year = {2000},
  journal = {Mathematical Programming},
  volume = {89},
  number = {1},
  pages = {149--185},
  issn = {0025-5610},
  doi = {10/dmkzpw}
}

@article{cadenas_wind_2016,
  title = {Wind {{Speed Forecasting Using}} the {{NARX Model}}, {{Case}}: {{La Mata}}, {{Oaxaca}}, {{M{\'e}xico}}},
  author = {Cadenas, Erasmo and Rivera, Wilfrido and {Campos-Amezcua}, Rafael and Cadenas, Roberto},
  year = {2016},
  journal = {Neural Computing and Applications},
  volume = {27},
  number = {8},
  pages = {2417--2428},
  doi = {10/f9k443},
  annotation = {00010}
}

@article{cadenas_wind_2016a,
  title = {Wind {{Speed Forecasting Using}} the {{NARX Model}}, {{Case}}: {{La Mata}}, {{Oaxaca}}, {{M{\'e}xico}}},
  author = {Cadenas, Erasmo and Rivera, Wilfrido and {Campos-Amezcua}, Rafael and Cadenas, Roberto},
  year = {2016},
  journal = {Neural Computing and Applications},
  volume = {27},
  number = {8},
  pages = {2417--2428},
  doi = {10/f9k443},
  copyright = {All rights reserved},
  file = {/Users/antoniohortaribeiro/Zotero/storage/N2SPPCZZ/32167523.nbib}
}

@inproceedings{calafiore_leading_2016,
  title = {Leading Impulse Response Identification via the Weighted Elastic Net Criterion},
  booktitle = {2016 {{IEEE}} 55th {{Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Calafiore, G. C. and Novara, C. and Taragna, M.},
  year = {2016},
  month = dec,
  pages = {2926--2931},
  doi = {10.1109/CDC.2016.7798705},
  abstract = {This paper deals with the problem of finding a low-complexity estimate of the impulse response of a linear time-invariant discrete-time dynamic system from noise-corrupted input-output data. To this purpose, we introduce an identification criterion formed by the average (over the input perturbations) of a standard prediction error cost, plus a weighted {$\ell$}1 regularization term which promotes sparse solutions. While it is well known that such criteria do provide solutions with many zeros, a critical issue in our identification context is where these zeros are located, since sensible low-order models should be zero in the tail of the impulse response. The flavor of the key results in this paper is that, under quite standard assumptions (such as i.i.d. input and noise sequences and system stability), the estimate of the impulse response resulting from the proposed criterion is indeed identically zero from a certain time index nl (named the leading order) onwards, with arbitrarily high probability, for a sufficiently large data cardinality N. Numerical experiments are reported that support the theoretical results, and comparisons are made with some other state-of-the-art methodologies.},
  keywords = {Context,Convergence,data cardinality,discrete time systems,impulse response estimation,input perturbation,leading impulse response identification,linear systems,linear time-invariant discrete-time dynamic system,Noise measurement,noise sequence,noise-corrupted input-output data,nonparametric method,nonparametric statistics,parameter estimation,poles and zeros,prediction error cost,probability,Random variables,stability,Standards,system stability,time index,Time measurement,transient response,weighted {$\ell$}1 regularization term,weighted elastic net criterion,zeros},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7PAME3KP/calafiore_leading_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/ESPSFFZ9/7798705.html}
}

@article{calafiore_leading_2017,
  title = {Leading Impulse Response Identification via the {{Elastic Net}} Criterion},
  author = {Calafiore, Giuseppe C. and Novara, Carlo and Taragna, Michele},
  year = {2017},
  month = jun,
  journal = {Automatica},
  volume = {80},
  number = {Supplement C},
  pages = {75--87},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2017.01.011},
  abstract = {This paper deals with the problem of finding a low-complexity estimate of the impulse response of a linear time-invariant discrete-time dynamic system from noise-corrupted input--output data. To this purpose, we introduce an identification criterion formed by the average (over the input perturbations) of a standard prediction error cost, plus an {$\ell$}1 regularization term which promotes sparse solutions. While it is well known that such criteria do provide solutions with many zeros, a critical issue in our identification context is where these zeros are located, since sensible low-order models should be zero in the tail of the impulse response. The flavor of the key results in this paper is that, under quite standard assumptions (such as i.i.d.~input and noise sequences and system stability), the estimate of the impulse response resulting from the proposed criterion is indeed identically zero from a certain time index nl (named the leading order) onwards, with arbitrarily high probability, for a sufficiently large data cardinality N. Numerical experiments are reported that support the theoretical results, and comparisons are made with some other state-of-the-art methodologies.},
  keywords = {Elastic net,FIR identification,Lasso,Regularization,Sparsity},
  file = {/Users/antoniohortaribeiro/Zotero/storage/894XME8Z/calafiore_leading_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/6UIKAP3M/S0005109817300213.html}
}

@article{calafiore_sparse_2014,
  title = {Sparse {{Identification}} of {{Polynomial}} and {{Posynomial Models}}},
  author = {Calafiore, Giuseppe Carlo and Ghaoui, Laurent El and Novara, Carlo},
  year = {2014},
  month = jan,
  journal = {19th IFAC World Congress},
  volume = {47},
  number = {3},
  pages = {3238--3243},
  issn = {1474-6670},
  doi = {10.3182/20140824-6-ZA-1003.01549},
  keywords = {Coordinate-descent methods,Identification,Posynomial models,Sparse optimization,Square-root LASSO},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3DA2ZZI6/calafiore_sparse_2014.pdf}
}

@article{calafiore_sparse_2015,
  title = {Sparse Identification of Posynomial Models},
  author = {Calafiore, Giuseppe C. and El Ghaoui, Laurent M. and Novara, Carlo},
  year = {2015},
  month = sep,
  journal = {Automatica},
  volume = {59},
  number = {Supplement C},
  pages = {27--34},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2015.06.003},
  abstract = {Posynomials are nonnegative combinations of monomials with possibly fractional and both positive and negative exponents. Posynomial models are widely used in various engineering design endeavors, such as circuits, aerospace and structural design, mainly due to the fact that design problems cast in terms of posynomial objectives and constraints can be solved efficiently by means of a convex optimization technique known as geometric programming (GP). However, while quite a vast literature exists on GP-based design, very few contributions can yet be found on the problem of identifying posynomial models from experimental data. Posynomial identification amounts to determining not only the coefficients of the combination, but also the exponents in the monomials, which renders the identification problem hard. In this paper, we propose an approach to the identification of multivariate posynomial models based on the expansion on a given large-scale basis of monomials. The model is then identified by seeking coefficients of the combination that minimize a mixed objective, composed by a term representing the fitting error and a term inducing sparsity in the representation, which results in a problem formulation of the ``square-root LASSO'' type, with nonnegativity constraints on the variables. We propose to solve the problem via a sequential coordinate-minimization scheme, which is suitable for large-scale implementations. A numerical example is finally presented, dealing with the identification of a posynomial model for a NACA 4412 airfoil.},
  keywords = {Geometric programming,Posynomial models,regularizazion,Sparse optimization,Square-root LASSO},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7RKCIU4D/calafiore_sparse_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/3HERMZ6I/S0005109815002319.html;/Users/antoniohortaribeiro/Zotero/storage/4JVWRNUD/S0005109815002319.html}
}

@inproceedings{candes_compressive_2006,
  title = {Compressive Sampling},
  booktitle = {Proceedings of the International Congress of Mathematicians},
  author = {Cand{\`e}s, Emmanuel J},
  year = {2006},
  volume = {3},
  pages = {1433--1452},
  organization = {Madrid, Spain},
  file = {/Users/antoniohortaribeiro/Zotero/storage/M26BYQJ3/CompressiveSampling06.pdf}
}

@article{candes_conformalized_2023,
  title = {Conformalized {{Survival Analysis}}},
  author = {Cand{\`e}s, Emmanuel J. and Lei, Lihua and Ren, Zhimei},
  year = {2023},
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {85},
  number = {1},
  eprint = {2103.09763},
  pages = {24--45},
  doi = {10.1093/jrsssb/qkac004},
  abstract = {Existing survival analysis techniques heavily rely on strong modelling assumptions and are, therefore, prone to model misspecification errors. In this paper, we develop an inferential method based on ideas from conformal prediction, which can wrap around any survival prediction algorithm to produce calibrated, covariate-dependent lower predictive bounds on survival times. In the Type I right-censoring setting, when the censoring times are completely exogenous, the lower predictive bounds have guaranteed coverage in finite samples without any assumptions other than that of operating on independent and identically distributed data points. Under a more general conditionally independent censoring assumption, the bounds satisfy a doubly robust property which states the following: marginal coverage is approximately guaranteed if either the censoring mechanism or the conditional survival function is estimated well. Further, we demonstrate that the lower predictive bounds remain valid and informative for other types of censoring. The validity and efficiency of our procedure are demonstrated on synthetic data and real COVID-19 data from the UK Biobank.},
  archiveprefix = {arXiv},
  keywords = {\_tablet,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/antoniohortaribeiro/Zotero/storage/R328535F/CandÃ¨s et al_2023_Conformalized Survival Analysis.pdf;/Users/antoniohortaribeiro/Zotero/storage/IF99SGSM/2103.html}
}

@article{candes_dantzig_2007,
  title = {The {{Dantzig Selector}}: {{Statistical Estimation When}} p {{Is Much Larger}} than n},
  shorttitle = {The {{Dantzig Selector}}},
  author = {Candes, Emmanuel and Tao, Terence},
  year = {2007},
  journal = {The Annals of Statistics},
  volume = {35},
  number = {6},
  eprint = {25464587},
  eprinttype = {jstor},
  pages = {2313--2351},
  issn = {0090-5364},
  abstract = {In many important statistical applications, the number of variables or parameters p is much larger than the number of observations n. Suppose then that we have observations y = X{$\beta$} + z, where \${\textbackslash}beta {\textbackslash}in \{{\textbackslash}bf R\}{\textasciicircum}\{p\}\$ is a parameter vector of interest, X is a data matrix with possibly far fewer rows than columns, n << p, and the \$z\_\{i\}{\textbackslash}text\{'\}\{{\textbackslash}rm s\}\$ are i.i.d. N(0, {$\sigma^2$}). Is it possible to estimate {$\beta$} reliably based on the noisy data y? To estimate {$\beta$}, we introduce a new estimator-we call it the Dantzig selector-which is a solution to the l{$_1$}-regularization problem \${\textbackslash}underset {\textbackslash}tilde\{{\textbackslash}beta\}{\textbackslash}in \{{\textbackslash}bf R\}{\textasciicircum}\{p\}{\textbackslash}to\{\{{\textbackslash}rm min\}\}{\textbackslash}{\textbar}{\textbackslash}tilde\{{\textbackslash}beta\}{\textbackslash}{\textbar}\_\{{\textbackslash}ell \_\{1\}\}\$ subject to \${\textbackslash}{\textbar}X{\textasciicircum}\{{\textbackslash}ast \}r{\textbackslash}{\textbar}\_\{{\textbackslash}ell \_\{{\textbackslash}infty\}\}{\textbackslash}leq (1+t{\textasciicircum}\{-1\}){\textbackslash}sqrt\{2{\textbackslash},\{{\textbackslash}rm log\}{\textbackslash},p\}{\textbackslash}cdot {\textbackslash}sigma \$, where r is the residual vector \$y-X{\textbackslash}tilde\{{\textbackslash}beta\}\$ and t is a positive scalar. We show that if X obeys a uniform uncertainty principle (with unit-normed columns) and if the true parameter vector {$\beta$} is sufficiently sparse (which here roughly guarantees that the model is identifiable), then with very large probability, \${\textbackslash}{\textbar}{\textbackslash}hat\{{\textbackslash}beta\}-{\textbackslash}beta {\textbackslash}{\textbar}\_\{{\textbackslash}ell \_\{2\}\}{\textasciicircum}\{2\}{\textbackslash}leq C{\textasciicircum}\{2\}{\textbackslash}cdot 2{\textbackslash},\{{\textbackslash}rm log\}{\textbackslash},p{\textbackslash}cdot {\textbackslash}left({\textbackslash}sigma {\textasciicircum}\{2\}+{\textbackslash}sum\_\{i\}\{{\textbackslash}rm min\}({\textbackslash}beta \_\{i\}{\textasciicircum}\{2\},{\textbackslash}sigma {\textasciicircum}\{2\}){\textbackslash}right)\$. Our results are nonasymptotic and we give values for the constant C. Even though n may be much smaller than p, our estimator achieves a loss within a logarithmic factor of the ideal mean squared error one would achieve with an oracle which would supply perfect information about which coordinates are nonzero, and which were above the noise level. In multivariate regression and from a model selection viewpoint, our result says that it is possible nearly to select the best subset of variables by solving a very simple convex program, which, in fact, can easily be recast as a convenient linear program (LP).},
  keywords = {â“Multiple DOI},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DIJKIICG/candes_the_2007.pdf}
}

@article{candes_decoding_2005,
  title = {Decoding by Linear Programming},
  author = {Candes, Emmanuel J and Tao, Terence},
  year = {2005},
  journal = {IEEE transactions on information theory},
  volume = {51},
  number = {12},
  pages = {4203--4215},
  publisher = {IEEE}
}

@article{candes_modern_2006,
  title = {Modern Statistical Estimation via Oracle Inequalities},
  author = {Cand{\`e}s, Emmanuel J.},
  year = {2006},
  month = may,
  journal = {Acta Numerica},
  volume = {15},
  pages = {257--325},
  issn = {0962-4929, 1474-0508},
  doi = {10.1017/S0962492906230010},
  urldate = {2022-12-28},
  abstract = {A number of fundamental results in modern statistical theory involve thresholding estimators. This survey paper aims at reconstructing the history of how thresholding rules came to be popular in statistics and describing, in a not overly technical way, the domain of their application. Two notions play a fundamental role in our narrative: sparsity and oracle inequalities. Sparsity is a property of the object to estimate, which seems to be characteristic of many modern problems, in statistics as well as applied mathematics and theoretical computer science, to name a few. `Oracle inequalities' are a powerful decision-theoretic tool which has served to understand the optimality of thresholding rules, but which has many other potential applications, some of which we will discuss.             Our story is also the story of the dialogue between statistics and applied harmonic analysis. Starting with the work of Wiener, we will see that certain representations emerge as being optimal for estimation. A leitmotif throughout our exposition is that efficient representations lead to efficient estimation.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MNXJ6M29/CandÃ¨s - 2006 - Modern statistical estimation via oracle inequalit.pdf}
}

@article{cantelmo_adaptive_2010,
  title = {Adaptive Model Selection for Polynomial {{NARX}} Models},
  author = {Cantelmo, C. and Piroddi, L.},
  year = {2010},
  month = dec,
  journal = {IET Control Theory \& Applications},
  volume = {4},
  number = {12},
  pages = {2693--2706},
  issn = {1751-8644, 1751-8652},
  doi = {10.1049/iet-cta.2009.0581},
  urldate = {2017-09-13},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9VGXII2J/cantelmo_adaptive_2010.pdf;/Users/antoniohortaribeiro/Zotero/storage/EDSS4K63/cantelmo_adaptive_2010.pdf}
}

@article{cantwell_rethinking_2018,
  title = {Rethinking Multiscale Cardiac Electrophysiology with Machine Learning and Predictive Modelling},
  author = {Cantwell, Chris D. and Mohamied, Yumnah and Tzortzis, Konstantinos N. and Garasto, Stef and Houston, Charles and Chowdhury, Rasheda A. and Ng, Fu Siong and Bharath, Anil A. and Peters, Nicholas S.},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.04227},
  eprint = {1810.04227},
  urldate = {2018-10-20},
  abstract = {We review some of the latest approaches to analysing cardiac electrophysiology data using machine learning and predictive modelling. Cardiac arrhythmias, particularly atrial fibrillation, are a major global healthcare challenge. Treatment is often through catheter ablation, which involves the targeted localized destruction of regions of the myocardium responsible for initiating or perpetuating the arrhythmia. Ablation targets are either anatomically defined, or identified based on their functional properties as determined through the analysis of contact intracardiac electrograms acquired with increasing spatial density by modern electroanatomic mapping systems. While numerous quantitative approaches have been investigated over the past decades for identifying these critical curative sites, few have provided a reliable and reproducible advance in success rates. Machine learning techniques, including recent deep-learning approaches, offer a potential route to gaining new insight from this wealth of highly complex spatio-temporal information that existing methods struggle to analyse. Coupled with predictive modelling, these techniques offer exciting opportunities to advance the field and produce more accurate diagnoses and robust personalised treatment. We outline some of these methods and illustrate their use in making predictions from the contact electrogram and augmenting predictive modelling tools, both by more rapidly predicting future states of the system and by inferring the parameters of these models from experimental observations.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Machine Learning,Mathematics - Dynamical Systems,Quantitative Biology - Tissues and Organs,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TPBVN54H/cantwell_rethinking_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/5JPGVNMK/1810.html}
}

@article{cardoso_longitudinal_2016,
  title = {Longitudinal Study of Patients with Chronic {{Chagas}} Cardiomyopathy in {{Brazil}} ({{SaMi-Trop}} Project): A Cohort Profile},
  shorttitle = {Longitudinal Study of Patients with Chronic {{Chagas}} Cardiomyopathy in {{Brazil}} ({{SaMi-Trop}} Project)},
  author = {Cardoso, Clareci Silva and Sabino, Ester Cerdeira and Oliveira, Claudia Di Lorenzo and de Oliveira, Lea Campos and Ferreira, Ariela Mota and {Cunha-Neto}, Ed{\'e}cio and Bierrenbach, Ana Luiza and Ferreira, Jo{\~a}o Eduardo and Haikal, Desir{\'e}e Sant'Ana and Reingold, Arthur L. and Ribeiro, Antonio Luiz P.},
  year = {2016},
  month = may,
  journal = {BMJ Open},
  volume = {6},
  number = {5},
  pages = {e011181},
  publisher = {British Medical Journal Publishing Group},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2016-011181},
  urldate = {2021-11-25},
  abstract = {Purpose We have established a prospective cohort of 1959 patients with chronic Chagas cardiomyopathy to evaluate if a clinical prediction rule based on ECG, brain natriuretic peptide (BNP) levels, and other biomarkers can be useful in clinical practice. This paper outlines the study and baseline characteristics of the participants. Participants The study is being conducted in 21 municipalities of the northern part of Minas Gerais State in Brazil, and includes a follow-up of 2 years. The baseline evaluation included collection of sociodemographic information, social determinants of health, health-related behaviours, comorbidities, medicines in use, history of previous treatment for Chagas disease, functional class, quality of life, blood sample collection, and ECG. Patients were mostly female, aged 50--74 years, with low family income and educational level, with known Chagas disease for {$>$}10 years; 46\% presented with functional class {$>$}II. Previous use of benznidazole was reported by 25.2\% and permanent use of pacemaker by 6.2\%. Almost half of the patients presented with high blood cholesterol and hypertension, and one-third of them had diabetes mellitus. N-terminal of the prohormone BNP (NT-ProBNP) level was {$>$}300 pg/mL in 30\% of the sample. Findings to date Clinical and laboratory markers predictive of severe and progressive Chagas disease were identified as high NT-ProBNP levels, as well as symptoms of advanced heart failure. These results confirm the important residual morbidity of Chagas disease in the remote areas, thus supporting political decisions that should prioritise in addition to epidemiological surveillance the medical treatment of chronic Chagas cardiomyopathy in the coming years. The S{\~a}o Paulo-Minas Gerais Tropical Medicine Research Center (SaMi-Trop) represents a major challenge for focused research in neglected diseases, with knowledge that can be applied in primary healthcare. Future plans We will continue following this patients' cohort to provide relevant information about the development and progression of Chagas disease in remotes areas, with social and economic inequalities. Trial registration number NCT02646943; Pre-results.},
  chapter = {Cardiovascular medicine},
  copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://www.bmj.com/company/products-services/rights-and-licensing/. This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  pmid = {27147390},
  keywords = {Biomarkers,Chagas disease,CHEMICAL PATHOLOGY,Cohort Studies},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FQ5Y9NRN/Cardoso et al_2016_Longitudinal study of patients with chronic Chagas cardiomyopathy in Brazil.pdf;/Users/antoniohortaribeiro/Zotero/storage/KUWEL87V/e011181.html}
}

@article{carin_deep_2018,
  title = {On {{Deep Learning}} for {{Medical Image Analysis}}},
  author = {Carin, Lawrence and Pencina, Michael J.},
  year = {2018},
  month = sep,
  journal = {JAMA},
  volume = {320},
  number = {11},
  pages = {1192--1193},
  issn = {0098-7484},
  doi = {10.1001/jama.2018.13316},
  urldate = {2018-10-20},
  abstract = {This JAMA Guide to Statistics and Methods explains the basic concepts underlying convolutional neural networks (CNNs), a type of machine learning being used to automate the reading of medical images.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GAAVCS7Y/2702856.html}
}

@inproceedings{carlini_certified_2023,
  title = {({{Certified}}!!) {{Adversarial Robustness}} for {{Free}}!},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Carlini, Nicholas and Tramer, Florian and Dvijotham, Krishnamurthy Dj and Rice, Leslie and Sun, Mingjie and Kolter, J. Zico},
  year = {2023},
  month = mar,
  eprint = {2206.10550},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.10550},
  urldate = {2023-04-13},
  abstract = {In this paper we show how to achieve state-of-the-art certified adversarial robustness to 2-norm bounded perturbations by relying exclusively on off-the-shelf pretrained models. To do so, we instantiate the denoised smoothing approach of Salman et al. 2020 by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This allows us to certify 71\% accuracy on ImageNet under adversarial perturbations constrained to be within an 2-norm of 0.5, an improvement of 14 percentage points over the prior certified SoTA using any approach, or an improvement of 30 percentage points over denoised smoothing. We obtain these results using only pretrained diffusion models and image classifiers, without requiring any fine tuning or retraining of model parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/S6J5XZDI/Carlini et al. - 2023 - (Certified!!) Adversarial Robustness for Free!.pdf;/Users/antoniohortaribeiro/Zotero/storage/CCM4BH7R/2206.html}
}

@article{carraro_indirect_2014,
  title = {Indirect Multiple Shooting for Nonlinear Parabolic Optimal Control Problems with Control Constraints},
  author = {Carraro, T and Geiger, Michael and Rannacher, Rolf},
  year = {2014},
  journal = {SIAM Journal on Scientific Computing},
  volume = {36},
  number = {2},
  pages = {A452-A481},
  issn = {1064-8275},
  doi = {10.1137/120895809}
}

@article{carraro_indirect_2014a,
  title = {Indirect {{Multiple Shooting}} for {{Nonlinear Parabolic Optimal Control Problems}} with {{Control Constraints}}},
  author = {Carraro, T and Geiger, Michael and Rannacher, Rolf},
  year = {2014},
  journal = {SIAM Journal on Scientific Computing},
  volume = {36},
  number = {2},
  pages = {A452-A481},
  issn = {1064-8275},
  doi = {10/f54rgr}
}

@article{carratino_learning_2019,
  title = {Learning with {{SGD}} and {{Random Features}}},
  author = {Carratino, Luigi and Rudi, Alessandro and Rosasco, Lorenzo},
  year = {2019},
  month = jan,
  journal = {arXiv:1807.06343 [cs, stat]},
  eprint = {1807.06343},
  primaryclass = {cs, stat},
  urldate = {2020-08-10},
  abstract = {Sketching and stochastic gradient methods are arguably the most common techniques to derive efficient large scale learning algorithms. In this paper, we investigate their application in the context of nonparametric statistical learning. More precisely, we study the estimator defined by stochastic gradient with mini batches and random features. The latter can be seen as form of nonlinear sketching and used to define approximate kernel methods. The considered estimator is not explicitly penalized/constrained and regularization is implicit. Indeed, our study highlights how different parameters, such as number of features, iterations, step-size and mini-batch size control the learning properties of the solutions. We do this by deriving optimal finite sample bounds, under standard assumptions. The obtained results are corroborated and illustrated by numerical experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SID2WGUD/Carratino et al. - 2019 - Learning with SGD and Random Features.pdf;/Users/antoniohortaribeiro/Zotero/storage/WW7TJBU4/1807.html}
}

@article{castro_causality_2020,
  title = {Causality Matters in Medical Imaging},
  author = {Castro, Daniel C. and Walker, Ian and Glocker, Ben},
  year = {2020},
  month = jul,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {3673},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17478-w},
  urldate = {2023-04-04},
  abstract = {Causal reasoning can shed new light on the major challenges in machine learning for medical imaging: scarcity of high-quality annotated data and mismatch between the development dataset and the target environment. A causal perspective on these issues allows decisions about data collection, annotation, preprocessing, and learning strategies to be made and scrutinized more transparently, while providing a detailed categorisation of potential biases and mitigation techniques. Along with worked clinical examples, we highlight the importance of establishing the causal relationship between images and their annotations, and offer step-by-step recommendations for future studies.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Computational models,Data processing,Machine learning,Medical research,Predictive medicine},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NULI6FTS/Castro et al_2020_Causality matters in medical imaging.pdf}
}

@article{cauchy_methode_1847,
  title = {M{\'e}thode G{\'e}n{\'e}rale Pour La R{\'e}solution Des Systemes D{\'e}quations Simultan{\'e}es},
  author = {Cauchy, Augustin},
  year = {1847},
  journal = {Comp. Rend. Sci. Paris},
  volume = {25},
  number = {1847},
  pages = {536--538},
  keywords = {ðŸ”No DOI found}
}

@article{chandar_nonsaturating_2019,
  title = {Towards {{Non-Saturating Recurrent Units}} for {{Modelling Long-Term Dependencies}}},
  author = {Chandar, Sarath and Sankar, Chinnadhurai and Vorontsov, Eugene and Kahou, Samira Ebrahimi and Bengio, Yoshua},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  pages = {3280--3287},
  issn = {2374-3468, 2159-5399},
  doi = {10/gf8gdm},
  urldate = {2019-09-18},
  abstract = {Modelling long-term dependencies is a challenge for recurrent neural networks. This is primarily due to the fact that gradients vanish during training, as the sequence length increases. Gradients can be attenuated by transition operators and are attenuated or dropped by activation functions. Canonical architectures like LSTM alleviate this issue by skipping information through a memory mechanism. We propose a new recurrent architecture (Non-saturating Recurrent Unit; NRU) that relies on a memory mechanism but forgoes both saturating activation functions and saturating gates, in order to further alleviate vanishing gradients. In a series of synthetic and real world tasks, we demonstrate that the proposed model is the only model that performs among the top 2 models across all tasks with and without long-term dependencies, when compared against a range of other architectures.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SDKMP7DI/Chandar et al. - 2019 - Towards Non-Saturating Recurrent Units for Modelli.pdf}
}

@article{chandra_coevolutionary_2017,
  title = {Co-Evolutionary Multi-Task Learning with Predictive Recurrence for Multi-Step Chaotic Time Series Prediction},
  author = {Chandra, Rohitash and Ong, Yew-Soon and Goh, Chi-Keong},
  year = {2017},
  month = jun,
  journal = {Neurocomputing},
  volume = {243},
  pages = {21--34},
  issn = {09252312},
  doi = {10.1016/j.neucom.2017.02.065},
  urldate = {2018-05-07},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EQQFSJ6R/chandra_co-evoluti_2017.pdf}
}

@article{chandra_competition_2015,
  title = {Competition and {{Collaboration}} in {{Cooperative Coevolution}} of {{Elman Recurrent Neural Networks}} for {{Time-Series Prediction}}},
  author = {Chandra, Rohitash},
  year = {2015},
  month = dec,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {26},
  number = {12},
  pages = {3123--3136},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2015.2404823},
  urldate = {2018-05-07},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HI4ZTVM4/chandra_competitio_2015.pdf}
}

@inproceedings{chang_realtime_2007,
  title = {Real-Time {{DSP}} Implementation on Local Stereo Matching},
  booktitle = {Multimedia and {{Expo}}, 2007 {{IEEE International Conference}} On},
  author = {Chang, Nelson and Lin, Ting-Min and Tsai, Tsung-Hsien and Tseng, Yu-Cheng and Chang, Tian-Sheuan},
  year = {2007},
  pages = {2090--2093},
  publisher = {IEEE}
}

@article{chatterji_foolish_2022,
  title = {Foolish {{Crowds Support Benign Overfitting}}},
  author = {Chatterji, Niladri S. and Long, Philip M.},
  year = {2022},
  month = mar,
  journal = {arXiv:2110.02914},
  eprint = {2110.02914},
  urldate = {2022-05-18},
  abstract = {We prove a lower bound on the excess risk of sparse interpolating procedures for linear regression with Gaussian data in the overparameterized regime. We apply this result to obtain a lower bound for basis pursuit (the minimum \${\textbackslash}ell\_1\$-norm interpolant) that implies that its excess risk can converge at an exponentially slower rate than OLS (the minimum \${\textbackslash}ell\_2\$-norm interpolant), even when the ground truth is sparse. Our analysis exposes the benefit of an effect analogous to the "wisdom of the crowd", except here the harm arising from fitting the \${\textbackslash}textit\{noise\}\$ is ameliorated by spreading it among many directions -- the variance reduction arises from a \${\textbackslash}textit\{foolish\}\$ crowd.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/LTYLYQ3B/Chatterji and Long - 2022 - Foolish Crowds Support Benign Overfitting.pdf;/Users/antoniohortaribeiro/Zotero/storage/6A8ESGSZ/2110.html}
}

@article{chattopadhyay_datadriven_2020,
  title = {Data-Driven Prediction of a Multi-Scale {{Lorenz}} 96 Chaotic System Using Deep Learning Methods: {{Reservoir}} Computing, {{ANN}}, and {{RNN-LSTM}}},
  shorttitle = {Data-Driven Prediction of a Multi-Scale {{Lorenz}} 96 Chaotic System Using Deep Learning Methods},
  author = {Chattopadhyay, Ashesh and Hassanzadeh, Pedram and Subramanian, Devika},
  year = {2020},
  month = jul,
  journal = {Nonlinear Processes in Geophysics},
  volume = {27},
  number = {3},
  eprint = {1906.08829},
  pages = {373--389},
  issn = {1607-7946},
  doi = {10.5194/npg-27-373-2020},
  urldate = {2021-04-01},
  abstract = {In this paper, the performance of three deep learning methods for predicting short-term evolution and for reproducing the long-term statistics of a multi-scale spatio-temporal Lorenz 96 system is examined. The methods are: echo state network (a type of reservoir computing, RC-ESN), deep feed-forward artificial neural network (ANN), and recurrent neural network with long short-term memory (RNN-LSTM). This Lorenz 96 system has three tiers of nonlinearly interacting variables representing slow/large-scale (\$X\$), intermediate (\$Y\$), and fast/small-scale (\$Z\$) processes. For training or testing, only \$X\$ is available; \$Y\$ and \$Z\$ are never known or used. We show that RC-ESN substantially outperforms ANN and RNN-LSTM for short-term prediction, e.g., accurately forecasting the chaotic trajectories for hundreds of numerical solver's time steps, equivalent to several Lyapunov timescales. The RNN-LSTM and ANN show some prediction skills as well; RNN-LSTM bests ANN. Furthermore, even after losing the trajectory, data predicted by RC-ESN and RNN-LSTM have probability density functions (PDFs) that closely match the true PDF, even at the tails. The PDF of the data predicted using ANN, however, deviates from the true PDF. Implications, caveats, and applications to data-driven and data-assisted surrogate modeling of complex nonlinear dynamical systems such as weather/climate are discussed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Nonlinear Sciences - Chaotic Dynamics,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/N7KEJJ3I/Chattopadhyay et al. - 2020 - Data-driven prediction of a multi-scale Lorenz 96 .pdf;/Users/antoniohortaribeiro/Zotero/storage/VYW6AQ4Z/1906.html}
}

@inproceedings{chaudhari_stochastic_2018,
  title = {Stochastic {{Gradient Descent Performs Variational Inference}}, {{Converges}} to {{Limit Cycles}} for {{Deep Networks}}},
  booktitle = {2018 {{Information Theory}} and {{Applications Workshop}} ({{ITA}})},
  author = {Chaudhari, Pratik and Soatto, Stefano},
  year = {2018},
  month = feb,
  pages = {1--10},
  publisher = {IEEE},
  address = {San Diego, CA},
  doi = {10/gf3d8t},
  urldate = {2019-06-01},
  abstract = {Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such ``out-of-equilibrium'' behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1\% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.},
  isbn = {978-1-72810-124-8},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4GIJNKVC/chaudhari_stochastic_2018.pdf}
}

@article{chen_atomic_1998,
  title = {Atomic Decomposition by Basis Pursuit},
  author = {Chen, Scott Shaobing and Donoho, David L. and Saunders, Michael A.},
  year = {1998},
  journal = {SIAM Journal on Scientific Computing},
  volume = {20},
  number = {1},
  eprint = {https://doi.org/10.1137/S1064827596304010},
  pages = {33--61},
  doi = {10.1137/S1064827596304010},
  abstract = {The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB).Basis Pursuit (BP) is a principle for decomposing a signal into an "optimal" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising.BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.}
}

@article{chen_detection_2020,
  title = {Detection and {{Classification}} of {{Cardiac Arrhythmias}} by a {{Challenge-Best Deep Learning Neural Network Model}}},
  author = {Chen, Tsai-Min and Huang, Chih-Han and Shih, Edward S.C. and Hu, Yu-Feng and Hwang, Ming-Jing},
  year = {2020},
  month = mar,
  journal = {iScience},
  volume = {23},
  number = {3},
  pages = {100886},
  issn = {25890042},
  doi = {10.1016/j.isci.2020.100886},
  urldate = {2020-06-25},
  abstract = {Electrocardiograms (ECGs) are widely used to clinically detect cardiac arrhythmias (CAs). They are also being used to develop computer-assisted methods for heart disease diagnosis. We have developed a convolution neural network model to detect and classify CAs, using a large 12-lead ECG dataset (6,877 recordings) provided by the China Physiological Signal Challenge (CPSC) 2018. Our model, which was ranked first in the challenge competition, achieved a median overall F1-score of 0.84 for the nine-type CA classification of CPSC2018's hidden test set of 2,954 ECG recordings. Further analysis showed that concurrent CAs were adequately predictive for 476 patients with multiple types of CA diagnoses in the dataset. Using only single-lead data yielded a performance that was only slightly worse than using the full 12-lead data, with leads aVR and V1 being the most prominent. We extensively consider these results in the context of their agreement with and relevance to clinical observations.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AX748WIL/Chen et al. - 2020 - Detection and Classification of Cardiac Arrhythmia.pdf}
}

@book{chen_linear_1998,
  title = {Linear System Theory and Design, Holt, Winehart and Winston},
  author = {Chen, C.T.},
  year = {1998},
  edition = {3rd},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XGHC5KAS/Linear System Theory And Design - Chi-Tsong Chen.pdf}
}

@inproceedings{chen_multiple_2021,
  title = {Multiple {{Descent}}: {{Design Your Own Generalization Curve}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Lin and Min, Yifei and Belkin, Mikhail and Karbasi, Amin},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
  year = {2021},
  volume = {34},
  pages = {8898--8912},
  publisher = {Curran Associates, Inc.}
}

@incollection{chen_neural_2018,
  title = {Neural {{Ordinary Differential Equations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {6571--6582},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-04},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EB3PRSZC/chen_neural_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/KFR5X777/chen_neural_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/6QXRRJUM/7892-neural-ordinary-differential-equations.html}
}

@article{chen_nonlinear_1990,
  title = {Non-Linear System Identification Using Neural Networks},
  author = {Chen, Sheng and Billings, S. A. and Grant, P. M.},
  year = {1990},
  journal = {International Journal of Control},
  volume = {51},
  number = {6},
  pages = {1191--1214},
  keywords = {â“Multiple DOI}
}

@article{chen_nonlinear_1990a,
  title = {Non-{{Linear System Identification Using Neural Networks}}},
  author = {Chen, Sheng and Billings, S. A. and Grant, P. M.},
  year = {1990},
  journal = {International Journal of Control},
  volume = {51},
  number = {6},
  pages = {1191--1214},
  doi = {10/cg8bhx},
  annotation = {01127}
}

@article{chen_orthogonal_1989,
  title = {Orthogonal Least Squares Methods and Their Application to Non-Linear System Identification},
  author = {Chen, Sheng and Billings, Stephen A and Luo, Wan},
  year = {1989},
  journal = {International Journal of Control},
  volume = {50},
  number = {5},
  pages = {1873--1896},
  doi = {10.1080/00207178908953472},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CQ4GKVMN/chen_orthogonal_1989.pdf}
}

@article{chen_orthogonal_1989a,
  title = {Orthogonal {{Least Squares Methods}} and {{Their Application}} to {{Non-Linear System Identification}}},
  author = {Chen, Sheng and Billings, Stephen A and Luo, Wan},
  year = {1989},
  journal = {International Journal of Control},
  volume = {50},
  number = {5},
  pages = {1873--1896},
  doi = {10/dd4g9h},
  annotation = {01591}
}

@article{chen_orthogonalleastsquares_2009,
  title = {Orthogonal-Least-Squares Regression: {{A}} Unified Approach for Data Modelling},
  author = {Chen, Sheng and Hong, Xia and Luk, Bing Lam and Harris, Chris J},
  year = {2009},
  journal = {Neurocomputing},
  volume = {72},
  number = {10},
  pages = {2670--2681},
  keywords = {ðŸ”No DOI found}
}

@article{chen_practical_1990,
  title = {Practical Identification of {{NARMAX}} Models Using Radial Basis Functions},
  author = {Chen, S. and Billings, S. A. and Cowan, C. F. N. and Grant, P. M.},
  year = {1990},
  journal = {International Journal of Control},
  volume = {52},
  number = {6},
  pages = {1327--1350},
  doi = {10.1080/00207179008953599}
}

@article{chen_representations_1989,
  title = {Representations of Non-Linear Systems: The {{NARMAX}} Model},
  author = {Chen, S and Billings, {\relax SA}},
  year = {1989},
  journal = {International Journal of Control},
  volume = {49},
  number = {3},
  pages = {1013--1032},
  doi = {10.1080/00207178908559683}
}

@article{chen_system_2014,
  title = {System Identification via Sparse Multiple Kernel-Based Regularization Using Sequential Convex Optimization Techniques},
  author = {Chen, Tianshi and Andersen, Martin S. and Ljung, Lennart and Chiuso, Alessandro and Pillonetto, Gianluigi},
  year = {2014},
  journal = {IEEE Transactions on Automatic Control},
  volume = {59},
  number = {11},
  pages = {2933--2945},
  doi = {10.1109/TAC.2014.2351851},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MQ3IC5DP/chen_system_2014.pdf}
}

@article{cheng_adaptive_2009,
  title = {Adaptive Neural Network Tracking Control for Manipulators with Uncertain Kinematics, Dynamics and Actuator Model},
  author = {Cheng, Long and Hou, Zeng-Guang and Tan, Min},
  year = {2009},
  month = oct,
  journal = {Automatica},
  volume = {45},
  number = {10},
  pages = {2312--2318},
  issn = {0005-1098},
  doi = {10/cghh6t},
  abstract = {A neural-network-based adaptive controller is proposed for the tracking problem of manipulators with uncertain kinematics, dynamics and actuator model. The adaptive Jacobian scheme is used to estimate the unknown kinematics parameters. Uncertainties in the manipulator dynamics and actuator model are compensated by three-layer neural networks. External disturbances and approximation errors are counteracted by robust signals. The actuator controller is designed based on the backstepping scheme. Compared with the existing work, the proposed method considers the manipulator kinematics uncertainty, does not need the ``linearity-in-parameters'' assumption for the uncertain terms in the dynamics of manipulator and actuator, and guarantees the tracking error to be as small as desired. Finally, the performance of the proposed approach is illustrated by the simulation example.},
  keywords = {Manipulators,Neural networks,Tracking,Uncertainty}
}

@incollection{cheng_errors_1998,
  title = {Errors in Variables in Econometrics},
  booktitle = {Econometrics in Theory and Practice: {{Festschrift}} for Hans Schneewei{\ss}},
  author = {Cheng, Chi-Lun and Van Ness, John W.},
  editor = {Galata, Robert and K{\"u}chenhoff, Helmut},
  year = {1998},
  pages = {3--13},
  publisher = {Physica-Verlag HD},
  address = {Heidelberg},
  doi = {10.1007/978-3-642-47027-1_1},
  abstract = {This article discusses the use of instrumental variables and grouping methods in the linear errors-in-variables or measurement error model. Comparisons are made between these methods, standard measurement error model methods with side conditions, least squares methods, and replicated models. It is demonstrated that there are close relationships between these apparently diverse estimation techniques.},
  isbn = {978-3-642-47027-1}
}

@misc{chernozhukov_double_2017,
  title = {Double/{{Debiased Machine Learning}} for {{Treatment}} and {{Causal Parameters}}},
  author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
  year = {2017},
  month = dec,
  number = {arXiv:1608.00060},
  eprint = {1608.00060},
  primaryclass = {econ, stat},
  publisher = {arXiv},
  urldate = {2023-08-30},
  abstract = {We revisit the classic semiparametric problem of inference on a low dimensional parameter {\texttheta}0 in the presence of high-dimensional nuisance parameters {$\eta$}0. We depart from the classical setting by allowing for {$\eta$}0 to be so high-dimensional that the traditional assumptions, such as Donsker properties, that limit complexity of the parameter space for this object break down. To estimate {$\eta$}0, we consider the use of statistical or machine learning (ML) methods which are particularly well-suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating {$\eta$}0 cause a heavy bias in estimators of {\texttheta}0 that are obtained by naively plugging ML estimators of {$\eta$}0 into estimating equations for {\texttheta}0. This bias results in the naive estimator failing to be N -1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest {\texttheta}0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate {\texttheta}0, and (2) making use of cross-fitting which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in a N -1/2-neighborhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of DML applied to learn the main regression parameter in a partially linear regression model, DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model, DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness, and DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62G,Economics - Econometrics,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DH6DW294/Chernozhukov et al. - 2017 - DoubleDebiased Machine Learning for Treatment and.pdf}
}

@inproceedings{chiarugi_adaptive_2007,
  title = {Adaptive Threshold {{QRS}} Detector with Best Channel Selection Based on a Noise Rating System},
  booktitle = {2007 {{Computers}} in {{Cardiology}}},
  author = {Chiarugi, F. and Sakkalis, V. and Emmanouilidou, D. and Krontiris, T. and Varanini, M. and Tollis, I.},
  year = {2007},
  month = sep,
  pages = {157--160},
  doi = {10.1109/CIC.2007.4745445},
  abstract = {QRS detection performance can depend on the type of noise present in each lead involved in the overall processing. A common approach to QRS detection is based on a QRS enhanced signal obtained from the derivatives of the pre-filtered leads. However, the signal pre-filtering cannot be able to perform a complete noise rejection and the use of derivatives can enhance the noise as well. In many cases the noise occurs only on one lead and the addition of a noisy lead to the QRS enhanced signal decreases the overall detection performances of the QRS detector. For this reason the noise estimation on each channel, providing information for the channel inclusion or rejection in building the QRS enhanced signal, can improve the overall performances of the QRS detector. The results have been evaluated on the 48 records of the MIT-BIH Arrhythmia Database where each ECG record is composed by 2 leads sampled at 360 Hz for a total duration of about 30 minutes. The annotated QRSs are 109494 in total. The results have been very satisfying on all the annotated QRSs and, with the inclusion of an automatic criterion for ventricular flutter detection, a sensitivity=99.76\% and a positive predictive value=99.81\% have been obtained.},
  keywords = {adaptive threshold QRS detector,Band pass filters,Cardiology,Computer science,Databases,Detectors,ECG signal recording,electrocardiogram,electrocardiography,filtering theory,Frequency,frequency 360 Hz,medical information systems,medical signal detection,medical signal processing,MIT-BIH arrhythmia database,Noise level,noise rating system,Noise reduction,nonlinear filters,prefiltered signal,QRS detection algorithm,QRS enhanced signal,signal noise estimation,time 30 min,ventricular flutter detection},
  file = {/Users/antoniohortaribeiro/Zotero/storage/V463C344/chiarugi_adaptive_2007.pdf;/Users/antoniohortaribeiro/Zotero/storage/TMA3M8MT/4745445.html}
}

@article{chicco_machine_2020,
  title = {Machine Learning Can Predict Survival of Patients with Heart Failure from Serum Creatinine and Ejection Fraction Alone},
  author = {Chicco, Davide and Jurman, Giuseppe},
  year = {2020},
  month = feb,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {20},
  number = {1},
  pages = {16},
  issn = {1472-6947},
  doi = {10.1186/s12911-020-1023-5},
  abstract = {Cardiovascular diseases kill approximately 17 million people globally every year, and they mainly exhibit as myocardial infarctions and heart failures. Heart failure (HF) occurs when the heart cannot pump enough blood to meet the needs of the body.Available electronic medical records of patients quantify symptoms, body features, and clinical laboratory test values, which can be used to perform biostatistics analysis aimed at highlighting patterns and correlations otherwise undetectable by medical doctors. Machine learning, in particular, can predict patients' survival from their data and can individuate the most important features among those included in their medical records.}
}

@article{chiu_stateoftheart_2017,
  title = {State-of-the-Art {{Speech Recognition With Sequence-to-Sequence Models}}},
  author = {Chiu, Chung-Cheng and Sainath, Tara N. and Wu, Yonghui and Prabhavalkar, Rohit and Nguyen, Patrick and Chen, Zhifeng and Kannan, Anjuli and Weiss, Ron J. and Rao, Kanishka and Gonina, Ekaterina and Jaitly, Navdeep and Li, Bo and Chorowski, Jan and Bacchiani, Michiel},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.01769 [cs, eess, stat]},
  eprint = {1712.01769},
  primaryclass = {cs, eess, stat},
  abstract = {Attention-based encoder-decoder architectures such as Listen, Attend, and Spell (LAS), subsume the acoustic, pronunciation and language model components of a traditional automatic speech recognition (ASR) system into a single neural network. In previous work, we have shown that such architectures are comparable to state-of-theart ASR systems on dictation tasks, but it was not clear if such architectures would be practical for more challenging tasks such as voice search. In this work, we explore a variety of structural and optimization improvements to our LAS model which significantly improve performance. On the structural side, we show that word piece models can be used instead of graphemes. We also introduce a multi-head attention architecture, which offers improvements over the commonly-used single-head attention. On the optimization side, we explore synchronous training, scheduled sampling, label smoothing, and minimum word error rate optimization, which are all shown to improve accuracy. We present results with a unidirectional LSTM encoder for streaming recognition. On a 12, 500 hour voice search task, we find that the proposed changes improve the WER from 9.2\% to 5.6\%, while the best conventional system achieves 6.7\%; on a dictation task our model achieves a WER of 4.1\% compared to 5\% for the conventional system.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/V2IARUNZ/chiu_state-of-t_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/EHKVK3PD/1712.html}
}

@inproceedings{chiuso_nonparametric_2010,
  title = {Nonparametric Sparse Estimators for Identification of Large Scale Linear Systems},
  booktitle = {49th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Chiuso, A. and Pillonetto, G.},
  year = {2010},
  month = dec,
  pages = {2942--2947},
  doi = {10.1109/CDC.2010.5717169},
  abstract = {Identification of sparse high dimensional linear systems pose sever challenges to off-the-shelf techniques for system identification. This is particularly so when relatively small data sets, as compared to the number of inputs and outputs, have to be used. In this paper we introduce a new nonparametric technique which borrows ideas from a recently introduced Kernel estimator called ``stable-spline'' as well as from sparsity inducing priors which use {$\ell$}1 penalty. We compare the new method with a group LAR-type of algorithm applied to estimation of sparse Vector Autoregressive models and to standard PEM methods.},
  keywords = {autoregressive processes,Data models,Estimation,group LAR-type,Hidden Markov models,identification,Kernel,Kernel estimator,large scale linear systems,linear systems,nonparametric sparse estimator,nonparametric statistics,nonparametric technique,off-the-shelf techniques,Predictive models,sparse high dimensional linear systems,sparse vector autoregressive model,stable spline,standard PEM method,system identification,Vectors},
  file = {/Users/antoniohortaribeiro/Zotero/storage/R4A4AVQI/chiuso_nonparamet_2010.pdf;/Users/antoniohortaribeiro/Zotero/storage/M77QZFXR/5717169.html}
}

@inproceedings{chizat_global_2018,
  title = {On the Global Convergence of Gradient Descent for Over-Parameterized Models Using Optimal Transport},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chizat, L{\'e}na{\"i}c and Bach, Francis},
  year = {2018},
  volume = {31},
  file = {/Users/antoniohortaribeiro/Zotero/storage/P8ZG8K8A/Chizat and Bach - 2018 - On the Global Convergence of Gradient Descent for .pdf}
}

@article{chizat_lazy_2019,
  title = {On {{Lazy Training}} in {{Differentiable Programming}}},
  author = {Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  year = {2019},
  journal = {Advances in Neural Information Processing Systems 32},
  eprint = {1812.07956},
  abstract = {In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this "lazy training" phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely that "lazy training" is behind the many successes of neural networks in difficult high dimensional tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4U6BEJRW/Chizat et al. - 2020 - On Lazy Training in Differentiable Programming.pdf;/Users/antoniohortaribeiro/Zotero/storage/B5W9PRJ3/1812.html}
}

@inproceedings{cho_kernel_2009,
  title = {Kernel Methods for Deep Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Cho, Youngmin and Saul, Lawrence},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. and Williams, C. and Culotta, A.},
  year = {2009},
  volume = {22},
  publisher = {Curran Associates, Inc.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Z896AUNP/Cho and Saul - Kernel Methods for Deep Learning.pdf}
}

@inproceedings{cho_learning_2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}--{{Decoder}} for {{Statistical Machine Translation}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Cho, Kyunghyun and {van Merri{\"e}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = oct,
  pages = {1724--1734},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1179},
  urldate = {2019-11-22},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WJKTZ3XM/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoderâ€“.pdf}
}

@article{cho_properties_2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder-Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  year = {2014},
  month = sep,
  journal = {arXiv:1409.1259 [cs, stat]},
  eprint = {1409.1259},
  primaryclass = {cs, stat},
  abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2WXCNTNE/cho_on the_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/JQ89FHQI/cho_on the_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/FF956TEB/1409.html;/Users/antoniohortaribeiro/Zotero/storage/S2HDWT9T/1409.html}
}

@article{choi_tutorial_2020,
  title = {Tutorial: A Guide to Performing Polygenic Risk Score Analyses},
  shorttitle = {Tutorial},
  author = {Choi, Shing Wan and Mak, Timothy Shin-Heng and O'Reilly, Paul F.},
  year = {2020},
  month = sep,
  journal = {Nature Protocols},
  volume = {15},
  number = {9},
  pages = {2759--2772},
  publisher = {Nature Publishing Group},
  issn = {1750-2799},
  doi = {10.1038/s41596-020-0353-1},
  urldate = {2024-01-24},
  abstract = {A polygenic score (PGS) or polygenic risk score (PRS) is an estimate of an individual's genetic liability to a trait or disease, calculated according to their genotype profile and relevant genome-wide association study (GWAS) data. While present PRSs typically explain only a small fraction of trait variance, their correlation with the single largest contributor to phenotypic variation---genetic liability---has led to the routine application of PRSs across biomedical research. Among a range of applications, PRSs are exploited to assess shared etiology between phenotypes, to evaluate the clinical utility of genetic data for complex disease and as part of experimental studies in which, for example, experiments are performed that compare outcomes (e.g., gene expression and cellular response to treatment) between individuals with low and high PRS values. As GWAS sample sizes increase and PRSs become more powerful, PRSs are set to play a key role in research and stratified medicine. However, despite the importance and growing application of PRSs, there are limited guidelines for performing PRS analyses, which can lead to inconsistency between studies and misinterpretation of results. Here, we provide detailed guidelines for performing and interpreting PRS analyses. We outline standard quality control steps, discuss different methods for the calculation of PRSs, provide an introductory online tutorial, highlight common misconceptions relating to PRS results, offer recommendations for best practice and discuss future challenges.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Genetic association study,Population genetics,Software},
  file = {/Users/antoniohortaribeiro/Zotero/storage/V4XI86QZ/Choi et al. - 2020 - Tutorial a guide to performing polygenic risk sco.pdf}
}

@article{chua_deep_2018,
  title = {Deep {{Reinforcement Learning}} in a {{Handful}} of {{Trials}} Using {{Probabilistic Dynamics Models}}},
  author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  year = {2018},
  month = may,
  journal = {arXiv:1805.12114 [cs, stat]},
  eprint = {1805.12114},
  primaryclass = {cs, stat},
  urldate = {2019-02-06},
  abstract = {Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8686UJYH/chua_deep_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/QK25WTBK/1805.html}
}

@article{chung_empirical_2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  month = dec,
  journal = {arXiv:1412.3555 [cs]},
  eprint = {1412.3555},
  primaryclass = {cs},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FAXTHQX3/chung_empirical_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/XCTI7B6R/1412.html}
}

@inproceedings{cisse_parseval_2017,
  title = {Parseval Networks: {{Improving}} Robustness to Adversarial Examples},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
  author = {Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
  year = {2017},
  series = {{{ICML}}'17},
  pages = {854--863},
  publisher = {JMLR},
  file = {/Users/antoniohortaribeiro/Zotero/storage/S8Y2J29G/Cisse et al. - 2017 - Parseval Networks Improving Robustness to Adversa.pdf}
}

@article{clark_what_2019,
  title = {What {{Does BERT Look At}}? {{An Analysis}} of {{BERT}}'s {{Attention}}},
  shorttitle = {What {{Does BERT Look At}}?},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.04341 [cs]},
  eprint = {1906.04341},
  primaryclass = {cs},
  urldate = {2019-06-25},
  abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9Z5YAHVC/clark_what does_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/LB6TCSS3/1906.html}
}

@article{clarke_jeffreys_1994,
  title = {Jeffreys' Prior Is Asymptotically Least Favorable under Entropy Risk},
  author = {Clarke, Bertrand S. and Barron, Andrew R.},
  year = {1994},
  month = aug,
  journal = {Journal of Statistical Planning and Inference},
  volume = {41},
  number = {1},
  pages = {37--60},
  issn = {03783758},
  doi = {10.1016/0378-3758(94)90153-8},
  urldate = {2018-10-07},
  abstract = {We provide a rigorous proof that Jeffreys' prior asymptotically maximizes Shannon's mutual information between a sample of size n and the parameter. This was conjectured by Bernard0 (1979) and, despite the absence of a proof, forms the basis of the reference prior method in Bayesian statistical analysis. Our proof rests on an examination of large sample decision theoretic properties associated with the relative entropy or the Kullback-Leibler distance between probability density functions for independent and identically distributed random variables. For smooth finite-dimensional parametric families we derive an asymptotic expression for the minimax risk and for the related maximin risk. As a result, we show that, among continuous positive priors, Jeffreys' prior uniquely achieves the asymptotic maximin value. In the discrete parameter case we show that, asymptotically, the Bayes risk reduces to the entropy of the prior so that the reference prior is seen to be the maximum entropy prior. We identify the physical significance of the risks by giving two information-theoretic interpretations in terms of probabilistic coding.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/D28K7X2Z/clarke_jeffreys'_1994.pdf}
}

@book{clarke_optimization_1990,
  title = {Optimization and {{Nonsmooth Analysis}}},
  author = {Clarke, Frank H.},
  year = {1990},
  eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611971309},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611971309}
}

@article{clifford_af_2017,
  title = {{{AF Classification}} from a {{Short Single Lead ECG Recording}}: The {{PhysioNet}}/{{Computing}} in {{Cardiology Challenge}} 2017},
  shorttitle = {{{AF Classification}} from a {{Short Single Lead ECG Recording}}},
  author = {Clifford, Gari D and Liu, Chengyu and Moody, Benjamin and Lehman, Li-wei H. and Silva, Ikaro and Li, Qiao and Johnson, A E and Mark, Roger G.},
  year = {2017},
  month = sep,
  journal = {Computing in Cardiology},
  volume = {44},
  issn = {2325-8861},
  urldate = {2018-10-21},
  abstract = {The PhysioNet/Computing in Cardiology (CinC) Challenge 2017 focused on differentiating AF from noise, normal or other rhythms in short term (from 9--61 s) ECG recordings performed by patients. A total of 12,186 ECGs were used: 8,528 in the public training set and 3,658 in the private hidden test set. Due to the high degree of inter-expert disagreement between a significant fraction of the expert labels we implemented a mid-competition bootstrap approach to expert relabeling of the data, levering the best performing Challenge entrants' algorithms to identify contentious labels., A total of 75 independent teams entered the Challenge using a variety of traditional and novel methods, ranging from random forests to a deep learning approach applied to the raw data in the spectral domain. Four teams won the Challenge with an equal high F1 score (averaged across all classes) of 0.83, although the top 11 algorithms scored within 2\% of this. A combination of 45 algorithms identified using LASSO achieved an F1 of 0.87, indicating that a voting approach can boost performance.},
  pmcid = {PMC5978770},
  pmid = {29862307},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5JWJJN85/clifford_af_2017.pdf}
}

@inproceedings{cohen_certified_2019,
  title = {Certified {{Adversarial Robustness}} via {{Randomized Smoothing}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Cohen, Jeremy M. and Rosenfeld, Elan and Kolter, J. Zico},
  year = {2019},
  month = jun,
  volume = {97},
  eprint = {1902.02918},
  primaryclass = {cs, stat},
  pages = {1310--1320},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1902.02918},
  urldate = {2023-04-14},
  abstract = {We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the \${\textbackslash}ell\_2\$ norm. This "randomized smoothing" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in \${\textbackslash}ell\_2\$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49\% under adversarial perturbations with \${\textbackslash}ell\_2\$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified \${\textbackslash}ell\_2\$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at http://github.com/locuslab/smoothing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/W2EZYCGF/Cohen et al. - 2019 - Certified Adversarial Robustness via Randomized Sm.pdf;/Users/antoniohortaribeiro/Zotero/storage/S9LBUN3R/1902.html}
}

@article{cohen_coefficient_1960,
  title = {A {{Coefficient}} of {{Agreement}} for {{Nominal Scales}}},
  author = {Cohen, Jacob},
  year = {1960},
  month = apr,
  journal = {Educational and Psychological Measurement},
  volume = {20},
  number = {1},
  pages = {37--46},
  issn = {0013-1644, 1552-3888},
  doi = {10/dghsrr},
  urldate = {2019-02-19},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5ZYQDDRS/Cohen - 1960 - A Coefficient of Agreement for Nominal Scales.pdf}
}

@inproceedings{cohen_gradient_2020,
  title = {Gradient {{Descent}} on {{Neural Networks Typically Occurs}} at the {{Edge}} of {{Stability}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Cohen, Jeremy and Kaur, Simran and Li, Yuanzhi and Kolter, J. Zico and Talwalkar, Ameet},
  year = {2020},
  month = sep,
  urldate = {2021-03-19},
  abstract = {We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum...},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4I8K5M5E/Cohen et al. - 2020 - Gradient Descent on Neural Networks Typically Occu.pdf;/Users/antoniohortaribeiro/Zotero/storage/WGHMA3Q4/forum.html}
}

@article{coleman_efficient_1995,
  title = {An Efficient Trust Region Method for Unconstrained Discrete-Time Optimal Control Problems},
  author = {Coleman, Thomas F and Liao, Aiping},
  year = {1995},
  journal = {Computational Optimization and Applications},
  volume = {4},
  number = {1},
  pages = {47--66},
  keywords = {â“Multiple DOI}
}

@article{coleman_efficient_1995a,
  title = {An {{Efficient Trust Region Method}} for {{Unconstrained Discrete-Time Optimal Control Problems}}},
  author = {Coleman, Thomas F and Liao, Aiping},
  year = {1995},
  journal = {Computational Optimization and Applications},
  volume = {4},
  number = {1},
  pages = {47--66},
  doi = {10/dxsjw9},
  annotation = {00041}
}

@article{combettes_lipschitz_2020,
  title = {Lipschitz {{Certificates}} for {{Layered Network Structures Driven}} by {{Averaged Activation Operators}}},
  author = {Combettes, Patrick L. and Pesquet, Jean-Christophe},
  year = {2020},
  month = jun,
  journal = {arXiv:1903.01014 [math]},
  eprint = {1903.01014},
  primaryclass = {math},
  urldate = {2021-08-17},
  abstract = {Obtaining sharp Lipschitz constants for feed-forward neural networks is essential to assess their robustness in the face of perturbations of their inputs. We derive such constants in the context of a general layered network model involving compositions of nonexpansive averaged operators and affine operators. By exploiting this architecture, our analysis finely captures the interactions between the layers, yielding tighter Lipschitz constants than those resulting from the product of individual bounds for groups of layers. The proposed framework is shown to cover in particular many practical instances encountered in feed-forward neural networks. Our Lipschitz constant estimates are further improved in the case of structures employing scalar nonlinear functions, which include standard convolutional networks as special cases.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q62N4TZV/Combettes_Pesquet_2020_Lipschitz Certificates for Layered Network Structures Driven by Averaged.pdf;/Users/antoniohortaribeiro/Zotero/storage/JEYRQNPH/1903.html}
}

@article{condat_fast_2016,
  title = {Fast Projection onto the Simplex and the \$\${\textbackslash}pmb \{l\}\_{\textbackslash}mathbf \{1\}\$\$ball},
  author = {Condat, Laurent},
  year = {2016},
  month = jul,
  journal = {Mathematical Programming},
  volume = {158},
  number = {1},
  pages = {575--585},
  issn = {1436-4646},
  doi = {10.1007/s10107-015-0946-6},
  urldate = {2024-05-22},
  abstract = {A new algorithm is proposed to project, exactly and in finite time, a vector of arbitrary size onto a simplex or an \$\$l\_1\$\$-norm ball. It can be viewed as a Gauss--Seidel-like variant of Michelot's variable fixing algorithm; that is, the threshold used to fix the variables is updated after each element is read, instead of waiting for a full reading pass over the list of non-fixed elements. This algorithm is empirically demonstrated to be faster than existing methods.},
  langid = {english},
  keywords = {49M30,65C60,65K05,90C25,l\_1-Norm ball,Large-scale optimization,Simplex},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CU7T4X2D/Condat - 2016 - Fast projection onto the simplex and the $$pmb l.pdf}
}

@book{conn_trustregion_2000,
  title = {Trust-Region Methods},
  author = {Conn, A. R. and Gould, Nicholas I. M. and Toint, Ph L.},
  year = {2000},
  series = {{{MPS-SIAM}} Series on Optimization},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia, PA},
  isbn = {978-0-89871-460-9},
  lccn = {QA402.5 .C6485 2000},
  keywords = {Mathematical optimization},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3KF8SBZV/conn_trust-regi_2000.pdf}
}

@article{connally_predictionand_2007,
  title = {Prediction-and Simulation-Error Based Perceptron Training: Solution Space Analysis and a Novel Combined Training Scheme},
  author = {Connally, Patrick and Li, Kang and Irwin, George W},
  year = {2007},
  journal = {Neurocomputing},
  volume = {70},
  number = {4},
  pages = {819--827},
  keywords = {ðŸ”No DOI found}
}

@book{cooper_computability_2017,
  title = {Computability {{Theory}}},
  author = {Cooper, S.B.},
  year = {2017},
  series = {Chapman {{Hall}}/{{CRC Mathematics Series}}},
  publisher = {CRC Press},
  isbn = {978-1-351-99196-4},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2MCSGSKF/cooper_computabil_2017.pdf}
}

@book{corbet_linux_2005,
  title = {Linux Device Drivers},
  author = {Corbet, Jonathan and Rubini, Alessandro and {Kroah-Hartman}, Greg and Rubini, Alessandro},
  year = {2005},
  edition = {3rd ed},
  publisher = {O'Reilly},
  address = {Beijing ; Sebastopol, CA},
  isbn = {978-0-596-00590-0},
  lccn = {QA76.76.D49 R92 2005},
  keywords = {Linux device drivers (Computer programs)},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IKESWNW7/corbet_linux_2005.pdf}
}

@book{cormen_introduction_2009,
  title = {Introduction to Algorithms},
  editor = {Cormen, Thomas H.},
  year = {2009},
  edition = {3rd ed},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-03384-8 978-0-262-53305-8},
  lccn = {QA76.6 .C662 2009},
  keywords = {Computer algorithms,Computer programming},
  annotation = {OCLC: ocn311310321},
  file = {/Users/antoniohortaribeiro/Zotero/storage/N3ACUJ43/cormen_introducti_2009.pdf}
}

@inproceedings{cortes_impact_2010,
  title = {On the Impact of Kernel Approximation on Learning Accuracy},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  author = {Cortes, Corinna and Mohri, Mehryar and Talwalkar, Ameet},
  editor = {Teh, Yee Whye and Titterington, Mike},
  year = {2010-05-13/2010-05-15},
  series = {Proceedings of Machine Learning Research},
  volume = {9},
  pages = {113--120},
  publisher = {PMLR},
  address = {Chia Laguna Resort, Sardinia, Italy},
  abstract = {Kernel approximation is commonly used to scale kernel-based algorithms to applications containing as many as several million instances. This paper analyzes the effect of such approximations in the kernel matrix on the hypothesis generated by several widely used learning algorithms. We give stability bounds based on the norm of the kernel approximation for these algorithms, including SVMs, KRR, and graph Laplacian-based regularization algorithms. These bounds help determine the degree of approximation that can be tolerated in the estimation of the kernel matrix. Our analysis is general and applies to arbitrary approximations of the kernel matrix. However, we also give a specific analysis of the Nystrom low-rank approximation in this context and report the results of experiments evaluating the quality of the Nystrom low-rank kernel approximation when used with ridge regression.},
  pdf = {http://proceedings.mlr.press/v9/cortes10a/cortes10a.pdf},
  file = {/Users/antoniohortaribeiro/Zotero/storage/H8KZATXZ/Cortes et al. - On the Impact of Kernel Approximation on Learning .pdf}
}

@article{cortez_modeling_2009,
  title = {Modeling Wine Preferences by Data Mining from Physicochemical Properties},
  author = {Cortez, Paulo and Cerdeira, Ant{\'o}nio and Almeida, Fernando and Matos, Telmo and Reis, Jos{\'e}},
  year = {2009},
  month = nov,
  journal = {Decision Support Systems},
  series = {Smart {{Business Networks}}: {{Concepts}} and {{Empirical Evidence}}},
  volume = {47},
  number = {4},
  pages = {547--553},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2009.05.016},
  urldate = {2024-05-16},
  abstract = {We propose a data mining approach to predict human wine taste preferences that is based on easily available analytical tests at the certification step. A large dataset (when compared to other studies in this domain) is considered, with white and red vinho verde samples (from Portugal). Three regression techniques were applied, under a computationally efficient procedure that performs simultaneous variable and model selection. The support vector machine achieved promising results, outperforming the multiple regression and neural network methods. Such model is useful to support the oenologist wine tasting evaluations and improve wine production. Furthermore, similar techniques can help in target marketing by modeling consumer tastes from niche markets.},
  keywords = {Model selection,Neural networks,Regression,Sensory preferences,Support vector machines,Variable selection},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KIIXRDRR/Cortez et al. - 2009 - Modeling wine preferences by data mining from phys.pdf;/Users/antoniohortaribeiro/Zotero/storage/RU69NC8N/S0167923609001377.html}
}

@book{cover_elements_2006,
  title = {Elements of Information Theory},
  author = {Cover, T. M. and Thomas, Joy A.},
  year = {2006},
  edition = {2nd ed},
  publisher = {Wiley-Interscience},
  address = {Hoboken, N.J},
  isbn = {978-0-471-24195-9},
  lccn = {Q360 .C68 2006},
  keywords = {Information theory},
  annotation = {OCLC: ocm59879802},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NN4U4X3B/cover_elements_2006.pdf}
}

@inproceedings{croce_robustbench_2021,
  title = {{{RobustBench}}: A Standardized Adversarial Robustness Benchmark},
  shorttitle = {{{RobustBench}}},
  booktitle = {{{NeurIPS Datasets}} and {{Benchmarks}} Track},
  author = {Croce, Francesco and Andriushchenko, Maksym and Sehwag, Vikash and Debenedetti, Edoardo and Flammarion, Nicolas and Chiang, Mung and Mittal, Prateek and Hein, Matthias},
  year = {2021},
  month = oct,
  eprint = {2010.09670},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2010.09670},
  urldate = {2023-04-13},
  abstract = {As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models. We evaluate adversarial robustness with AutoAttack, an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks, especially where AutoAttack flags a potential overestimation of robustness. Our leaderboard, hosted at https://robustbench.github.io/, contains evaluations of 120+ models and aims at reflecting the current state of the art in image classification on a set of well-defined tasks in \${\textbackslash}ell\_{\textbackslash}infty\$- and \${\textbackslash}ell\_2\$-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library https://github.com/RobustBench/robustbench that provides unified access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/69UST945/Croce et al. - 2021 - RobustBench a standardized adversarial robustness.pdf;/Users/antoniohortaribeiro/Zotero/storage/HATW268Y/2010.html}
}

@article{cubanski_neural_1994,
  title = {A {{Neural Network System}} for {{Detection}} of {{Atrial Fibrillation}} in {{Ambulatory Electrocardiograms}}},
  author = {CUBANSKI, {\relax DAVID} and CYGANSKI, {\relax DAVID} and ANTMAN, ELLIOTT M. and FELDMAN, CHARLES L.},
  year = {1994},
  month = jul,
  journal = {Journal of Cardiovascular Electrophysiology},
  volume = {5},
  number = {7},
  pages = {602--608},
  issn = {1045-3873},
  doi = {10/crkd95},
  urldate = {2019-01-15},
  abstract = {Neural Network for Detecting AF. Introduction: A neural network classifier has been designed, which is able to distinguish atrial fibrillation (AF) from other supraventricular arrhythmias in ambulatory (Holter) ECGs. Method and Results: The classification algorithm uses a rhythm analysts that considers the ECG to be a time series of RR interval durations. This is combined with an analysis of baseline morphology that considers the morphological characteristics of the non-QRS portions of the waveform. A back propagation-based neural network has been used as part of the classifier implementation. When applied to a library consisting exclusively of 42,970 examples of AF and other supraventricular rhythm disturbances validated by an experienced cardiologist, the algorithm demonstrated a sensitivity of 82.4\% for 10-beat runs of paroxysmal atrial fibrillation (PAF) and a specificity of 96.6\%. Since this system has been implemented as a postprocessor to a conventional automated Holter system, operating only on segments of ECG that are known to contain supraventricular arrhythmias rather than ventricular arrhythmias or sinus rhythm, it can be added to most existing Holter processing systems without significantly increasing the average time to process a tape. Conclusion: A neural network system has been designed, which can potentially provide, for the first time, an accurate, quantitative technique to determine the natural history of PAF and to evaluate potential treatments for PAF.},
  keywords = {atrial fibrillation,Holter monitoring,neural networks,paroxysmal atriul fibrillation}
}

@article{cucker_mathematical_2001,
  title = {On the Mathematical Foundations of Learning},
  author = {Cucker, Felipe and Smale, Steve},
  year = {2001},
  month = oct,
  journal = {Bulletin of the American Mathematical Society},
  volume = {39},
  number = {01},
  pages = {1--50},
  issn = {0273-0979},
  doi = {10.1090/S0273-0979-01-00923-5},
  urldate = {2019-12-31},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XTS5BPW2/Cucker and Smale - 2001 - On the mathematical foundations of learning.pdf}
}

@article{cucker_mathematical_2002,
  title = {On the Mathematical Foundations of Learning},
  author = {Cucker, Felipe and Smale, Steve},
  year = {2002},
  journal = {Bulletin of the American mathematical society},
  volume = {39},
  number = {1},
  pages = {1--49},
  doi = {10.1090/S0273-0979-01-00923-5},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ECW2UC8K/cucker_on the_2002.pdf}
}

@inproceedings{curth_uturn_2023,
  title = {A {{U-turn}} on {{Double Descent}}: {{Rethinking Parameter Counting}} in {{Statistical Learning}}},
  shorttitle = {A {{U-turn}} on {{Double Descent}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Curth, Alicia and Jeffares, Alan and van der Schaar, Mihaela},
  year = {2023},
  month = nov,
  urldate = {2023-12-12},
  abstract = {Conventional statistical wisdom established a well-understood relationship between model complexity and prediction error, typically presented as a \_U-shaped curve\_ reflecting a transition between under- and overfitting regimes. However, motivated by the success of overparametrized neural networks, recent influential work has suggested this theory to be generally incomplete, introducing an additional regime that exhibits a second descent in test error as the parameter count \$p\$ grows past sample size \$n\$ -- a phenomenon dubbed \_double descent\_. While most attention has naturally been given to the deep-learning setting, double descent was shown to emerge more generally across non-neural models: known cases include \_linear regression, trees, and boosting\_. In this work, we take a closer look at the evidence surrounding these more classical statistical machine learning methods and challenge the claim that observed cases of double descent truly extend the limits of a traditional U-shaped complexity-generalization curve therein. We show that once careful consideration is given to \_what is being plotted\_ on the x-axes of their double descent plots, it becomes apparent that there are implicitly multiple, distinct complexity axes along which the parameter count grows. We demonstrate that the second descent appears exactly (and \_only\_) when and where the transition between these underlying axes occurs, and that its location is thus \_not\_ inherently tied to the interpolation threshold \$p=n\$. We then gain further insight by adopting a classical nonparametric statistics perspective. We interpret the investigated methods as \_smoothers\_ and propose a generalized measure for the \_effective\_ number of parameters they use \_on unseen examples\_, using which we find that their apparent double descent curves do indeed fold back into more traditional convex shapes -- providing a resolution to the ostensible tension between double descent and traditional statistical intuition.},
  langid = {english},
  keywords = {\_tablet\_modified},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8ARK4ZL5/Curth et al_2023_A U-turn on Double Descent.pdf}
}

@article{dacostalopes_controloriented_2015,
  title = {A Control-Oriented Model of a {{PEM}} Fuel Cell Stack Based on {{NARX}} and {{NOE}} Neural Networks},
  author = {{da Costa Lopes}, Francisco and Watanabe, Edson H and Rolim, Lu{\'i}s Guilherme B},
  year = {2015},
  journal = {IEEE Transactions on Industrial Electronics},
  volume = {62},
  number = {8},
  pages = {5155--5163},
  doi = {10.1109/TIE.2015.2412519}
}

@article{dacostalopes_controloriented_2015a,
  title = {A {{Control-Oriented Model}} of a {{PEM Fuel Cell Stack Based}} on {{NARX}} and {{NOE Neural Networks}}},
  author = {{da Costa Lopes}, Francisco and Watanabe, Edson H and Rolim, Lu{\'i}s Guilherme B},
  year = {2015},
  journal = {IEEE Transactions on Industrial Electronics},
  volume = {62},
  number = {8},
  pages = {5155--5163},
  doi = {10/gfjwmk}
}

@article{daehlen_nonlinear_2014,
  title = {Nonlinear Model Predictive Control Using Trust-Region Derivative-Free Optimization},
  author = {{D{\textbackslash}a ehlen}, Jon S and Eikrem, Gisle Otto and Johansen, Tor Arne},
  year = {2014},
  journal = {Journal of Process Control},
  volume = {24},
  number = {7},
  pages = {1106--1120},
  keywords = {ðŸ”No DOI found}
}

@incollection{dai_coupled_2018,
  title = {Coupled {{Variational Bayes}} via {{Optimization Embedding}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Dai, Bo and Dai, Hanjun and He, Niao and Liu, Weiyang and Liu, Zhen and Chen, Jianshu and Xiao, Lin and Song, Le},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {9690--9700},
  publisher = {Curran Associates, Inc.},
  urldate = {2019-03-07},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CCYLWJ34/dai_coupled_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/HGPYKFCE/8177-coupled-variational-bayes-via-optimization-embedding.html}
}

@article{dai_semisupervised_2015,
  title = {Semi-Supervised {{Sequence Learning}}},
  author = {Dai, Andrew M. and Le, Quoc V.},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.01432 [cs]},
  eprint = {1511.01432},
  primaryclass = {cs},
  urldate = {2019-06-08},
  abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9UAM3N3M/dai_semi-super_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/HF8QBXSN/1511.html}
}

@article{dai_transformerxl_2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed-Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  journal = {arXiv:1901.02860 [cs, stat]},
  eprint = {1901.02860},
  primaryclass = {cs, stat},
  urldate = {2020-05-27},
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GJNRW893/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;/Users/antoniohortaribeiro/Zotero/storage/J8U359XC/1901.html}
}

@inproceedings{dalvi_adversarial_2004,
  title = {Adversarial Classification},
  booktitle = {Proceedings of the Tenth {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Dalvi, Nilesh and Domingos, Pedro and {Mausam} and Sanghai, Sumit and Verma, Deepak},
  year = {2004},
  doi = {10.1145/1014052.1014066},
  abstract = {Essentially all data mining algorithms assume that the data-generating process is independent of the data miner's activities. However, in many domains, including spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, this is far from the case: the data is actively manipulated by an adversary seeking to make the classifier produce false negatives. In these domains, the performance of a classifier can degrade rapidly after it is deployed, as the adversary learns to defeat it. Currently the only solution to this is repeated, manual, ad hoc reconstruction of the classifier. In this paper we develop a formal framework and algorithms for this problem. We view classification as a game between the classifier and the adversary, and produce a classifier that is optimal given the adversary's optimal strategy. Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way, and (within the parameters of the problem) automatically adapt the classifier to the adversary's evolving manipulations.},
  isbn = {1-58113-888-1},
  keywords = {cost-sensitive learning,game theory,integer linear programming,naive Bayes,spam detection}
}

@article{damour_underspecification_2020,
  title = {Underspecification {{Presents Challenges}} for {{Credibility}} in {{Modern Machine Learning}}},
  author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.03395},
  eprint = {2011.03395},
  urldate = {2020-11-11},
  abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8ZSQE5TM/D'Amour et al. - 2020 - Underspecification Presents Challenges for Credibi.pdf;/Users/antoniohortaribeiro/Zotero/storage/TXWGIF7Q/2011.html}
}

@inproceedings{dan_sharp_2020,
  title = {Sharp Statistical Guaratees for Adversarially Robust {{Gaussian}} Classification},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Dan, Chen and Wei, Yuting and Ravikumar, Pradeep},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020-07-13/2020-07-18},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {119},
  pages = {2345--2355},
  abstract = {Adversarial robustness has become a fundamental requirement in modern machine learning applications. Yet, there has been surprisingly little statistical understanding so far. In this paper, we provide the first result of the \emph{optimal} minimax guarantees for the excess risk for adversarially robust classification, under Gaussian mixture model proposed by schmidt2018adversarially. The results are stated in terms of the \emph{Adversarial Signal-to-Noise Ratio (AdvSNR)}, which generalizes a similar notion for standard linear classification to the adversarial setting. For the Gaussian mixtures with AdvSNR value of r, we prove an excess risk lower bound of order {$\Theta$}(e\textsuperscript{-({$\frac{1}{2}$}+o(1)) r{$^{2}$}} \textsuperscript{d}{\textfractionsolidus}\textsubscript{n}) and design a computationally efficient estimator that achieves this optimal rate. Our results built upon minimal assumptions while cover a wide spectrum of adversarial perturbations including \textsubscript{p} balls for any p 1.}
}

@article{daniel_realtime_2007,
  title = {Real-Time {{3D}} Vectorcardiography: An Application for Didactic Use},
  shorttitle = {Real-Time {{3D}} Vectorcardiography},
  author = {Daniel, G and Lissa, G and Redondo, D Medina and V{\'a}squez, L and Zapata, D},
  year = {2007},
  month = nov,
  journal = {Journal of Physics: Conference Series},
  volume = {90},
  pages = {012013},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/90/1/012013},
  urldate = {2018-07-17},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FUFIQFI2/daniel_real-time_2007.pdf}
}

@inproceedings{daniely_most_2020,
  title = {Most {{ReLU Networks Suffer}} from Ell 2 {{Adversarial Perturbations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Daniely, Amit and Shacham, Hadas},
  year = {2020},
  volume = {33},
  pages = {6629--6636},
  urldate = {2022-04-12},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YCTZM99N/Daniely_Shacham_2020_Most ReLU Networks Suffer from ell 2 Adversarial Perturbations.pdf}
}

@inproceedings{dascoli_triple_2020,
  title = {Triple Descent and the Two Kinds of Overfitting: Where \&amp; Why Do They Appear?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{d' Ascoli}, St{\'e}phane and Sagun, Levent and Biroli, Giulio},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {3058--3069},
  publisher = {Curran Associates, Inc.}
}

@article{daskalakis_limit_2018,
  title = {The {{Limit Points}} of ({{Optimistic}}) {{Gradient Descent}} in {{Min-Max Optimization}}},
  author = {Daskalakis, Constantinos and Panageas, Ioannis},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.03907 [math, stat]},
  eprint = {1807.03907},
  primaryclass = {math, stat},
  urldate = {2018-12-13},
  abstract = {Motivated by applications in Optimization, Game Theory, and the training of Generative Adversarial Networks, the convergence properties of first order methods in min-max problems have received extensive study. It has been recognized that they may cycle, and there is no good understanding of their limit points when they do not. When they converge, do they converge to local min-max solutions? We characterize the limit points of two basic first order methods, namely Gradient Descent/Ascent (GDA) and Optimistic Gradient Descent Ascent (OGDA). We show that both dynamics avoid unstable critical points for almost all initializations. Moreover, for small step sizes and under mild assumptions, the set of {\textbackslash}\{OGDA{\textbackslash}\}-stable critical points is a superset of {\textbackslash}\{GDA{\textbackslash}\}-stable critical points, which is a superset of local min-max solutions (strict in some cases). The connecting thread is that the behavior of these dynamics can be studied from a dynamical systems perspective.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/T2UFMCY7/daskalakis_the limit_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/B4UJUG5P/1807.html}
}

@article{daubechies_iteratively_2010,
  title = {Iteratively Reweighted Least Squares Minimization for Sparse Recovery},
  author = {Daubechies, Ingrid and DeVore, Ronald and Fornasier, Massimo and G{\"u}nt{\"u}rk, C Sinan},
  year = {2010},
  journal = {Communications on Pure and Applied Mathematics},
  volume = {63},
  number = {1},
  pages = {1--38},
  publisher = {Wiley Online Library},
  issn = {0010-3640}
}

@book{daubechies_ten_1992,
  title = {Ten Lectures on Wavelets},
  author = {Daubechies, Ingrid},
  year = {1992},
  series = {{{CBMS-NSF}} Regional Conference Series in Applied Mathematics},
  number = {61},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia, Pa},
  isbn = {978-0-89871-274-2},
  lccn = {QA403.3 .D38 1992},
  keywords = {Congresses,Wavelets (Mathematics)},
  file = {/Users/antoniohortaribeiro/Zotero/storage/P9XHTF29/daubechies_ten_1992.pdf}
}

@inproceedings{dauphin_language_2017,
  title = {Language Modeling with Gated Convolutional Networks},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning-Volume}} 70},
  author = {Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  year = {2017},
  pages = {933--941},
  publisher = {JMLR. org}
}

@article{davidon_variable_1991,
  title = {Variable Metric Method for Minimization},
  author = {Davidon, William C},
  year = {1991},
  journal = {SIAM Journal on Optimization},
  volume = {1},
  number = {1},
  pages = {1--17},
  doi = {10.1137/0801001}
}

@article{davidon_variable_1991a,
  title = {Variable {{Metric Method}} for {{Minimization}}},
  author = {Davidon, William C},
  year = {1991},
  journal = {SIAM Journal on Optimization},
  volume = {1},
  number = {1},
  pages = {1--17},
  doi = {10/cjnhbp},
  annotation = {01872}
}

@article{day_unsupervised_2007,
  title = {Unsupervised Segmentation of Continuous Genomic Data},
  author = {Day, Nathan and Hemmaplardh, Andrew and Thurman, Robert E. and Stamatoyannopoulos, John A. and Noble, William S.},
  year = {2007},
  month = jun,
  journal = {Bioinformatics},
  volume = {23},
  number = {11},
  pages = {1424--1426},
  issn = {1367-4803},
  doi = {10/bbjzck},
  urldate = {2019-04-01},
  abstract = {Abstract.  Summary: The advent of high-density, high-volume genomic data has created the need for tools to summarize large datasets at multiple scales. HMMSeg i},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TM8NDKCY/day_unsupervis_2007.pdf;/Users/antoniohortaribeiro/Zotero/storage/3V3IJ45A/200706.html}
}

@article{dayan_qlearning_1992,
  title = {Q-Learning},
  author = {Dayan, Peter and Watkins, {\relax CJCH}},
  year = {1992},
  journal = {Machine learning},
  volume = {8},
  number = {3},
  pages = {279--292},
  doi = {10.1023/A:1022657612745},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q935RXQT/dayan_q-learning_1992.pdf}
}

@misc{deazevedo_does_2020,
  title = {Does Gradient Descent Converge to a Minimum-Norm Solution in Least-Squares Problems?},
  author = {{de Azevedo}, Rodrigo},
  year = {2020},
  month = oct,
  eprint = {https://math.stackexchange.com/q/3499305},
  howpublished = {Mathematics Stack Exchange}
}

@article{decuyper_selecting_,
  title = {On Selecting Appropriate Training Data to Model an Au- Tonomous Oscillator},
  author = {Decuyper, J and Troyer, T De and Runacres, M C and Tiels, K and Schoukens, J},
  pages = {15},
  abstract = {In this work we study the effect of using different types of excitation signals as training data when constructing black box nonlinear models. We focus in particular on the class of nonlinear systems which exhibit autonomous oscillations. Three type of excitation signals are considered: random-phase multisines, filtered white noise and swept sines. It is shown that depending on the excitation signal, the resulting model can fail to reproduce the autonomous output. Results are presented for the forced Van der Pol oscillator and the oscillatory wake of an submerged circular cylinder in a flow. It is moreover shown that broadband excitations such as multisines or noise, are appropriate signals when intending to capture the synchronisation principle observed for autonomous oscillators. The latter is shown on computational fluid dynamic data of a submerged cylinder in a flow.},
  langid = {english},
  keywords = {â›” No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HWYZPXD7/decuyper_on_.pdf}
}

@article{defauw_clinically_2018,
  title = {Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease},
  author = {De Fauw, Jeffrey and Ledsam, Joseph R. and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O'Donoghue, Brendan and Visentin, Daniel and {van den Driessche}, George and Lakshminarayanan, Balaji and Meyer, Clemens and Mackinder, Faith and Bouton, Simon and Ayoub, Kareem and Chopra, Reena and King, Dominic and Karthikesalingam, Alan and Hughes, C{\'i}an O. and Raine, Rosalind and Hughes, Julian and Sim, Dawn A. and Egan, Catherine and Tufail, Adnan and Montgomery, Hugh and Hassabis, Demis and Rees, Geraint and Back, Trevor and Khaw, Peng T. and Suleyman, Mustafa and Cornebise, Julien and Keane, Pearse A. and Ronneberger, Olaf},
  year = {2018},
  month = sep,
  journal = {Nature Medicine},
  volume = {24},
  number = {9},
  pages = {1342--1350},
  issn = {1546-170X},
  doi = {10.1038/s41591-018-0107-6},
  abstract = {The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting.}
}

@inproceedings{defazio_saga_2014,
  title = {{{SAGA}}: {{A}} Fast Incremental Gradient Method with Support for Non-Strongly Convex Composite Objectives},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Defazio, Aaron and Bach, Francis and {Lacoste-Julien}, Simon},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K.Q.},
  year = {2014},
  volume = {27}
}

@book{degroot_probability_2012,
  title = {Probability and Statistics},
  author = {DeGroot, Morris H. and Schervish, Mark J.},
  year = {2012},
  edition = {4th ed},
  publisher = {Addison-Wesley},
  address = {Boston},
  isbn = {978-0-321-50046-5},
  lccn = {QA273 .D35 2012},
  keywords = {Mathematical statistics,Probabilities,Textbooks},
  annotation = {OCLC: ocn502674206},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NMMXUST6/degroot_probabilit_2012.pdf}
}

@article{dejesusrubio_stable_2017,
  title = {Stable {{Kalman}} Filter and Neural Network for the Chaotic Systems Identification},
  author = {{de Jes{\'u}s Rubio}, Jos{\'e}},
  year = {2017},
  month = nov,
  journal = {Journal of the Franklin Institute},
  volume = {354},
  number = {16},
  pages = {7444--7462},
  issn = {0016-0032},
  doi = {10/gcjcxf},
  abstract = {In this research, a modified Kalman filter is introduced for the adaptation of a neural network. The modified Kalman filter is an improved version of the extended Kalman filter based in the following two changes: (1) a term of the weights adaptation is modified in the modified algorithm to assure the uniform stability, convergence of the weights error, and local minimums avoidance, (2) the activation functions are used instead of the Jacobian terms in the modified algorithm to assure the boundedness of the weights error. The suggested algorithm is applied for the chaotic systems identification.}
}

@article{delathauwer_computation_2004,
  title = {Computation of the {{Canonical Decomposition}} by {{Means}} of a {{Simultaneous Generalized Schur Decomposition}}},
  author = {De Lathauwer, L. and De Moor, B. and Vandewalle, J.},
  year = {2004},
  month = jan,
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {26},
  number = {2},
  pages = {295--327},
  issn = {0895-4798},
  doi = {10.1137/S089547980139786X},
  abstract = {The canonical decomposition of higher-order tensors is a key tool in multilinear algebra. First we review the state of the art. Then we show that, under certain conditions, the problem can be rephrased as the simultaneous diagonalization, by equivalence or congruence, of a set of matrices. Necessary and sufficient conditions for the uniqueness of these simultaneous matrix decompositions are derived. In a next step, the problem can be translated into a simultaneous generalized Schur decomposition, with orthogonal unknowns [A.-J. van der Veen and A. Paulraj, IEEE Trans. Signal Process., 44 (1996), pp. 1136--1155]. A first-order perturbation analysis of the simultaneous generalized Schur decomposition is carried out. We discuss some computational techniques (including a new Jacobi algorithm) and illustrate their behavior by means of a number of numerical experiments.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2DUDB7G3/S089547980139786X.html;/Users/antoniohortaribeiro/Zotero/storage/JB7EVRBV/S089547980139786X.html}
}

@article{delathauwer_multilinear_2000,
  title = {A {{Multilinear Singular Value Decomposition}}},
  author = {De Lathauwer, L. and De Moor, B. and Vandewalle, J.},
  year = {2000},
  month = jan,
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {21},
  number = {4},
  pages = {1253--1278},
  issn = {0895-4798},
  doi = {10.1137/S0895479896305696},
  abstract = {We discuss a multilinear generalization of the singular value decomposition. There is a strong analogy between several properties of the matrix and the higher-order tensor decomposition; uniqueness, link with the matrix eigenvalue decomposition, first-order perturbation effects, etc., are analyzed. We investigate how tensor symmetries affect the decomposition and propose a multilinear generalization of the symmetric eigenvalue decomposition for pair-wise symmetric tensors.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4C9KQ785/S0895479896305696.html;/Users/antoniohortaribeiro/Zotero/storage/HLP8HDIM/S0895479896305696.html}
}

@article{delima_um_2010,
  title = {Um Sistema de Visao Est{\'e}reo Para Navega{\c c}ao de Um Carro Aut{\^o}nomo Em Ambientes Com Obst{\'a}culos},
  author = {{de Lima}, Danilo Alves and Pereira, Guilherme AS},
  year = {2010},
  keywords = {ðŸ”No DOI found}
}

@article{delmoral_sequential_2006,
  title = {Sequential {{Monte Carlo}} Samplers},
  author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
  year = {2006},
  month = jun,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {68},
  number = {3},
  pages = {411--436},
  issn = {1369-7412, 1467-9868},
  doi = {10/cfbsfg},
  urldate = {2018-12-12},
  abstract = {We propose a methodology to sample sequentially from a sequence of probability distributions that are defined on a common space, each distribution being known up to a normalizing constant. These probability distributions are approximated by a cloud of weighted random samples which are propagated over time by using sequential Monte Carlo methods. This methodology allows us to derive simple algorithms to make parallel Markov chain Monte Carlo algorithms interact to perform global optimization and sequential Bayesian estimation and to compute ratios of normalizing constants. We illustrate these algorithms for various integration tasks arising in the context of Bayesian inference.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ICVPEUCK/Del Moral et al. - 2006 - Sequential Monte Carlo samplers.pdf}
}

@inproceedings{delrio_assessment_2011,
  title = {Assessment of Different Methods to Estimate Electrocardiogram Signal Quality},
  booktitle = {Computing in {{Cardiology}}, 2011},
  author = {{del R{\'i}o}, B. Aldecoa S{\'a}nchez and Lopetegi, T. and Romero, I.},
  year = {2011},
  pages = {609--612},
  publisher = {IEEE},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XAWNATWC/del rÃ­o_assessment_2011.pdf}
}

@article{demoor_restricted_1991,
  title = {The {{Restricted Singular Value Decomposition}}: {{Properties}} and {{Applications}}},
  shorttitle = {The {{Restricted Singular Value Decomposition}}},
  author = {De Moor, B. and Golub, G.},
  year = {1991},
  month = jul,
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {12},
  number = {3},
  pages = {401--425},
  issn = {0895-4798},
  doi = {10.1137/0612029},
  abstract = {The restricted singular value decomposition (RSVD) is the factorization of a given matrix, relative to two other given matrices. It can be interpreted as the ordinary singular value decomposition with different inner products in row and column spaces. Its properties and structure, as well as its connection to generalized eigenvalue problems, canonical correlation analysis, and other generalizations of the singular value decomposition, are investigated in detail.Applications that are discussed include the analysis of the extended shorted operator, unitarily invariant norm minimization with rank constraints, rank minimization in matrix balls, the analysis and solution of linear matrix equations, rank minimization of a partitioned matrix, and the connection with generalized Schur complements, constrained linear and total linear least squares problems with mixed exact and noisy data, including a generalized Gauss--Markov estimation scheme.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/H9FMXT33/0612029.html;/Users/antoniohortaribeiro/Zotero/storage/P2T66L4U/0612029.html}
}

@article{deng_comparison_2023,
  title = {Comparison of {{State-of-the-Art Neural Network Survival Models}} with the {{Pooled Cohort Equations}} for {{Cardiovascular Disease Risk Prediction}}},
  author = {Deng, Yu and Liu, Lei and Jiang, Hongmei and Peng, Yifan and Wei, Yishu and Zhou, Zhiyang and Zhong, Yizhen and Zhao, Yun and Yang, Xiaoyun and Yu, Jingzhi and Lu, Zhiyong and Kho, Abel and Ning, Hongyan and Allen, Norrina B. and Wilkins, John T. and Liu, Kiang and {Lloyd-Jones}, Donald M. and Zhao, Lihui},
  year = {2023},
  month = jan,
  journal = {BMC Medical Research Methodology},
  volume = {23},
  number = {1},
  pages = {22},
  issn = {1471-2288},
  doi = {10.1186/s12874-022-01829-w},
  urldate = {2023-07-06},
  abstract = {The Pooled Cohort Equations (PCEs) are race- and sex-specific Cox proportional hazards (PH)-based models used for 10-year atherosclerotic cardiovascular disease (ASCVD) risk prediction with acceptable discrimination. In recent years, neural network models have gained increasing popularity with their success in image recognition and text classification. Various survival neural network models have been proposed by combining survival analysis and neural network architecture to take advantage of the strengths from both. However, the performance of these survival neural network models compared to each other and to PCEs in ASCVD prediction is unknown.},
  keywords = {Artificial intelligence,Cardiovascular disease,Cox regression,Deep learning,Machine learning,Neural network,Pooled Cohort Equations,Predictive modeling,Survival analysis},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UU7N8HS6/Deng et al. - 2023 - Comparison of State-of-the-Art Neural Network Surv.pdf}
}

@inproceedings{deng_imagenet_2009,
  title = {Imagenet: {{A}} Large-Scale Hierarchical Image Database},
  booktitle = {2009 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  pages = {248--255},
  organization = {Ieee}
}

@article{deng_mnist_2012,
  title = {The {{MNIST Database}} of {{Handwritten Digit Images}} for {{Machine Learning Research}}},
  author = {Deng, L.},
  year = {2012},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {141--142},
  issn = {1558-0792},
  doi = {10.1109/MSP.2012.2211477}
}

@inproceedings{deng_model_2020,
  title = {A {{Model}} of {{Double Descent}} for {{High-Dimensional Logistic Regression}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Deng, Zeyu and Kammoun, Abla and Thrampoulidis, Christos},
  year = {2020},
  month = may,
  pages = {4267--4271},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9053524},
  abstract = {We consider a model for logistic regression where only a subset of features of size p is used for training a linear classifier over n training samples. The classifier is obtained by running gradient-descent (GD) on the logistic-loss. For this model, we investigate the dependence of the classification error on the overparameterization ratio {$\kappa$} = p/n. First, building on known deterministic results on convergence properties of the GD, we uncover a phase-transition phenomenon for the case of Gaussian features: the classification error of GD is the same as that of the maximum-likelihood (ML) solution when {$\kappa$} {$<$}; {$\kappa$}*, and that of the max-margin (SVM) solution when {$\kappa$} {$>$} {$\kappa$}*. Next, using the convex Gaussian min-max theorem (CGMT), we sharply characterize the performance of both the ML and SVM solutions. Combining these results, we obtain curves that explicitly characterize the test error of GD for varying values of {$\kappa$}. The numerical results validate the theoretical predictions and unveil ``double-descent'' phenomena that complement similar recent observations in linear regression settings.},
  keywords = {Asymptotics,Binary Classification,Generalization error,Linear regression,Logistics,Max-margin,Numerical models,Overparameterization,Signal processing,Speech processing,Support vector machines,Training},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MAQ8NFLK/Deng et al_2020_A Model of Double Descent for High-Dimensional Logistic Regression.pdf}
}

@article{dennis_quasinewton_1977,
  title = {Quasi-{{Newton}} Methods, Motivation and Theory},
  author = {Dennis, Jr, John E and Mor{\'e}, Jorge J},
  year = {1977},
  journal = {SIAM review},
  volume = {19},
  number = {1},
  pages = {46--89},
  doi = {10.1137/1019005}
}

@article{dennis_two_1979,
  title = {Two New Unconstrained Optimization Algorithms Which Use Function and Gradient Values},
  author = {Dennis, John E and Mei, H. H. W.},
  year = {1979},
  journal = {Journal of Optimization Theory and Applications},
  volume = {28},
  number = {4},
  pages = {453--482},
  doi = {10.1007/BF00932218}
}

@article{dennisjr_adaptive_1981,
  title = {An Adaptive Nonlinear Least-Squares Algorithm},
  author = {Dennis Jr, John E and Gay, David M and Walsh, Roy E},
  year = {1981},
  journal = {ACM Transactions on Mathematical Software (TOMS)},
  volume = {7},
  number = {3},
  pages = {348--368},
  keywords = {â“Multiple DOI},
  file = {/Users/antoniohortaribeiro/Zotero/storage/73SZIHH6/dennis jr_an_1981.pdf}
}

@article{dennisjr_adaptive_1981a,
  title = {An {{Adaptive Nonlinear Least-Squares Algorithm}}},
  author = {Dennis Jr, John E and Gay, David M and Walsh, Roy E},
  year = {1981},
  journal = {ACM Transactions on Mathematical Software (TOMS)},
  volume = {7},
  number = {3},
  pages = {348--368},
  keywords = {â“Multiple DOI},
  annotation = {00000}
}

@book{dennisjr_numerical_1996,
  title = {Numerical Methods for Unconstrained Optimization and Nonlinear Equations},
  author = {Dennis Jr, John E and Schnabel, Robert B},
  year = {1996},
  volume = {16},
  publisher = {Siam},
  isbn = {1-61197-120-9},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MH964H84/dennis jr_numerical_1996.pdf}
}

@article{dennisjr_quasinewton_1977,
  title = {Quasi-{{Newton Methods}}, {{Motivation}} and {{Theory}}},
  author = {Dennis, Jr, John E and Mor{\'e}, Jorge J},
  year = {1977},
  journal = {SIAM review},
  volume = {19},
  number = {1},
  pages = {46--89},
  doi = {10/cxhdhx},
  annotation = {01684}
}

@inproceedings{derezinski_improved_2020,
  title = {Improved Guarantees and a Multiple-Descent Curve for {{Column Subset Selection}} and the {{Nystrom}} Method},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Derezinski, Michal and Khanna, Rajiv and Mahoney, Michael W},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {4953--4964},
  publisher = {Curran Associates, Inc.}
}

@article{devet_when_2006,
  title = {When to Use Agreement versus Reliability Measures},
  author = {De Vet, Henrica C.W. and Terwee, Caroline B. and Knol, Dirk L. and Bouter, Lex M.},
  year = {2006},
  month = oct,
  journal = {Journal of Clinical Epidemiology},
  volume = {59},
  number = {10},
  pages = {1033--1039},
  issn = {08954356},
  doi = {10.1016/j.jclinepi.2005.10.015},
  urldate = {2023-08-30},
  abstract = {Background: Reproducibility concerns the degree to which repeated measurements provide similar results. Agreement parameters assess how close the results of the repeated measurements are, by estimating the measurement error in repeated measurements. Reliability parameters assess whether study objects, often persons, can be distinguished from each other, despite measurement errors. In that case, the measurement error is related to the variability between persons. Consequently, reliability parameters are highly dependent on the heterogeneity of the study sample, while the agreement parameters, based on measurement error, are more a pure characteristic of the measurement instrument. Methods and Results: Using an example of an interrater study, in which different physical therapists measure the range of motion of the arm in patients with shoulder complaints, the differences and relationships between reliability and agreement parameters for continuous variables are illustrated. Conclusion: If the research question concerns the distinction of persons, reliability parameters are the most appropriate. But if the aim is to measure change in health status, which is often the case in clinical practice, parameters of agreement are preferred. {\'O} 2006 Elsevier Inc. All rights reserved.},
  langid = {english},
  keywords = {\_tablet\_modified},
  file = {/Users/antoniohortaribeiro/Zotero/storage/S9Z5B4HY/De Vet et al_2006_When to use agreement versus reliability measures.pdf}
}

@article{devlin_bert_2018,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.04805 [cs]},
  eprint = {1810.04805},
  primaryclass = {cs},
  urldate = {2019-06-08},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BUZX5U9P/devlin_bert_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/NX95XTDD/1810.html}
}

@article{diaconescu_use_2008,
  title = {The Use of {{NARX}} Neural Networks to Predict Chaotic Time Series},
  author = {Diaconescu, Eugen},
  year = {2008},
  journal = {WSEAS Transactions on Computer Research},
  volume = {3},
  number = {3},
  pages = {182--191},
  keywords = {ðŸ”No DOI found}
}

@article{diamant_patient_2022,
  title = {Patient Contrastive Learning: {{A}} Performant, Expressive, and Practical Approach to Electrocardiogram Modeling},
  shorttitle = {Patient Contrastive Learning},
  author = {Diamant, Nathaniel and Reinertsen, Erik and Song, Steven and Aguirre, Aaron D. and Stultz, Collin M. and Batra, Puneet},
  editor = {Kouyos, Roger Dimitri},
  year = {2022},
  month = feb,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {2},
  pages = {e1009862},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1009862},
  urldate = {2024-07-10},
  abstract = {Supervised machine learning applications in health care are often limited due to a scarcity of labeled training data. To mitigate the effect of small sample size, we introduce a pre-training approach, Patient Contrastive Learning of Representations (PCLR), which creates latent representations of electrocardiograms (ECGs) from a large number of unlabeled examples using contrastive learning. The resulting representations are expressive, performant, and practical across a wide spectrum of clinical tasks. We develop PCLR using a large health care system with over 3.2 million 12-lead ECGs and demonstrate that training linear models on PCLR representations achieves a 51\% performance increase, on average, over six training set sizes and four tasks (sex classification, age regression, and the detection of left ventricular hypertrophy and atrial fibrillation), relative to training neural network models from scratch. We also compared PCLR to three other ECG pre-training approaches (supervised pre-training, unsupervised pre-training with an autoencoder, and pre-training using a contrastive multi ECG-segment approach), and show significant performance benefits in three out of four tasks. We found an average performance benefit of 47\% over the other models and an average of a 9\% performance benefit compared to best model for each task. We release PCLR to enable others to extract ECG representations at https://github.com/ broadinstitute/ml4h/tree/master/model\_zoo/PCLR.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PNJX6GDA/Diamant et al. - 2022 - Patient contrastive learning A performant, expres.pdf}
}

@article{diamond_cvxpy_2016,
  title = {{{CVXPY}}: {{A Python-embedded}} Modeling Language for Convex Optimization},
  author = {Diamond, Steven and Boyd, Stephen},
  year = {2016},
  journal = {Journal of Machine Learning Research},
  volume = {17},
  number = {83},
  pages = {1--5}
}

@incollection{diehl_fast_2006,
  title = {Fast Direct Multiple Shooting Algorithms for Optimal Robot Control},
  booktitle = {Fast Motions in Biomechanics and Robotics},
  author = {Diehl, Moritz and Bock, Hans Georg and Diedam, Holger and Wieber, P-B},
  year = {2006},
  pages = {65--93},
  publisher = {Springer}
}

@article{dietterich_approximate_1998,
  title = {Approximate {{Statistical Tests}} for {{Comparing Supervised Classification Learning Algorithms}}},
  author = {Dietterich, Thomas G.},
  year = {1998},
  month = oct,
  journal = {Neural Computation},
  volume = {10},
  number = {7},
  pages = {1895--1923},
  issn = {0899-7667, 1530-888X},
  doi = {10/fqc9w5},
  urldate = {2018-11-22},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CRP2FNIK/Dietterich - 1998 - Approximate Statistical Tests for Comparing Superv.pdf}
}

@book{dimopoulos_analog_2012,
  title = {Analog Electronic Filters: Theory, Design and Synthesis},
  shorttitle = {Analog Electronic Filters},
  author = {Dimopoulos, Hercules G.},
  year = {2012},
  series = {Analog Circuits and Signal Processing},
  publisher = {Springer},
  address = {Dordrecht ; New York},
  isbn = {978-94-007-2189-0 978-94-007-2190-6},
  lccn = {TK7872.F5 D56 2012},
  keywords = {Analog electronic systems,Electric filters},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IUPIJTNT/dimopoulos_analog_2012.pdf}
}

@inproceedings{ding_r1pca_2006,
  title = {R1-Pca: {{Rotational}} Invariant L1-Norm Principal Component Analysis for Robust Subspace Factorization},
  booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
  author = {Ding, Chris and Zhou, Ding and He, Xiaofeng and Zha, Hongyuan},
  year = {2006},
  series = {{{ICML}} '06},
  pages = {281--288},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1143844.1143880},
  abstract = {Principal component analysis (PCA) minimizes the sum of squared errors (L2-norm) and is sensitive to the presence of outliers. We propose a rotational invariant L1-norm PCA (R1-PCA). R1-PCA is similar to PCA in that (1) it has a unique global solution, (2) the solution are principal eigenvectors of a robust covariance matrix (re-weighted to soften the effects of outliers), (3) the solution is rotational invariant. These properties are not shared by the L1-norm PCA. A new subspace iteration algorithm is given to compute R1-PCA efficiently. Experiments on several real-life datasets show R1-PCA can effectively handle outliers. We extend R1-norm to K-means clustering and show that L1-norm K-means leads to poor results while R1-K-means outperforms standard K-means.},
  isbn = {1-59593-383-2},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RAUCMC95/Ding et al. - 2006 - R 1 -PCA rotational invariant .pdf}
}

@inproceedings{diochnos_adversarial_2018,
  title = {Adversarial Risk and Robustness: {{General}} Definitions and Implications for the Uniform Distribution},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Diochnos, Dimitrios and Mahloujifar, Saeed and Mahmoody, Mohammad},
  year = {2018},
  volume = {31},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PA45DS5N/Diochnos et al. - Adversarial Risk and Robustness General Definitio.pdf}
}

@article{dobriban_highdimensional_2018,
  title = {High-Dimensional Asymptotics of Prediction: {{Ridge}} Regression and Classification},
  shorttitle = {High-Dimensional Asymptotics of Prediction},
  author = {Dobriban, Edgar and Wager, Stefan},
  year = {2018},
  month = feb,
  journal = {Annals of Statistics},
  volume = {46},
  number = {1},
  pages = {247--279},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/17-AOS1549},
  urldate = {2020-11-26},
  abstract = {We provide a unified analysis of the predictive risk of ridge regression and regularized discriminant analysis in a dense random effects model. We work in a high-dimensional asymptotic regime where p,n{$\rightarrow\infty$}p,n{$\rightarrow\infty$}p,n{\textbackslash}to{\textbackslash}infty and p/n{$\rightarrow\gamma>$}0p/n{$\rightarrow\gamma>$}0p/n{\textbackslash}to{\textbackslash}gamma{$>$}0, and allow for arbitrary covariance among the features. For both methods, we provide an explicit and efficiently computable expression for the limiting predictive risk, which depends only on the spectrum of the feature-covariance matrix, the signal strength and the aspect ratio {$\gamma\gamma\backslash$}gamma. Especially in the case of regularized discriminant analysis, we find that predictive accuracy has a nuanced dependence on the eigenvalue distribution of the covariance matrix, suggesting that analyses based on the operator norm of the covariance matrix may not be sharp. Our results also uncover an exact inverse relation between the limiting predictive risk and the limiting estimation risk in high-dimensional linear models. The analysis builds on recent advances in random matrix theory.},
  langid = {english},
  mrnumber = {MR3766952},
  zmnumber = {06865111},
  keywords = {High-dimensional asymptotics,prediction error,random matrix theory,regularized discriminant analysis,ridge regression},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JLJWPNDF/Dobriban and Wager - 2015 - High-Dimensional Asymptotics of Prediction Ridge .pdf}
}

@article{dobriban_provable_2022,
  title = {Provable Tradeoffs in Adversarially Robust Classification},
  author = {Dobriban, Edgar and Hassani, Hamed and Hong, David and Robey, Alexander},
  year = {2022},
  month = jan,
  journal = {arXiv:2006.05161},
  eprint = {2006.05161},
  doi = {10.48550/arXiv.2006.05161},
  urldate = {2023-10-03},
  abstract = {It is well known that machine learning methods can be vulnerable to adversarially-chosen perturbations of their inputs. Despite significant progress in the area, foundational open problems remain. In this paper, we address several key questions. We derive exact and approximate Bayes-optimal robust classifiers for the important setting of two- and three-class Gaussian classification problems with arbitrary imbalance, for \${\textbackslash}ell\_2\$ and \${\textbackslash}ell\_{\textbackslash}infty\$ adversaries. In contrast to classical Bayes-optimal classifiers, determining the optimal decisions here cannot be made pointwise and new theoretical approaches are needed. We develop and leverage new tools, including recent breakthroughs from probability theory on robust isoperimetry, which, to our knowledge, have not yet been used in the area. Our results reveal fundamental tradeoffs between standard and robust accuracy that grow when data is imbalanced. We also show further results, including an analysis of classification calibration for convex losses in certain models, and finite sample rates for the robust risk.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VVFS3Y7J/Dobriban et al. - 2022 - Provable tradeoffs in adversarially robust classif.pdf;/Users/antoniohortaribeiro/Zotero/storage/A2LZ9U8E/2006.html}
}

@inproceedings{dohmatob_generalized_2019,
  title = {Generalized No Free Lunch Theorem for Adversarial Robustness},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {Dohmatob, Elvis},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  year = {2019-06-09/2019-06-15},
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {1646--1654},
  publisher = {PMLR},
  abstract = {This manuscript presents some new impossibility results on adversarial robustness in machine learning, a very important yet largely open problem. We show that if conditioned on a class label the data distribution satisfies the W{$_2$} Talagrand transportation-cost inequality (for example, this condition is satisfied if the conditional distribution has density which is log-concave; is the uniform measure on a compact Riemannian manifold with positive Ricci curvature, any classifier can be adversarially fooled with high probability once the perturbations are slightly greater than the natural noise level in the problem. We call this result The Strong "No Free Lunch" Theorem as some recent results (Tsipras et al. 2018, Fawzi et al. 2018, etc.) on the subject can be immediately recovered as very particular cases. Our theoretical bounds are demonstrated on both simulated and real data (MNIST). We conclude the manuscript with some speculation on possible future research directions.},
  pdf = {http://proceedings.mlr.press/v97/dohmatob19a/dohmatob19a.pdf}
}

@techreport{dolan_benchmarking_2004,
  title = {Benchmarking Optimization Software with {{COPS}} 3.0.},
  author = {Dolan, Elizabeth D and Mor{\'e}, Jorge J and Munson, Todd S},
  year = {2004},
  institution = {Argonne National Lab., Argonne, IL (US)},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IZSFHEN2/dolan_benchmarki_2004.pdf}
}

@techreport{dolan_benchmarking_2004a,
  title = {Benchmarking {{Optimization Software}} with {{COPS}} 3.0.},
  author = {Dolan, Elizabeth D and Mor{\'e}, Jorge J and Munson, Todd S},
  year = {2004},
  institution = {Argonne National Lab., Argonne, IL (US)}
}

@article{dong_identification_2017,
  title = {Identification and {{Robust Control}} of the {{Nonlinear Photoelectrothermal Dynamics}} of {{LED Systems}}},
  author = {Dong, Jianfei and Zhang, Guoqi},
  year = {2017},
  journal = {IEEE Transactions on Industrial Electronics},
  volume = {64},
  number = {3},
  pages = {2215--2225},
  doi = {10.1109/TIE.2016.2619659}
}

@article{donoho_compressed_2006,
  title = {Compressed Sensing},
  author = {Donoho, David L.},
  year = {2006},
  journal = {IEEE Transactions on Information Theory},
  volume = {52},
  number = {4},
  pages = {1289--1306}
}

@article{doucet_tutorial_2009,
  title = {A Tutorial on Particle Filtering and Smoothing: {{Fifteen}} Years Later},
  author = {Doucet, Arnaud and Johansen, Adam M},
  year = {2009},
  journal = {Handbook of nonlinear filtering},
  volume = {12},
  number = {656-704},
  pages = {3},
  keywords = {â›” No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KS5UYJ8V/doucet_johansen_tutorialPF2011.pdf}
}

@inproceedings{doya_bifurcations_1993,
  title = {Bifurcations of {{Recurrent Neural Networks}} in {{Gradient Descent Learning}}},
  author = {Doya, Kenji},
  year = {1993},
  abstract = {Asymptotic behavior of a recurrent neural network changes qualitatively at certain points in the parameter space, which are known as {\textbackslash}bifurcation points". At bifurcation points, the output of a network can change discontinuously with the change of parameters and therefore convergence of gradient descent algorithms is not guaranteed. Furthermore, learning equations used for error gradient estimation can be unstable. However, some kinds of bifurcations are inevitable in training a recurrent network as an automaton or an oscillator. Some of the factors underlying successful training of recurrent networks are investigated, such as choice of initial connections, choice of input patterns, teacher forcing, and truncated learning equations.},
  keywords = {Algorithm,Artificial neural network,Automaton,Bifurcation (procedure),Bifurcation theory,Control theory,Dynamical system,Gradient descent,Neural Network Simulation,Neural Networks,Oscillator Device Component,Population Parameter,Recurrent neural network,Unstable Medical Device Problem},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QRHWDPJB/doya_bifurcatio_1993.pdf}
}

@book{draper_applied_1966,
  title = {Applied Regression Analysis},
  author = {Draper, Norman Richard and Smith, Harry and Pownell, Elizabeth},
  year = {1966},
  volume = {3},
  publisher = {Wiley New York}
}

@article{dreesen_recovering_2015,
  title = {Recovering {{Wiener-Hammerstein}} Nonlinear State-Space Models Using Linear Algebra},
  author = {Dreesen, Philippe and Ishteva, Mariya and Schoukens, Johan},
  year = {2015},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {17th {{IFAC Symposium}} on {{System Identification SYSID}} 2015},
  volume = {48},
  number = {28},
  pages = {951--956},
  issn = {2405-8963},
  doi = {10/gfkd8f},
  urldate = {2018-11-26},
  abstract = {This paper considers Wiener-Hammerstein systems consisting of a cascade of a linear dynamical system, a static nonlinearity and another linear dynamical system. We start from a black-box nonlinear state-space description of the system and develop a method to reconstruct the parameters of the underlying Wiener-Hammerstein block structure by means of linear algebra operations. First, the static nonlinearity is retrieved by decoupling the nonlinear part of the state-space equations into a single-branch nonlinear function. From there on, a canonical Wiener-Hammerstein nonlinear state-space model is recovered by using linear algebraic and geometric tools. The method is validated on a simulation example.},
  keywords = {block-oriented system identification,nonlinear system identification,state-space models,Wiener-Hammerstein systems},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HVN5VBYI/dreesen_recovering_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/HUQQYG5M/S2405896315028773.html}
}

@misc{drenkow_systematic_2022,
  title = {A {{Systematic Review}} of {{Robustness}} in {{Deep Learning}} for {{Computer Vision}}: {{Mind}} the Gap?},
  shorttitle = {A {{Systematic Review}} of {{Robustness}} in {{Deep Learning}} for {{Computer Vision}}},
  author = {Drenkow, Nathan and Sani, Numair and Shpitser, Ilya and Unberath, Mathias},
  year = {2022},
  month = nov,
  number = {arXiv:2112.00639},
  eprint = {2112.00639},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.00639},
  urldate = {2023-04-04},
  abstract = {Deep neural networks for computer vision are deployed in increasingly safety-critical and socially-impactful applications, motivating the need to close the gap in model performance under varied, naturally occurring imaging conditions. Robustness, ambiguously used in multiple contexts including adversarial machine learning, refers here to preserving model performance under naturally-induced image corruptions or alterations. We perform a systematic review to identify, analyze, and summarize current definitions and progress towards non-adversarial robustness in deep learning for computer vision. We find this area of research has received disproportionately less attention relative to adversarial machine learning, yet a significant robustness gap exists that manifests in performance degradation similar in magnitude to adversarial conditions. Toward developing a more transparent definition of robustness, we provide a conceptual framework based on a structural causal model of the data generating process and interpret non-adversarial robustness as pertaining to a model's behavior on corrupted images corresponding to low-probability samples from the unaltered data distribution. We identify key architecture-, data augmentation-, and optimization tactics for improving neural network robustness. This robustness perspective reveals that common practices in the literature correspond to causal concepts. We offer perspectives on how future research may mind this evident and significant non-adversarial robustness gap.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NAVCGQLQ/Drenkow et al_2022_A Systematic Review of Robustness in Deep Learning for Computer Vision.pdf;/Users/antoniohortaribeiro/Zotero/storage/Y9V3337G/2112.html}
}

@article{du_gradient_2018,
  title = {Gradient {{Descent Finds Global Minima}} of {{Deep Neural Networks}}},
  author = {Du, Simon S. and Lee, Jason D. and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.03804 [cs, math, stat]},
  eprint = {1811.03804},
  primaryclass = {cs, math, stat},
  urldate = {2018-11-13},
  abstract = {Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. Our bounds also shed light on the advantage of using ResNet over the fully connected feedforward architecture; our bound requires the number of neurons per layer scaling exponentially with depth for feedforward networks whereas for ResNet the bound only requires the number of neurons per layer scaling polynomially with depth. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {ðŸ”No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UMHZHVKZ/Du et al. - 2018 - Gradient Descent Finds Global Minima of Deep Neura.pdf}
}

@inproceedings{du_gradient_2019,
  title = {Gradient Descent Finds Global Minima of Deep Neural Networks},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  year = {2019-06-09/2019-06-15},
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {1675--1685},
  publisher = {PMLR},
  abstract = {Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UVJG5P23/Du et al. - 2019 - Gradient Descent Finds Global Minima of Deep Neura.pdf}
}

@article{du_gradient_2019a,
  title = {Gradient {{Descent Provably Optimizes Over-parameterized Neural Networks}}},
  author = {Du, Simon S. and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  year = {2019},
  month = feb,
  journal = {arXiv:1810.02054 [cs, math, stat]},
  eprint = {1810.02054},
  primaryclass = {cs, math, stat},
  urldate = {2020-07-27},
  abstract = {One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an \$m\$ hidden node shallow neural network with ReLU activation and \$n\$ training data, we show as long as \$m\$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. Our analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BW2Y65YP/Du et al. - 2019 - Gradient Descent Provably Optimizes Over-parameter.pdf;/Users/antoniohortaribeiro/Zotero/storage/WCLQ594G/1810.html}
}

@book{dubey_fundamentals_2002,
  title = {Fundamentals of {{Electrical Drives}}},
  author = {Dubey, G.K.},
  year = {2002},
  publisher = {Alpha Science International},
  isbn = {978-0-8493-2422-2},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JBWZHR8G/dubey_fundamenta_2002.pdf}
}

@book{dummit_abstract_,
  title = {Abstract {{Algebra}}},
  author = {Dummit, David S. and Foote, Richard M.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AS56E799/David S. Dummit, Richard M. Foote - Abstract Algebra, 3rd Edition  -Wiley (2003).pdf}
}

@article{dvoretzky_asymptotic_1956,
  title = {Asymptotic {{Minimax Character}} of the {{Sample Distribution Function}} and of the {{Classical Multinomial Estimator}}},
  author = {Dvoretzky, A. and Kiefer, J. and Wolfowitz, J.},
  year = {1956},
  month = sep,
  journal = {The Annals of Mathematical Statistics},
  volume = {27},
  number = {3},
  pages = {642--669},
  issn = {0003-4851, 2168-8990},
  doi = {10/csg5cg},
  urldate = {2019-04-16},
  abstract = {This paper is devoted, in the main, to proving the asymptotic minimax character of the sample distribution function (d.f.) for estimating an unknown d.f. in FF{\textbackslash}mathscr\{F\} or FcFc{\textbackslash}mathscr\{F\}\_c (defined in Section 1) for a wide variety of weight functions. Section 1 contains definitions and a discussion of measurability considerations. Lemma 2 of Section 2 is an essential tool in our proofs and seems to be of interest per se; for example, it implies the convergence of the moment generating function of GnGnG\_n to that of GGG (definitions in (2.1)). In Section 3 the asymptotic minimax character is proved for a fundamental class of weight functions which are functions of the maximum deviation between estimating and true d.f. In Section 4 a device (of more general applicability in decision theory) is employed which yields the asymptotic minimax result for a wide class of weight functions of this character as a consequence of the results of Section 3 for weight functions of the fundamental class. In Section 5 the asymptotic minimax character is proved for a class of integrated weight functions. A more general class of weight functions for which the asymptotic minimax character holds is discussed in Section 6. This includes weight functions for which the risk function of the sample d.f. is not a constant over Fc.Fc.{\textbackslash}mathscr\{F\}\_c. Most weight functions of practical interest are included in the considerations of Sections 3 to 6. Section 6 also includes a discussion of multinomial estimation problems for which the asymptotic minimax character of the classical estimator is contained in our results. Finally, Section 7 includes a general discussion of minimization of symmetric convex or monotone functionals of symmetric random elements, with special consideration of the "tied-down" Wiener process, and with a heuristic proof of the results of Sections 3, 4, 5, and much of Section 6.},
  langid = {english},
  mrnumber = {MR83864},
  zmnumber = {0073.14603},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MSGB5LHK/dvoretzky_asymptotic_1956.pdf;/Users/antoniohortaribeiro/Zotero/storage/7553XVW6/1177728174.html}
}

@article{eckhard_cost_2017,
  title = {Cost Function Shaping of the Output Error Criterion},
  author = {Eckhard, Diego and Bazanella, Alexandre S. and Rojas, Cristian R. and Hjalmarsson, H{\aa}kan},
  year = {2017},
  month = feb,
  journal = {Automatica},
  volume = {76},
  pages = {53--60},
  issn = {0005-1098},
  doi = {10/f9pgk7},
  urldate = {2019-04-14},
  abstract = {Identification of an output error model using the prediction error method leads to an optimization problem built on input/output data collected from the system to be identified. It is often hard to find the global solution of this optimization problem because in most cases both the corresponding objective function and the search space are nonconvex. The difficulty in solving the optimization problem depends mainly on the experimental conditions, more specifically on the spectra of the input/output data collected from the system. It is therefore possible to improve the convergence of the algorithms by properly choosing the data prefilters; in this paper we show how to perform this choice. We present the application of the proposed approach to case studies where the standard algorithms tend to fail to converge to the global minimum.},
  keywords = {Identification methods,Model fitting},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RA7YJJ2N/S0005109816304198.html}
}

@article{efron_bootstrap_1979,
  title = {Bootstrap {{Methods}}: {{Another Look}} at the {{Jackknife}}},
  author = {Efron, B.},
  year = {1979},
  month = jan,
  journal = {The Annals of Statistics},
  volume = {7},
  number = {1},
  pages = {1--26},
  doi = {10.1214/aos/1176344552}
}

@book{efron_introduction_1994,
  title = {An Introduction to the Bootstrap},
  author = {Efron, Bradley and Tibshirani, Robert J},
  year = {1994},
  publisher = {CRC press},
  isbn = {0-412-04231-2}
}

@article{efron_least_2004,
  title = {Least Angle Regression},
  author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
  year = {2004},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {32},
  number = {2},
  pages = {407--499},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053604000000067},
  urldate = {2017-09-04},
  abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
  mrnumber = {MR2060166},
  zmnumber = {1091.62054},
  keywords = {boosting,coefficient paths,Lasso,linear regression,variable selection},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SKBIPNU2/efron_least_2004.pdf}
}

@book{eiben_introduction_2003,
  title = {Introduction to Evolutionary Computing},
  author = {Eiben, Agoston E and Smith, James E},
  year = {2003},
  volume = {53},
  publisher = {Springer},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HK2KHX6E/eiben_introducti_2003.pdf}
}

@article{ek_offpolicy_,
  title = {Off-{{Policy Evaluation}} with {{Out-of-Sample Guarantees}}},
  author = {Ek, Sofia and Zachariah, Dave and Johansson, Fredrik D and Stoica, Petre},
  abstract = {We consider the problem of evaluating the performance of a decision policy using past observational data. The outcome of a policy is measured in terms of a loss (aka. disutility or negative reward) and the main problem is making valid inferences about its out-of-sample loss when the past data was observed under a different and possibly unknown policy. Using a sample-splitting method, we show that it is possible to draw such inferences with finitesample coverage guarantees about the entire loss distribution, rather than just its mean. Importantly, the method takes into account model misspecifications of the past policy --including unmeasured confounding. The evaluation method can be used to certify the performance of a policy using observational data under a specified range of credible model assumptions.},
  langid = {english},
  keywords = {\_tablet},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZXUHLUTT/Ek et al_Off-Policy Evaluation with Out-of-Sample Guarantees.pdf}
}

@article{eldar_asymptotic_2003,
  title = {On the Asymptotic Performance of the Decorrelator},
  author = {Eldar, Y.C. and Chan, A.M.},
  year = {2003},
  month = sep,
  journal = {IEEE Transactions on Information Theory},
  volume = {49},
  number = {9},
  pages = {2309--2313},
  issn = {0018-9448},
  doi = {10.1109/TIT.2003.815781},
  urldate = {2020-12-23},
  abstract = {We derive the asymptotic signal-to-interference ratio (SIR) of the decorrelator in the large system limit, both for the case in which the number of users exceeds the spreading gain and for the case in which the number of users is less than the spreading gain. We show that, contrary to what is claimed in [1], [2], when the number of users exceeds the spreading gain and the decorrelator is defined in terms of the Moore--Penrose pseudoinverse, the SIR does not converge to zero.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YWRWEM7R/Eldar and Chan - 2003 - On the asymptotic performance of the decorrelator.pdf}
}

@article{elvira_rethinking_2018,
  title = {Rethinking the {{Effective Sample Size}}},
  author = {Elvira, V{\'i}ctor and Martino, Luca and Robert, Christian P.},
  year = {2018},
  month = sep,
  urldate = {2018-11-12},
  langid = {english},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FZZWA3AM/elvira_rethinking_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/WJSF7YDX/1809.html}
}

@article{erhan_why_,
  title = {Why {{Does Unsupervised Pre-training Help Deep Learning}}?},
  author = {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal},
  pages = {8},
  abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of autoencoder variants with impressive results being obtained in several areas, mostly on vision and language datasets. The best results obtained on supervised learning tasks often involve an unsupervised learning component, usually in an unsupervised pre-training phase. The main question investigated here is the following: why does unsupervised pre-training work so well? Through extensive experimentation, we explore several possible explanations discussed in the literature including its action as a regularizer (Erhan et al., 2009b) and as an aid to optimization (Bengio et al., 2007). Our results build on the work of Erhan et al. (2009b), showing that unsupervised pre-training appears to play predominantly a regularization role in subsequent supervised training. However our results in an online setting, with a virtually unlimited data stream, point to a somewhat more nuanced interpretation of the roles of optimization and regularization in the unsupervised pre-training effect.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/N7M66WR4/Erhan et al. - Why Does Unsupervised Pre-training Help Deep Learn.pdf}
}

@article{eriksson_transferability_2024,
  title = {Transferability and {{Adversarial Training}} in {{Automatic Classification}} of the {{Electrocardiogram}} with {{Deep Learning}}},
  author = {Eriksson, Arvid and Sch{\"o}n, Thomas B and RIbeiro, Antonio H},
  year = {2024},
  journal = {Computers in Cardiology (CinC)},
  copyright = {All rights reserved}
}

@article{ernst_chromhmm_2012,
  title = {{{ChromHMM}}: Automating Chromatin-State Discovery and Characterization},
  shorttitle = {{{ChromHMM}}},
  author = {Ernst, Jason and Kellis, Manolis},
  year = {2012},
  month = mar,
  journal = {Nature Methods},
  volume = {9},
  number = {3},
  pages = {215--216},
  issn = {1548-7105},
  doi = {10/gffkzn},
  urldate = {2019-04-01},
  abstract = {ChromHMM: automating chromatin-state discovery and characterization},
  copyright = {2012 Nature Publishing Group},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q3EICYBV/ernst_chromhmm_2012.pdf;/Users/antoniohortaribeiro/Zotero/storage/D9UTYUEL/nmeth.html}
}

@incollection{ernst_mutual_2008,
  title = {Mutual Information Based Semi-Global Stereo Matching on the {{GPU}}},
  booktitle = {Advances in {{Visual Computing}}},
  author = {Ernst, Ines and Hirschm{\"u}ller, Heiko},
  year = {2008},
  pages = {228--239},
  publisher = {Springer}
}

@article{esfahani_polynomial_2017,
  title = {Polynomial State-Space Model Decoupling for the Identification of Hysteretic Systems},
  author = {Esfahani, Alireza Fakhrizadeh and Dreesen, Philippe and Tiels, Koen and No{\"e}l, Jean-Philippe and Schoukens, Johan},
  year = {2017},
  journal = {IFAC-PapersOnLine},
  volume = {50},
  number = {1},
  pages = {458--463},
  doi = {10.1016/j.ifacol.2017.08.082},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FESESGPC/esfahani_polynomial_2017.pdf}
}

@article{espinoza_kernel_2005,
  title = {Kernel Based Partially Linear Models and Nonlinear Identification},
  author = {Espinoza, M. and Suykens, J. A. K. and Moor, Bart De},
  year = {2005},
  month = oct,
  journal = {IEEE Transactions on Automatic Control},
  volume = {50},
  number = {10},
  pages = {1602--1606},
  issn = {0018-9286},
  doi = {10.1109/TAC.2005.856656},
  abstract = {In this note, we propose partially linear models with least squares support vector machines (LS-SVMs) for nonlinear ARX models. We illustrate how full black-box models can be improved when prior information about model structure is available. A real-life example, based on the Silverbox benchmark data, shows significant improvements in the generalization ability of the structured model with respect to the full black-box model, reflected also by a reduction in the effective number of parameters.},
  keywords = {Artificial neural networks,autoregressive processes,Councils,full black box model,identification,Kernel,kernel based partially linear model,Kernels,least squares approximations,Least squares methods,least squares support vector machine,least squares support vector machine (LS-SVM),linear systems,nonlinear ARX model,nonlinear control systems,Nonlinear equations,nonlinear identification,Nonlinear system identification,nonlinear systems,Parametric statistics,partially linear models,polynomials,Predictive models,Silverbox benchmark data,support vector machines},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RVS65MMQ/espinoza_kernel_2005.pdf;/Users/antoniohortaribeiro/Zotero/storage/5BP2MU6T/1516261.html;/Users/antoniohortaribeiro/Zotero/storage/7D2VV3GD/1516261.html}
}

@article{estes_computerized_2013,
  title = {Computerized Interpretation of {{ECGs}}: Supplement Not a Substitute},
  shorttitle = {Computerized Interpretation of {{ECGs}}},
  author = {Estes, N. A. Mark},
  year = {2013},
  month = feb,
  journal = {Circulation. Arrhythmia and Electrophysiology},
  volume = {6},
  number = {1},
  pages = {2--4},
  issn = {1941-3084},
  doi = {10.1161/CIRCEP.111.000097},
  langid = {english},
  pmid = {23424219},
  keywords = {{Signal Processing, Computer-Assisted},Electrocardiography,Female,Heart Conduction System,Humans,Long QT Syndrome,Male},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZVVCESMN/estes_computeriz_2013.pdf}
}

@book{eykhoff_system_1974,
  title = {System Identification: Parameter and State Estimation},
  shorttitle = {System Identification},
  author = {Eykhoff, Pieter},
  year = {1974},
  month = may,
  publisher = {Wiley-Interscience},
  googlebooks = {8f0pAQAAMAAJ},
  isbn = {978-0-471-24980-1},
  langid = {english},
  keywords = {Control theory,Estimation theory,Mathematics / Probability {\textbackslash}\& Statistics / General,Mathematics / Probability \& Statistics / General,parameter estimation,Science / System Theory,System analysis,system identification,Technology {\textbackslash}\& Engineering / Electrical,Technology \& Engineering / Electrical}
}

@book{eykhoff_trends_2014,
  title = {Trends and {{Progress}} in {{System Identification}}: {{Ifac Series}} for {{Graduates}}, {{Research Workers}} \& {{Practising Engineers}}},
  shorttitle = {Trends and {{Progress}} in {{System Identification}}},
  author = {Eykhoff, Pieter},
  year = {2014},
  month = may,
  publisher = {Elsevier},
  abstract = {Trends and Progress in System Identification is a three-part book that focuses on model considerations, identification methods, and experimental conditions involved in system identification. Organized into 10 chapters, this book begins with a discussion of model method in system identification, citing four examples differing on the nature of the models involved, the nature of the fields, and their goals. Subsequent chapters describe the most important aspects of model theory; the ""classical"" methods and time series estimation; application of least squares and related techniques for the estimation of dynamic system parameters; the maximum likelihood and error prediction methods; and the modern development of statistical methods. Non-parametric approaches, identification of nonlinear systems by piecewise approximation, and the minimax identification are then explained. Other chapters explore the Bayesian approach to system identification; choice of input signals; and choice and effect of different feedback configurations in system identification. This book will be useful for control engineers, system scientists, biologists, and members of other disciplines dealing withdynamical relations.},
  googlebooks = {hcyjBQAAQBAJ},
  isbn = {978-1-4831-4866-3},
  langid = {english},
  keywords = {Mathematics / Calculus,Mathematics / Mathematical Analysis}
}

@inproceedings{fahlman_cascadecorrelation_1989,
  title = {The {{Cascade-Correlation Learning Architecture}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Fahlman, Scott and Lebiere, Christian},
  year = {1989},
  volume = {2},
  publisher = {Morgan-Kaufmann},
  urldate = {2022-09-22},
  abstract = {Cascade-Correlation is a new architecture and supervised learning algo(cid:173) rithm for artificial neural networks.  Instead of just adjusting the weights  in a network of fixed topology. Cascade-Correlation begins with a min(cid:173) imal network,  then automatically trains  and adds new hidden  units  one  by  one,  creating a  multi-layer structure.  Once  a  new  hidden  unit  has  been added  to the network, its  input-side weights are frozen.  This  unit  then becomes a permanent feature-detector in the network, available for  producing  outputs  or for  creating other,  more complex  feature  detec(cid:173) tors.  The Cascade-Correlation architecture has  several advantages over  existing algorithms:  it  learns  very quickly,  the network . determines  its  own size and  topology, it retains  the structures  it  has  built even  if the  training set changes, and it requires no back-propagation of error signals  through  the connections of the network.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5MGCCBT5/Fahlman_Lebiere_1989_The Cascade-Correlation Learning Architecture.pdf}
}

@article{falck_kernel_2014,
  title = {Kernel Based Identification of Systems with Multiple Outputs Using Nuclear Norm Regularization},
  author = {Falck, Tillmann and De Moor, Bart and Suykens, Johan AK},
  year = {2014},
  journal = {Regularization, Optimization, Kernels, and Support Vector Machines},
  pages = {371},
  issn = {1482241390},
  keywords = {ðŸ”No DOI found}
}

@article{falck_leastsquares_2012,
  title = {Least-{{Squares Support Vector Machines}} for the Identification of {{Wiener}}--{{Hammerstein}} Systems},
  author = {Falck, Tillmann and Dreesen, Philippe and De Brabanter, Kris and Pelckmans, Kristiaan and De Moor, Bart and Suykens, Johan A. K.},
  year = {2012},
  month = nov,
  journal = {Control Engineering Practice},
  series = {Special {{Section}}: {{Wiener-Hammerstein System Identification Benchmark}}},
  volume = {20},
  number = {11},
  pages = {1165--1174},
  issn = {0967-0661},
  doi = {10.1016/j.conengprac.2012.05.006},
  abstract = {This paper considers the identification of Wiener--Hammerstein systems using Least-Squares Support Vector Machines based models. The power of fully black-box NARX-type models is evaluated and compared with models incorporating information about the structure of the systems. For the NARX models it is shown how to extend the kernel-based estimator to large data sets. For the structured model the emphasis is on preserving the convexity of the estimation problem through a suitable relaxation of the original problem. To develop an empirical understanding of the implications of the different model design choices, all considered models are compared on an artificial system under a number of different experimental conditions. The obtained results are then validated on the Wiener--Hammerstein benchmark data set and the final models are presented. It is illustrated that black-box models are a suitable technique for the identification of Wiener--Hammerstein systems. The incorporation of structural information results in significant improvements in modeling performance.},
  keywords = {Kernel-based models,Large-scale data processing,LS-SVMs,Nonlinear system identification,Overparameterization},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MNKKH852/falck_least-squa_2012.pdf;/Users/antoniohortaribeiro/Zotero/storage/2BTBSU2K/S0967066112001098.html;/Users/antoniohortaribeiro/Zotero/storage/RFCCUW47/S0967066112001098.html}
}

@inproceedings{falck_nuclear_2010,
  title = {Nuclear Norm Regularization for Overparametrized {{Hammerstein}} Systems},
  booktitle = {49th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Falck, T. and Suykens, J. A. K. and Schoukens, J. and Moor, B. De},
  year = {2010},
  month = dec,
  pages = {7202--7207},
  doi = {10.1109/CDC.2010.5717892},
  abstract = {In this paper we study the overparametrization scheme for Hammerstein systems in the presence of regularization. The quality of the convex approximation is analysed, that is obtained by relaxing the implicit rank one constraint. To obtain an improved convex relaxation we propose the use of nuclear norms, instead of using ridge regression. On several simple examples we illustrate that this yields a solution close to the best possible convex approximation. Furthermore the experiments suggest that ridge regression in combination with a projection step yield a generalization performance close to the one obtained by nuclear norms.},
  keywords = {approximation theory,Bandwidth,convex approximation,Convex functions,convex programming,Correlation,Estimation,Fasteners,Hammerstein systems,identification,Least squares approximation,nuclear norm regularization,overparametrization scheme,regression analysis,ridge regression,system identification},
  file = {/Users/antoniohortaribeiro/Zotero/storage/T4NTQ8H5/falck_nuclear_2010.pdf;/Users/antoniohortaribeiro/Zotero/storage/7K57KZXG/5717892.html;/Users/antoniohortaribeiro/Zotero/storage/RAEXUVEE/5717892.html}
}

@article{far_spectra_2006,
  title = {Spectra of Large Block Matrices},
  author = {Far, Reza Rashidi and Oraby, Tamer and Bryc, Wlodzimierz and Speicher, Roland},
  year = {2006},
  month = oct,
  journal = {arXiv:cs/0610045},
  eprint = {cs/0610045},
  urldate = {2021-06-28},
  abstract = {In a frequency selective slow-fading channel in a MIMO system, the channel matrix is of the form of a block matrix. This paper proposes a method to calculate the limit of the eigenvalue distribution of block matrices if the size of the blocks tends to infinity. While it considers random matrices, it takes an operator-valued free probability approach to achieve this goal. Using this method, one derives a system of equations, which can be solved numerically to compute the desired eigenvalue distribution. The paper initially tackles the problem for square block matrices, then extends the solution to rectangular block matrices. Finally, it deals with Wishart type block matrices. For two special cases, the results of our approach are compared with results from simulations. The first scenario investigates the limit eigenvalue distribution of block Toeplitz matrices. The second scenario deals with the distribution of Wishart type block matrices for a frequency selective slow-fading channel in a MIMO system for two different cases of \$n\_R=n\_T\$ and \$n\_R=2n\_T\$. Using this method, one may calculate the capacity and the Signal-to-Interference-and-Noise Ratio in large MIMO systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,H.1.1,Mathematics - Operator Algebras},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JBTY9DRC/Far et al_2006_Spectra of large block matrices.pdf;/Users/antoniohortaribeiro/Zotero/storage/2IADRWXZ/0610045.html}
}

@inproceedings{farina_convergence_2008,
  title = {Some Convergence Properties of Multi-Step Prediction Error Identification Criteria},
  booktitle = {2008 47th {{IEEE Conference}} on {{Decision}} and {{Control}}},
  author = {Farina, M. and Piroddi, L.},
  year = {2008},
  month = dec,
  pages = {756--761},
  doi = {10/d92tnw},
  abstract = {Multi-step prediction error identification methods are preferred over plain one-step ahead prediction error ones in application contexts (e.g., predictive control) where model accuracy is required over a wide horizon. For sufficiently high prediction horizons, their properties can be shown to be conveniently related to those of output error methods, for which several important issues (e.g., uniqueness of estimation, robustness with respect to the noise model) have been characterized in the literature. The convergence properties of such criteria with respect to the prediction horizon are analyzed.},
  keywords = {black-box identification,Computational modeling,Context modeling,Convergence,convergence of numerical methods,convergence property,error analysis,Error correction,Frequency estimation,identification,iterative methods,Minimization methods,multistep prediction error identification criteria,Noise robustness,prediction error minimization,prediction horizon analysis,Predictive control,Predictive models,simulation,simulation error minimization identification,Time series analysis},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YGLG7CXN/farina_some_2008.pdf;/Users/antoniohortaribeiro/Zotero/storage/EWX4GGSK/4738744.html}
}

@inproceedings{farina_convergence_2008a,
  title = {Some Convergence Properties of Multi-Step Prediction Error Identification Criteria},
  booktitle = {Decision and {{Control}}, 2008. {{CDC}} 2008. 47th {{IEEE Conference}} On},
  author = {Farina, Marcello and Piroddi, Luigi},
  year = {2008},
  pages = {756--761},
  publisher = {IEEE}
}

@article{farina_identification_2012,
  title = {Identification of Polynomial Input/Output Recursive Models with Simulation Error Minimisation Methods},
  author = {Farina, Marcello and Piroddi, Luigi},
  year = {2012},
  journal = {International Journal of Systems Science},
  volume = {43},
  number = {2},
  pages = {319--333},
  doi = {10.1080/00207721.2010.496055},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JYT6V6NE/Farina and Piroddi - 2012 - Identification of polynomial inputoutput recursiv.pdf}
}

@article{farina_iterative_2010,
  title = {An Iterative Algorithm for Simulation Error Based Identification of Polynomial Input--Output Models Using Multi-Step Prediction},
  author = {Farina, Marcello and Piroddi, Luigi},
  year = {2010},
  journal = {International Journal of Control},
  volume = {83},
  number = {7},
  pages = {1442--1456},
  doi = {10.1080/00207171003793262}
}

@article{farina_iterative_2010a,
  title = {An {{Iterative Algorithm}} for {{Simulation Error Based Identification}} of {{Polynomial Input}}--Output {{Models Using Multi-Step Prediction}}},
  author = {Farina, Marcello and Piroddi, Luigi},
  year = {2010},
  journal = {International Journal of Control},
  volume = {83},
  number = {7},
  pages = {1442--1456},
  doi = {10/b84h6q},
  annotation = {00011}
}

@article{farina_simulation_2011,
  title = {Simulation Error Minimization Identification Based on Multi-Stage Prediction},
  author = {Farina, M and Piroddi, L},
  year = {2011},
  journal = {International Journal of Adaptive Control and Signal Processing},
  volume = {25},
  number = {5},
  pages = {389--406},
  doi = {10.1002/acs.1203}
}

@article{farina_simulation_2011a,
  title = {Simulation Error Minimization Identification Based on Multi-Stage Prediction},
  author = {Farina, M. and Piroddi, L.},
  year = {2011},
  journal = {International Journal of Adaptive Control and Signal Processing},
  volume = {25},
  number = {5},
  pages = {389--406},
  issn = {1099-1115},
  doi = {10/bn2vnd},
  urldate = {2019-04-04},
  abstract = {Classical prediction error minimization (PEM) methods are widely used for model identification, but they are also known to provide satisfactory results only in specific identification conditions, e.g. disturbance model matching. If these conditions are not met, the obtained model may have quite different dynamical behavior compared with the original system, resulting in poor long range prediction or simulation performance, which is a critical factor for model analysis, simulation, model-based control design. In the mentioned non-ideal conditions a robust and reliable alternative is based on the minimization of the simulation error. Unfortunately, direct optimization of a simulation error minimization (SEM) criterion is an intrinsically complex and computationally intensive task. In this paper a low-complexity approximate SEM approach is discussed, based on the iteration of multi-step PEM methods. The soundness of the proposed approach is demonstrated by showing that, for sufficiently high prediction horizons, the k-steps ahead (single- or multi-step) PEM criteria converge to the SEM one. Identifiability issues and convergence properties of the algorithm are also discussed. Some examples are provided to illustrate the mentioned properties of the algorithm. Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {multi-step prediction,output error identification,prediction error minimization,simulation,simulation error minimization},
  file = {/Users/antoniohortaribeiro/Zotero/storage/464LZBTJ/farina_simulation_2011.pdf;/Users/antoniohortaribeiro/Zotero/storage/FNL7TXNQ/acs.html}
}

@inproceedings{faugeras_calibration_1986,
  title = {The Calibration Problem for Stereo},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Faugeras, Olivier D and Toscani, Giorgio},
  year = {1986},
  volume = {86},
  pages = {15--20}
}

@article{fawzi_analysis_2018,
  title = {Analysis of Classifiers' Robustness to Adversarial Perturbations},
  author = {Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  year = {2018},
  month = mar,
  journal = {Machine Learning},
  volume = {107},
  number = {3},
  pages = {481--508},
  issn = {1573-0565},
  doi = {10.1007/s10994-017-5663-3},
  urldate = {2022-04-29},
  abstract = {The goal of this paper is to analyze the intriguing instability of classifiers to adversarial perturbations (Szegedy et al., in: International conference on learning representations (ICLR), 2014). We provide a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and show fundamental upper bounds on the robustness of classifiers. Specifically, we establish a general upper bound on the robustness of classifiers to adversarial perturbations, and then illustrate the obtained upper bound on two practical classes of classifiers, namely the linear and quadratic classifiers. In both cases, our upper bound depends on a distinguishability measure that captures the notion of difficulty of the classification task. Our results for both classes imply that in tasks involving small distinguishability, no classifier in the considered set will be robust to adversarial perturbations, even if a good accuracy is achieved. Our theoretical framework moreover suggests that the phenomenon of adversarial instability is due to the low flexibility of classifiers, compared to the difficulty of the classification task (captured mathematically by the distinguishability measure). We further show the existence of a clear distinction between the robustness of a classifier to random noise and its robustness to adversarial perturbations. Specifically, the former is shown to be larger than the latter by a factor that is proportional to \$\${\textbackslash}sqrt\{d\}\$\$(with d being the signal dimension) for linear classifiers. This result gives a theoretical explanation for the discrepancy between the two robustness properties in high dimensional problems, which was empirically observed by Szegedy et al. in the context of neural networks. We finally show experimental results on controlled and real-world data that confirm the theoretical analysis and extend its spirit to more complex classification schemes.},
  langid = {english},
  keywords = {Adversarial examples,Classification robustness,Deep networks,Instability,Random noise},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SGUWH9NN/Fawzi et al_2018_Analysis of classifiersâ€™ robustness to adversarial perturbations.pdf}
}

@article{fazlyab_efficient_2019,
  title = {Efficient and {{Accurate Estimation}} of {{Lipschitz Constants}} for {{Deep Neural Networks}}},
  author = {Fazlyab, Mahyar and Robey, Alexander and Hassani, Hamed and Morari, Manfred and Pappas, George J.},
  year = {2019},
  month = jun,
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  eprint = {1906.04893},
  urldate = {2020-07-08},
  abstract = {Tight estimation of the Lipschitz constant for deep neural networks (DNNs) is useful in many applications ranging from robustness certification of classifiers to stability analysis of closed-loop systems with reinforcement learning controllers. Existing methods in the literature for estimating the Lipschitz constant suffer from either lack of accuracy or poor scalability. In this paper, we present a convex optimization framework to compute guaranteed upper bounds on the Lipschitz constant of DNNs both accurately and efficiently. Our main idea is to interpret activation functions as gradients of convex potential functions. Hence, they satisfy certain properties that can be described by quadratic constraints. This particular description allows us to pose the Lipschitz constant estimation problem as a semidefinite program (SDP). The resulting SDP can be adapted to increase either the estimation accuracy (by capturing the interaction between activation functions of different layers) or scalability (by decomposition and parallel implementation). We illustrate the utility of our approach with a variety of experiments on randomly generated networks and on classifiers trained on the MNIST and Iris datasets. In particular, we experimentally demonstrate that our Lipschitz bounds are the most accurate compared to those in the literature. We also study the impact of adversarial training methods on the Lipschitz bounds of the resulting classifiers and show that our bounds can be used to efficiently provide robustness guarantees.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZS5R98Y2/Fazlyab et al. - 2019 - Efficient and Accurate Estimation of Lipschitz Con.pdf;/Users/antoniohortaribeiro/Zotero/storage/VZFC9SY2/1906.html}
}

@book{fialho_automacao_2004,
  title = {Automa{\c c}{\~a}o Pneum{\'a}tica Projetos, Dimensionamento e An{\'a}lise de Circuitos},
  author = {Fialho, Arivelto Bustamante},
  year = {2004},
  edition = {2},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FHJGLW9L/Fialho - 2004 - AutomaÃ§Ã£o pneumÃ¡tica projetos, dimensionamento e a.pdf}
}

@article{finlay_improved_2019,
  title = {Improved Robustness to Adversarial Examples Using {{Lipschitz}} Regularization of the Loss},
  author = {Finlay, Chris and Oberman, Adam and Abbasi, Bilal},
  year = {2019},
  month = sep,
  journal = {arXiv:1810.00953 [cs, stat]},
  eprint = {1810.00953},
  primaryclass = {cs, stat},
  urldate = {2020-07-07},
  abstract = {We augment adversarial training (AT) with worst case adversarial training (WCAT) which improves adversarial robustness by 11\% over the current state-of-the-art result in the \${\textbackslash}ell\_2\$ norm on CIFAR-10. We obtain verifiable average case and worst case robustness guarantees, based on the expected and maximum values of the norm of the gradient of the loss. We interpret adversarial training as Total Variation Regularization, which is a fundamental tool in mathematical image processing, and WCAT as Lipschitz regularization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FSWWQFRM/1810.html}
}

@article{finlayson_generative_2018,
  title = {Towards Generative Adversarial Networks as a New Paradigm for Radiology Education},
  author = {Finlayson, Samuel G. and Lee, Hyunkwang and Kohane, Isaac S. and {Oakden-Rayner}, Luke},
  year = {2018},
  month = dec,
  urldate = {2018-12-13},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/V25PBHH7/finlayson_towards_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/J4RNMRE7/1812.html}
}

@article{fisher_use_1936,
  title = {The Use of Multiple Measurements in Taxonomic Problems},
  author = {Fisher, R.A.},
  year = {1936},
  month = sep,
  journal = {Annals of Eugenics},
  volume = {7},
  number = {2},
  pages = {179--188},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {2050-1420},
  doi = {10.1111/j.1469-1809.1936.tb02137.x},
  urldate = {2024-05-21},
  abstract = {The articles published by the Annals of Eugenics (1925?1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.}
}

@article{fletcher_function_1964,
  title = {Function Minimization by Conjugate Gradients},
  author = {Fletcher, Reeves and Reeves, Colin M},
  year = {1964},
  journal = {The computer journal},
  volume = {7},
  number = {2},
  pages = {149--154},
  doi = {10.1093/comjnl/7.2.149}
}

@book{fletcher_modified_1971,
  title = {A {{Modified Marquardt Subroutine}} for {{Non-linear Least Squares}}},
  author = {Fletcher, R. and Authority, United Kingdom Atomic Energy and {H.M.S.O.}},
  year = {1971},
  series = {{{AERE}} Report},
  publisher = {Theoretical Physics Division, Atomic Energy Research Establishment}
}

@book{fletcher_modified_1971a,
  title = {A {{Modified Marquardt Subroutine}} for {{Non-Linear Least Squares}}},
  author = {Fletcher, R. and Authority, United Kingdom Atomic Energy and {H.M.S.O.}},
  year = {1971},
  series = {{{AERE}} Report},
  publisher = {Theoretical Physics Division, Atomic Energy Research Establishment},
  annotation = {00003}
}

@book{fletcher_practical_2013,
  title = {Practical Methods of Optimization},
  author = {Fletcher, Roger},
  year = {2013},
  publisher = {John Wiley \& Sons},
  isbn = {1-118-72318-X},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PZVA28PC/fletcher_practical_2013.pdf}
}

@article{fletcher_user_1998,
  title = {User Manual for {{filterSQP}}},
  author = {Fletcher, Roger and Leyffer, Sven},
  year = {1998},
  journal = {Numerical Analysis Report NA/181, Department of Mathematics, University of Dundee, Dundee, Scotland},
  keywords = {ðŸ”No DOI found}
}

@book{forsyth_computer_2012,
  title = {Computer {{Vision}}: {{A Modern Approach}}},
  author = {Forsyth, D. and Ponce, J.},
  year = {2012},
  series = {Always Learning},
  publisher = {Pearson},
  isbn = {978-0-13-608592-8},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6WSJR9BD/forsyth_computer_2012.pdf}
}

@article{forsyth_modern_2003,
  title = {A {{Modern Approach}}},
  author = {Forsyth, David A and Ponce, Jean},
  year = {2003},
  journal = {Computer Vision: A Modern Approach},
  keywords = {ðŸ”No DOI found}
}

@book{forsythe_computer_1977,
  title = {Computer Methods for Mathematical Computations},
  author = {Forsythe, George Elmer and Moler, Cleve B and Malcolm, Michael A},
  year = {1977},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DKWHW2RV/forsythe_computer_1977.pdf}
}

@article{fort_deep_2020,
  title = {Deep {{Ensembles}}: {{A Loss Landscape Perspective}}},
  shorttitle = {Deep {{Ensembles}}},
  author = {Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  year = {2020},
  month = jun,
  journal = {arXiv:1912.02757 [cs, stat]},
  eprint = {1912.02757},
  primaryclass = {cs, stat},
  urldate = {2020-07-13},
  abstract = {Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QWPPD3SG/Fort et al. - 2020 - Deep Ensembles A Loss Landscape Perspective.pdf;/Users/antoniohortaribeiro/Zotero/storage/MSW8RIND/1912.html}
}

@article{fox_tutorial_2012,
  title = {A Tutorial on Variational {{Bayesian}} Inference},
  author = {Fox, Charles W. and Roberts, Stephen J.},
  year = {2012},
  month = aug,
  journal = {Artificial Intelligence Review},
  volume = {38},
  number = {2},
  pages = {85--95},
  issn = {0269-2821, 1573-7462},
  doi = {10/fd6kdm},
  urldate = {2018-12-05},
  abstract = {This tutorial describes the mean-field variational Bayesian approximation to inference in graphical models, using modern machine learning terminology rather than statistical physics concepts. It begins by seeking to find an approximate mean-field distribution close to the target joint in the KL-divergence sense. It then derives local node updates and reviews the recent Variational Message Passing framework.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/V9CXIXIK/Fox and Roberts - 2012 - A tutorial on variational Bayesian inference.pdf}
}

@book{franco_design_2002,
  title = {Design with Operational Amplifiers and Analog Integrated Circuits},
  author = {Franco, S.},
  year = {2002},
  series = {{{McGraw-Hill}} Series in Electrical and Computer Engineering},
  publisher = {McGraw-Hill},
  isbn = {978-0-07-232084-8},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8AFKAZW9/franco_design_2002.pdf}
}

@article{frank_statistical_1993,
  title = {A {{Statistical View}} of {{Some Chemometrics Regression Tools}}},
  author = {Frank, Ildiko E. and Friedman, Jerome H.},
  year = {1993},
  month = may,
  journal = {Technometrics},
  volume = {35},
  number = {2},
  eprint = {1269656},
  eprinttype = {jstor},
  pages = {109},
  issn = {00401706},
  doi = {10.2307/1269656},
  urldate = {2017-09-08},
  file = {/Users/antoniohortaribeiro/Zotero/storage/K9PPTNXB/frank_a_1993.pdf}
}

@inproceedings{frankle_early_2020,
  title = {The Early Phase of Neural Network Training},
  booktitle = {International Conference on Learning Representations},
  author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
  year = {2020},
  file = {/Users/antoniohortaribeiro/Zotero/storage/LJG5S8PQ/Frankle et al. - 2020 - THE EARLY PHASE OF NEURAL NETWORK TRAINING.pdf}
}

@article{frankle_linear_2020,
  title = {Linear {{Mode Connectivity}} and the {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
  year = {2020},
  month = feb,
  journal = {arXiv:1912.05671 [cs, stat]},
  eprint = {1912.05671},
  primaryclass = {cs, stat},
  urldate = {2020-07-04},
  abstract = {We introduce "instability analysis," which assesses whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise. We find that standard vision models become "stable" in this way early in training. From then on, the outcome of optimization is determined to within a linearly connected region. We use instability to study "iterative magnitude pruning" (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained to full accuracy from initialization. We find that these subnetworks only reach full accuracy when they are stable, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (Resnet-50 and Inception-v3 on ImageNet). This submission subsumes 1903.01611 ("Stabilizing the Lottery Ticket Hypothesis" and "The Lottery Ticket Hypothesis at Scale")},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/LBS9D5YJ/Frankle et al. - 2020 - Linear Mode Connectivity and the Lottery Ticket Hy.pdf;/Users/antoniohortaribeiro/Zotero/storage/9FS7FXUI/1912.html}
}

@article{frankle_lottery_2018,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Small}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.03635 [cs]},
  eprint = {1803.03635},
  primaryclass = {cs},
  urldate = {2018-11-06},
  abstract = {Neural network compression techniques are able to reduce the parameter counts of trained networks by over 90\%--decreasing storage requirements and improving inference performance--without compromising accuracy. However, contemporary experience is that it is difficult to train small architectures from scratch, which would similarly improve training performance. We articulate a new conjecture to explain why it is easier to train large networks: the "lottery ticket hypothesis." It states that large networks that train successfully contain subnetworks that--when trained in isolation--converge in a comparable number of iterations to comparable accuracy. These subnetworks, which we term "winning tickets," have won the initialization lottery: their connections have initial weights that make training particularly effective. We find that a standard technique for pruning unnecessary network weights naturally uncovers a subnetwork which, at the start of training, comprised a winning ticket. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis. We consistently find winning tickets that are less than 20\% of the size of several fully-connected, convolutional, and residual architectures for MNIST and CIFAR10. Furthermore, winning tickets at moderate levels of pruning (20-50\% of the original network size) converge up to 6.7x faster than the original network and exhibit higher test accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/D55C3LLV/frankle_the_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/GBS2LVMC/1803.html}
}

@article{frankle_stabilizing_2019,
  title = {Stabilizing the {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
  year = {2019},
  month = jun,
  journal = {arXiv:1903.01611 [cs, stat]},
  eprint = {1903.01611},
  primaryclass = {cs, stat},
  urldate = {2020-06-29},
  abstract = {Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar benefits during training. In particular, the "lottery ticket hypothesis" conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably finds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations. In this paper, we argue that these efforts have struggled on deeper networks because they have focused on pruning precisely at initialization. We modify IMP to search for subnetworks that could have been obtained by pruning early in training (0.1\% to 7\% through) rather than at iteration 0. With this change, it finds small subnetworks of deeper networks (e.g., 80\% sparsity on Resnet-50) that can complete the training process to match the accuracy of the original network on more challenging tasks (e.g., ImageNet). In situations where IMP fails at iteration 0, the accuracy benefits of delaying pruning accrue rapidly over the earliest iterations of training. To explain these behaviors, we study subnetwork "stability," finding that - as accuracy improves in this fashion - IMP subnetworks train to parameters closer to those of the full network and do so with improved consistency in the face of gradient noise. These results offer new insights into the opportunity to prune large-scale networks early in training and the behaviors underlying the lottery ticket hypothesis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/LCFPKWH2/Frankle et al. - 2019 - Stabilizing the Lottery Ticket Hypothesis.pdf;/Users/antoniohortaribeiro/Zotero/storage/RPCHKGQF/1903.html}
}

@book{friedman_elements_2001,
  title = {The Elements of Statistical Learning},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year = {2001},
  volume = {1},
  publisher = {Springer series in statistics New York},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4XX3N2CA/friedman_the_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/TEBEN3R7/friedman_the_2001.pdf}
}

@article{friedman_fast_2012,
  title = {Fast Sparse Regression and Classification},
  author = {Friedman, Jerome H.},
  year = {2012},
  month = jul,
  journal = {International Journal of Forecasting},
  volume = {28},
  number = {3},
  pages = {722--738},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2012.05.001},
  abstract = {Many present day applications of statistical learning involve large numbers of predictor variables. Often, that number is much larger than the number of cases or observations available for training the learning algorithm. In such situations, traditional methods fail. Recently, new techniques have been developed, based on regularization, which can often produce accurate models in these settings. This paper describes the basic principles underlying the method of regularization, then focuses on those methods which exploit the sparsity of the predicting model. The potential merits of these methods are then explored by example.},
  keywords = {-norm penalization,Bridge-regression,Classification,Elastic net,Lasso,Regression,Regularization,Sparsity,variable selection},
  file = {/Users/antoniohortaribeiro/Zotero/storage/S554ZZI2/friedman_fast_2012.pdf;/Users/antoniohortaribeiro/Zotero/storage/4UJ66Z37/S0169207012000490.html}
}

@article{friedman_glmnet_2009,
  title = {Glmnet: {{Lasso}} and Elastic-Net Regularized Generalized Linear Models},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  year = {2009},
  journal = {R package version},
  volume = {1},
  number = {4},
  keywords = {ðŸ”No DOI found}
}

@article{friedman_multivariate_1991,
  title = {Multivariate Adaptive Regression Splines},
  author = {Friedman, Jerome H},
  year = {1991},
  journal = {The annals of statistics},
  pages = {1--67},
  issn = {0090-5364},
  doi = {10.1214/aos/1176347963},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JU23KEFC/friedman_multivaria_1991.pdf}
}

@article{friedman_pathwise_2007,
  title = {Pathwise Coordinate Optimization},
  author = {Friedman, Jerome and Hastie, Trevor and H{\"o}fling, Holger and Tibshirani, Robert},
  year = {2007},
  month = dec,
  journal = {The Annals of Applied Statistics},
  volume = {1},
  number = {2},
  pages = {302--332},
  issn = {1932-6157},
  doi = {10.1214/07-AOAS131},
  urldate = {2017-09-13},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UIDEKAFW/friedman_pathwise_2007.pdf}
}

@article{friedman_regularization_2010,
  title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  year = {2010},
  journal = {Journal of statistical software},
  volume = {33},
  number = {1},
  pages = {1},
  doi = {10.18637/jss.v033.i01},
  urldate = {2017-09-14},
  file = {/Users/antoniohortaribeiro/Zotero/storage/S8A5HHNQ/friedman_regulariza_2010.pdf}
}

@article{friedman_sparse_2008,
  title = {Sparse Inverse Covariance Estimation with the Graphical Lasso},
  author = {Friedman, J. and Hastie, T. and Tibshirani, R.},
  year = {2008},
  month = jul,
  journal = {Biostatistics},
  volume = {9},
  number = {3},
  pages = {432--441},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxm045},
  urldate = {2017-09-18},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4PKBSS6N/friedman_sparse_2008.pdf}
}

@article{fu_penalized_1998,
  title = {Penalized {{Regressions}}: {{The Bridge}} versus the {{Lasso}}},
  shorttitle = {Penalized {{Regressions}}},
  author = {Fu, Wenjiang J.},
  year = {1998},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {7},
  number = {3},
  eprint = {1390712},
  eprinttype = {jstor},
  pages = {397--416},
  issn = {1061-8600},
  doi = {10.2307/1390712},
  abstract = {Bridge regression, a special family of penalized regressions of a penalty function {$\sum\vert\beta$} \textsubscript{j}{\textbar}\textsuperscript{{$\gamma$}} with {$\gamma$} {$\geq$} 1, is considered. A general approach to solve for the bridge estimator is developed. A new algorithm for the lasso ({$\gamma$} = 1) is obtained by studying the structure of the bridge estimators. The shrinkage parameter {$\gamma$} and the tuning parameter {$\lambda$} are selected via generalized cross-validation (GCV). Comparison between the bridge model ({$\gamma$} {$\geq$} 1) and several other shrinkage models, namely the ordinary least squares regression ({$\lambda$} = 0), the lasso ({$\gamma$} = 1) and ridge regression ({$\gamma$} = 2), is made through a simulation study. It is shown that the bridge regression performs well compared to the lasso and ridge regression. These methods are demonstrated through an analysis of a prostate cancer data. Some computational advantages and limitations are discussed.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EFRR53XQ/fu_penalized_1998.pdf}
}

@article{fujieda_wavelet_2018,
  title = {Wavelet {{Convolutional Neural Networks}}},
  author = {Fujieda, Shin and Takayama, Kohei and Hachisuka, Toshiya},
  year = {2018},
  month = may,
  journal = {arXiv:1805.08620 [cs]},
  eprint = {1805.08620},
  primaryclass = {cs},
  urldate = {2020-07-07},
  abstract = {Spatial and spectral approaches are two major approaches for image processing tasks such as image classification and object recognition. Among many such algorithms, convolutional neural networks (CNNs) have recently achieved significant performance improvement in many challenging tasks. Since CNNs process images directly in the spatial domain, they are essentially spatial approaches. Given that spatial and spectral approaches are known to have different characteristics, it will be interesting to incorporate a spectral approach into CNNs. We propose a novel CNN architecture, wavelet CNNs, which combines a multiresolution analysis and CNNs into one model. Our insight is that a CNN can be viewed as a limited form of a multiresolution analysis. Based on this insight, we supplement missing parts of the multiresolution analysis via wavelet transform and integrate them as additional components in the entire architecture. Wavelet CNNs allow us to utilize spectral information which is mostly lost in conventional CNNs but useful in most image processing tasks. We evaluate the practical performance of wavelet CNNs on texture classification and image annotation. The experiments show that wavelet CNNs can achieve better accuracy in both tasks than existing models while having significantly fewer parameters than conventional CNNs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YX8CYPUB/Fujieda et al. - 2018 - Wavelet Convolutional Neural Networks.pdf;/Users/antoniohortaribeiro/Zotero/storage/XD6YRWNL/1805.html}
}

@article{funk_doubly_2011,
  title = {Doubly {{Robust Estimation}} of {{Causal Effects}}},
  author = {Funk, Michele Jonsson and Westreich, Daniel and Wiesen, Chris and St{\"u}rmer, Til and Brookhart, M. Alan and Davidian, Marie},
  year = {2011},
  month = apr,
  journal = {American Journal of Epidemiology},
  volume = {173},
  number = {7},
  pages = {761--767},
  issn = {0002-9262},
  doi = {10.1093/aje/kwq439},
  urldate = {2024-07-16},
  abstract = {Doubly robust estimation combines a form of outcome regression with a model for the exposure (i.e., the propensity score) to estimate the causal effect of an exposure on an outcome. When used individually to estimate a causal effect, both outcome regression and propensity score methods are unbiased only if the statistical model is correctly specified. The doubly robust estimator combines these 2 approaches such that only 1 of the 2 models need be correctly specified to obtain an unbiased effect estimator. In this introduction to doubly robust estimators, the authors present a conceptual overview of doubly robust estimation, a simple worked example, results from a simulation study examining performance of estimated and bootstrapped standard errors, and a discussion of the potential advantages and limitations of this method. The supplementary material for this paper, which is posted on the Journal's Web site (http://aje.oupjournals.org/), includes a demonstration of the doubly robust property (Web Appendix 1) and a description of a SAS macro (SAS Institute, Inc., Cary, North Carolina) for doubly robust estimation, available for download at http://www.unc.edu/{$\sim$}mfunk/dr/.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8X4MJGIP/Funk et al. - 2011 - Doubly Robust Estimation of Causal Effects.pdf;/Users/antoniohortaribeiro/Zotero/storage/E9BJXLHZ/103691.html}
}

@article{furnival_regressions_1974,
  title = {Regressions by {{Leaps}} and {{Bounds}}},
  author = {Furnival, George M. and Wilson, Robert W.},
  year = {1974},
  month = nov,
  journal = {Technometrics},
  volume = {16},
  number = {4},
  pages = {499--511},
  issn = {0040-1706},
  doi = {10.1080/00401706.1974.10489231}
}

@article{gander_analysis_2007,
  title = {Analysis of the Parareal Time-Parallel Time-Integration Method},
  author = {Gander, Martin J and Vandewalle, Stefan},
  year = {2007},
  journal = {SIAM Journal on Scientific Computing},
  volume = {29},
  number = {2},
  pages = {556--578},
  doi = {10.1137/05064607X}
}

@article{ganin_unsupervised_2015,
  title = {Unsupervised {{Domain Adaptation}} by {{Backpropagation}}},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  year = {2015},
  journal = {International Conference on Machine Learning (ICML)},
  eprint = {1409.7495},
  primaryclass = {cs, stat},
  urldate = {2024-08-28},
  abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled targetdomain data is necessary).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/G9B8RKJP/Ganin and Lempitsky - 2015 - Unsupervised Domain Adaptation by Backpropagation.pdf}
}

@article{garcia_comprehensive_2015,
  title = {A Comprehensive Survey on Safe Reinforcement Learning},
  author = {Garc{\i}a, Javier and Fern{\'a}ndez, Fernando},
  year = {2015},
  journal = {Journal of Machine Learning Research},
  volume = {16},
  number = {1},
  pages = {1437--1480},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/B46QX9WU/garcÄ±a_a_2015.pdf}
}

@article{garfield_brief_2024,
  title = {A Brief Comparison of Polygenic Risk Scores and {{Mendelian}} Randomisation},
  author = {Garfield, Victoria and Anderson, Emma L.},
  year = {2024},
  month = jan,
  journal = {BMC Medical Genomics},
  volume = {17},
  number = {1},
  pages = {10},
  issn = {1755-8794},
  doi = {10.1186/s12920-023-01769-4},
  urldate = {2024-07-30},
  abstract = {Mendelian randomisation and polygenic risk score analysis have become increasingly popular in the last decade due to the advent of large-scale genome-wide association studies. Each approach has valuable applications, some of which are overlapping, yet there are important differences which we describe here.},
  keywords = {Genome-wide association studies,Horizontal pleiotropy,Mendelian randomisation,Polygenic risk scores},
  file = {/Users/antoniohortaribeiro/Zotero/storage/A8MWHDW6/Garfield and Anderson - 2024 - A brief comparison of polygenic risk scores and Me.pdf;/Users/antoniohortaribeiro/Zotero/storage/FCBQWVUR/s12920-023-01769-4.html}
}

@article{garnelo_conditional_,
  title = {Conditional {{Neural Processes}}},
  author = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Chris J and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo J and Eslami, S M Ali},
  pages = {10},
  abstract = {Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are inspired by the flexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classification and image completion.},
  langid = {english},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8TZ669N9/garnelo_conditiona_.pdf}
}

@article{garnelo_neural_2018,
  title = {Neural {{Processes}}},
  author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.01622 [cs, stat]},
  eprint = {1807.01622},
  primaryclass = {cs, stat},
  urldate = {2019-01-07},
  abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PT8NMLYF/garnelo_neural_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/N7EH695F/1807.html}
}

@book{garrels_bash_2010,
  title = {Bash {{Guide}} for {{Beginners}}},
  author = {Garrels, Machtelt},
  year = {2010},
  publisher = {Fultus Corporation},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PR5673XK/garrels_bash_2010.pdf}
}

@article{gastal_highorder_2015,
  title = {High-{{Order Recursive Filtering}} of {{Non-Uniformly Sampled Signals}} for {{Image}} and {{Video Processing}}},
  author = {Gastal, Eduardo S. L. and Oliveira, Manuel M.},
  year = {2015},
  month = may,
  journal = {Computer Graphics Forum},
  volume = {34},
  number = {2},
  pages = {81--93},
  issn = {01677055},
  doi = {10.1111/cgf.12543},
  urldate = {2020-07-16},
  abstract = {We present a discrete-time mathematical formulation for applying recursive digital filters to non-uniformly sampled signals. Our solution presents several desirable features: it preserves the stability of the original filters; is well-conditioned for low-pass, high-pass, and band-pass filters alike; its cost is linear in the number of samples and is not affected by the size of the filter support. Our method is general and works with any non-uniformly sampled signal and any recursive digital filter defined by a difference equation. Since our formulation directly uses the filter coefficients, it works out-of-the-box with existing methodologies for digital filter design. We demonstrate the effectiveness of our approach by filtering non-uniformly sampled signals in various image and video processing tasks including edge-preserving color filtering, noise reduction, stylization, and detail enhancement. Our formulation enables, for the first time, edge-aware evaluation of any recursive infinite impulse response digital filter (not only low-pass), producing high-quality filtering results in real time.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/355Z6QKA/Gastal and Oliveira - 2015 - High-Order Recursive Filtering of Non-Uniformly Sa.pdf}
}

@article{gatys_neural_2015,
  title = {A {{Neural Algorithm}} of {{Artistic Style}}},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  year = {2015},
  month = aug,
  journal = {arXiv:1508.06576 [cs, q-bio]},
  eprint = {1508.06576},
  primaryclass = {cs, q-bio},
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6Q7QU5JM/gatys_a neural_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/UX7ETRDR/1508.html}
}

@article{gbd2016causesofdeathcollaborators_global_2017,
  title = {Global, Regional, and National Age-Sex Specific Mortality for 264 Causes of Death, 1980-2016: A Systematic Analysis for the {{Global Burden}} of {{Disease Study}} 2016},
  shorttitle = {Global, Regional, and National Age-Sex Specific Mortality for 264 Causes of Death, 1980-2016},
  author = {{GBD 2016 Causes of Death Collaborators}},
  year = {2017},
  month = sep,
  journal = {Lancet (London, England)},
  volume = {390},
  number = {10100},
  pages = {1151--1210},
  issn = {1474-547X},
  doi = {10.1016/S0140-6736(17)32152-9},
  abstract = {BACKGROUND: Monitoring levels and trends in premature mortality is crucial to understanding how societies can address prominent sources of early death. The Global Burden of Disease 2016 Study (GBD 2016) provides a comprehensive assessment of cause-specific mortality for 264 causes in 195 locations from 1980 to 2016. This assessment includes evaluation of the expected epidemiological transition with changes in development and where local patterns deviate from these trends. METHODS: We estimated cause-specific deaths and years of life lost (YLLs) by age, sex, geography, and year. YLLs were calculated from the sum of each death multiplied by the standard life expectancy at each age. We used the GBD cause of death database composed of: vital registration (VR) data corrected for under-registration and garbage coding; national and subnational verbal autopsy (VA) studies corrected for garbage coding; and other sources including surveys and surveillance systems for specific causes such as maternal mortality. To facilitate assessment of quality, we reported on the fraction of deaths assigned to GBD Level 1 or Level 2 causes that cannot be underlying causes of death (major garbage codes) by location and year. Based on completeness, garbage coding, cause list detail, and time periods covered, we provided an overall data quality rating for each location with scores ranging from 0 stars (worst) to 5 stars (best). We used robust statistical methods including the Cause of Death Ensemble model (CODEm) to generate estimates for each location, year, age, and sex. We assessed observed and expected levels and trends of cause-specific deaths in relation to the Socio-demographic Index (SDI), a summary indicator derived from measures of average income per capita, educational attainment, and total fertility, with locations grouped into quintiles by SDI. Relative to GBD 2015, we expanded the GBD cause hierarchy by 18 causes of death for GBD 2016. FINDINGS: The quality of available data varied by location. Data quality in 25 countries rated in the highest category (5 stars), while 48, 30, 21, and 44 countries were rated at each of the succeeding data quality levels. Vital registration or verbal autopsy data were not available in 27 countries, resulting in the assignment of a zero value for data quality. Deaths from non-communicable diseases (NCDs) represented 72{$\cdot$}3\% (95\% uncertainty interval [UI] 71{$\cdot$}2-73{$\cdot$}2) of deaths in 2016 with 19{$\cdot$}3\% (18{$\cdot$}5-20{$\cdot$}4) of deaths in that year occurring from communicable, maternal, neonatal, and nutritional (CMNN) diseases and a further 8{$\cdot$}43\% (8{$\cdot$}00-8{$\cdot$}67) from injuries. Although age-standardised rates of death from NCDs decreased globally between 2006 and 2016, total numbers of these deaths increased; both numbers and age-standardised rates of death from CMNN causes decreased in the decade 2006-16-age-standardised rates of deaths from injuries decreased but total numbers varied little. In 2016, the three leading global causes of death in children under-5 were lower respiratory infections, neonatal preterm birth complications, and neonatal encephalopathy due to birth asphyxia and trauma, combined resulting in 1{$\cdot$}80 million deaths (95\% UI 1{$\cdot$}59 million to 1{$\cdot$}89 million). Between 1990 and 2016, a profound shift toward deaths at older ages occurred with a 178\% (95\% UI 176-181) increase in deaths in ages 90-94 years and a 210\% (208-212) increase in deaths older than age 95 years. The ten leading causes by rates of age-standardised YLL significantly decreased from 2006 to 2016 (median annualised rate of change was a decrease of 2{$\cdot$}89\%); the median annualised rate of change for all other causes was lower (a decrease of 1{$\cdot$}59\%) during the same interval. Globally, the five leading causes of total YLLs in 2016 were cardiovascular diseases; diarrhoea, lower respiratory infections, and other common infectious diseases; neoplasms; neonatal disorders; and HIV/AIDS and tuberculosis. At a finer level of disaggregation within cause groupings, the ten leading causes of total YLLs in 2016 were ischaemic heart disease, cerebrovascular disease, lower respiratory infections, diarrhoeal diseases, road injuries, malaria, neonatal preterm birth complications, HIV/AIDS, chronic obstructive pulmonary disease, and neonatal encephalopathy due to birth asphyxia and trauma. Ischaemic heart disease was the leading cause of total YLLs in 113 countries for men and 97 countries for women. Comparisons of observed levels of YLLs by countries, relative to the level of YLLs expected on the basis of SDI alone, highlighted distinct regional patterns including the greater than expected level of YLLs from malaria and from HIV/AIDS across sub-Saharan Africa; diabetes mellitus, especially in Oceania; interpersonal violence, notably within Latin America and the Caribbean; and cardiomyopathy and myocarditis, particularly in eastern and central Europe. The level of YLLs from ischaemic heart disease was less than expected in 117 of 195 locations. Other leading causes of YLLs for which YLLs were notably lower than expected included neonatal preterm birth complications in many locations in both south Asia and southeast Asia, and cerebrovascular disease in western Europe. INTERPRETATION: The past 37 years have featured declining rates of communicable, maternal, neonatal, and nutritional diseases across all quintiles of SDI, with faster than expected gains for many locations relative to their SDI. A global shift towards deaths at older ages suggests success in reducing many causes of early death. YLLs have increased globally for causes such as diabetes mellitus or some neoplasms, and in some locations for causes such as drug use disorders, and conflict and terrorism. Increasing levels of YLLs might reflect outcomes from conditions that required high levels of care but for which effective treatments remain elusive, potentially increasing costs to health systems. FUNDING: Bill \& Melinda Gates Foundation.},
  langid = {english},
  pmcid = {PMC5605883},
  pmid = {28919116},
  keywords = {{Aged, 80 and over},{Child, Preschool},{Infant, Newborn},Adolescent,Adult,Age Distribution,Aged,Cause of Death,Child,Communicable Diseases,Disasters,Female,Global Burden of Disease,Global Health,Humans,Infant,Male,Middle Aged,Noncommunicable Diseases,Nutrition Disorders,Pregnancy,Pregnancy Complications,Socioeconomic Factors,Wounds and Injuries,Young Adult},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GG6RUL7J/gbd 2016 causes of death collaborators_global,_2017.pdf}
}

@article{gedon_deep_2020,
  title = {Deep {{State Space Models}} for {{Nonlinear System Identification}}},
  author = {Gedon, Daniel and Wahlstr{\"o}m, Niklas and Sch{\"o}n, Thomas B. and Ljung, Lennart},
  year = {2020},
  journal = {Proceedings of the 2020 IEEE  59th Conference on Decision and Control},
  eprint = {2003.14162},
  abstract = {An actively evolving model class for generative temporal models developed in the deep learning community are deep state space models (SSMs) which have a close connection to classic SSMs. In this work six new deep SSMs are implemented and evaluated for the identification of established nonlinear dynamic system benchmarks. The models and their parameter learning algorithms are elaborated rigorously. The usage of deep SSMs as a black-box identification model can describe a wide range of dynamics due to the flexibility of deep neural networks. Additionally, the uncertainty of the system is modelled and therefore one obtains a much richer representation and a whole class of systems to describe the underlying dynamics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GKS9JTIS/Gedon et al. - 2020 - Deep State Space Models for Nonlinear System Ident.pdf;/Users/antoniohortaribeiro/Zotero/storage/5DYNF4J8/2003.html}
}

@inproceedings{gedon_first_2021,
  title = {First {{Steps Towards Self-Supervised Pretraining}} of the 12-{{Lead ECG}}},
  booktitle = {Computing in {{Cardiology}} ({{CinC}})},
  author = {Gedon, Daniel and Ribeiro, Ant{\^o}nio H. and Wahlstr{\"o}m, Niklas and Sch{\"o}n, Thomas B.},
  year = {2021},
  month = sep,
  volume = {48},
  pages = {1--4},
  issn = {2325-887X},
  doi = {10.23919/CinC53138.2021.9662748},
  abstract = {Self-supervised learning is a paradigm that extracts general features which describe the input space by artificially generating labels from the input without the need for explicit annotations. The learned features can then be used by transfer learning to boost the performance on a downstream task. Such methods have recently produced state of the art results in natural language processing and computer vision. Here, we propose a self-supervised learning method for 12-lead electrocardiograms (ECGs). For pretraining the model we design a task to mask out subsegements of all channels of the input signals and try to predict the actual values. As the model architecture, we use a U-ResNet containing an encoder-decoder structure. We test our method by self-supervised pretraining on the CODE dataset and then transfer the learnt features by finetuning on the PTB-XL and CPSC benchmarks to evaluate the effect of our method in the classification of 12-leads ECGs. The method does provide modest improvements in performance when compared to not using pretraining. In future work we will make use of these ideas in smaller dataset, where we believe it can lead to larger performance gains.},
  copyright = {All rights reserved},
  keywords = {Computational modeling,Computer vision,Electrocardiography,Learning systems,Performance gain,Predictive models,Transfer learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3WQM8VSB/Gedon et al_2021_First Steps Towards Self-Supervised Pretraining of the 12-Lead ECG.pdf;/Users/antoniohortaribeiro/Zotero/storage/5LMQ4GUS/9662748.html}
}

@article{gedon_invertible_2023,
  title = {Invertible {{Kernel PCA}} with {{Random Fourier Features}}},
  author = {Gedon, Daniel and Ribeiro, Ant{\^o}nio H. and Wahlstr{\"o}m, Niklas and Sch{\"o}n, Thomas B.},
  year = {2023},
  month = may,
  journal = {IEEE Signal Processing Letters},
  eprint = {2303.05043},
  doi = {10.1109/LSP.2023.3275499},
  abstract = {Kernel principal component analysis (kPCA) is a widely studied method to construct a low-dimensional data representation after a nonlinear transformation. The prevailing method to reconstruct the original input signal from kPCA---an important task for denoising---requires us to solve a supervised learning problem. In this paper, we present an alternative method where the reconstruction follows naturally from the compression step. We first approximate the kernel with random Fourier features. Then, we exploit the fact that the nonlinear transformation is invertible in a certain subdomain. Hence, the name invertible kernel PCA (ikPCA) . We experiment with different data modalities and show that ikPCA performs similarly to kPCA with supervised reconstruction on denoising tasks, making it a strong alternative.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q43LG9C2/Gedon et al. - 2023 - Invertible Kernel PCA with Random Fourier Features.pdf;/Users/antoniohortaribeiro/Zotero/storage/NW5IY6KD/2303.html}
}

@article{gedon_no_2024,
  title = {No {{Double Descent}} in {{Principal Component Regression}}: {{A High-Dimensional Analysis}}},
  author = {Gedon, Daniel and Ribeiro, Antonio H. and Sch{\"o}n, Thomas B.},
  year = {2024},
  journal = {International Conference on Machine Learning (ICML)},
  copyright = {All rights reserved}
}

@inproceedings{gedon_resnetbased_2021,
  title = {{{ResNet-based ECG Diagnosis}} of {{Myocardial Infarction}} in the {{Emergency Department}}},
  booktitle = {Machine Learning from Ground Truth: {{New}} Medical Imaging Datasets for Unsolved Medical Problems {{Workshop}} at {{NeurIPS}}},
  author = {Gedon, Daniel and Gustafsson, Stefan and Lampa, Erik and Ribeiro, Ant{\^o}nio H. and Holzmann, Martin J. and Sch{\"o}n, Thomas B. and Sundstr{\"o}m, Johan},
  year = {2021},
  copyright = {All rights reserved}
}

@inproceedings{gehring_convolutional_2017,
  title = {A {{Convolutional Encoder Model}} for {{Neural Machine Translation}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gehring, Jonas and Auli, Michael and Grangier, David and Dauphin, Yann},
  year = {2017},
  volume = {1},
  pages = {123--135}
}

@inproceedings{geiger_are_2012,
  title = {Are We Ready for Autonomous Driving? {{The KITTI}} Vision Benchmark Suite},
  shorttitle = {Are We Ready for Autonomous Driving?},
  booktitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Geiger, A. and Lenz, P. and Urtasun, R.},
  year = {2012},
  month = jun,
  pages = {3354--3361},
  publisher = {IEEE},
  address = {Providence, RI},
  doi = {10/gf7nxj},
  urldate = {2019-09-06},
  isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XT35GD9J/Geiger et al. - 2012 - Are we ready for autonomous driving The KITTI vis.pdf}
}

@article{geiger_jamming_2019,
  title = {Jamming Transition as a Paradigm to Understand the Loss Landscape of Deep Neural Networks},
  author = {Geiger, Mario and Spigler, Stefano and {d'Ascoli}, St{\'e}phane and Sagun, Levent and {Baity-Jesi}, Marco and Biroli, Giulio and Wyart, Matthieu},
  year = {2019},
  month = jul,
  journal = {Physical Review E},
  volume = {100},
  number = {1},
  pages = {012115},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.100.012115},
  urldate = {2021-05-23},
  abstract = {Deep learning has been immensely successful at a variety of tasks, ranging from classification to artificial intelligence. Learning corresponds to fitting training data, which is implemented by descending a very high-dimensional loss function. Understanding under which conditions neural networks do not get stuck in poor minima of the loss, and how the landscape of that loss evolves as depth is increased, remains a challenge. Here we predict, and test empirically, an analogy between this landscape and the energy landscape of repulsive ellipses. We argue that in fully connected deep networks a phase transition delimits the over- and underparametrized regimes where fitting can or cannot be achieved. In the vicinity of this transition, properties of the curvature of the minima of the loss (the spectrum of the Hessian) are critical. This transition shares direct similarities with the jamming transition by which particles form a disordered solid as the density is increased, which also occurs in certain classes of computational optimization and learning problems such as the perceptron. Our analysis gives a simple explanation as to why poor minima of the loss cannot be encountered in the overparametrized regime. Interestingly, we observe that the ability of fully connected networks to fit random data is independent of their depth, an independence that appears to also hold for real data. We also study a quantity {$\Delta$} which characterizes how well ({$\Delta<$}0) or badly ({$\Delta>$}0) a datum is learned. At the critical point it is power-law distributed on several decades, P+({$\Delta$}){$\sim\Delta\theta$} for {$\Delta>$}0 and P-({$\Delta$}){$\sim$}(-{$\Delta$})-{$\gamma$} for {$\Delta<$}0, with exponents that depend on the choice of activation function. This observation suggests that near the transition the loss landscape has a hierarchical structure and that the learning dynamics is prone to avalanche-like dynamics, with abrupt changes in the set of patterns that are learned.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/77SD3X7P/Geiger et al. - 2019 - Jamming transition as a paradigm to understand the.pdf;/Users/antoniohortaribeiro/Zotero/storage/FRFKZSLI/PhysRevE.100.html}
}

@article{geiger_scaling_2020,
  title = {Scaling Description of Generalization with Number of Parameters in Deep Learning},
  author = {Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and {d'Ascoli}, St{\'e}phane and Biroli, Giulio and Hongler, Cl{\'e}ment and Wyart, Matthieu},
  year = {2020},
  month = feb,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2020},
  number = {2},
  eprint = {1901.01608},
  pages = {023401},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/ab633c},
  urldate = {2021-05-23},
  abstract = {Supervised deep learning involves the training of neural networks with a large number \$N\$ of parameters. For large enough \$N\$, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as \$N\$ grows past a certain threshold \$N{\textasciicircum}\{*\}\$. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with \$N\$. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations \${\textbackslash}{\textbar}f\_\{N\}-{\textbackslash}bar\{f\}\_\{N\}{\textbackslash}{\textbar}{\textbackslash}sim N{\textasciicircum}\{-1/4\}\$ of the neural net output function \$f\_\{N\}\$ around its expectation \${\textbackslash}bar\{f\}\_\{N\}\$. These affect the generalization error \${\textbackslash}epsilon\_\{N\}\$ for classification: under natural assumptions, it decays to a plateau value \${\textbackslash}epsilon\_\{{\textbackslash}infty\}\$ in a power-law fashion \${\textbackslash}sim N{\textasciicircum}\{-1/2\}\$. This description breaks down at a so-called jamming transition \$N=N{\textasciicircum}\{*\}\$. At this threshold, we argue that \${\textbackslash}{\textbar}f\_\{N\}{\textbackslash}{\textbar}\$ diverges. This result leads to a plausible explanation for the cusp in test error known to occur at \$N{\textasciicircum}\{*\}\$. Our results are confirmed by extensive empirical observations on the MNIST and CIFAR image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond \$N{\textasciicircum}\{*\}\$, and averaging their outputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PZSRRTU2/Geiger et al. - 2020 - Scaling description of generalization with number .pdf;/Users/antoniohortaribeiro/Zotero/storage/GQBJ8NT7/1901.html}
}

@inproceedings{geisert_trajectory_2016,
  title = {Trajectory Generation for Quadrotor Based Systems Using Numerical Optimal Control},
  booktitle = {Robotics and {{Automation}} ({{ICRA}}), 2016 {{IEEE International Conference}} On},
  author = {Geisert, Mathieu and Mansard, Nicolas},
  year = {2016},
  pages = {2958--2964},
  publisher = {IEEE},
  isbn = {1-4673-8026-1}
}

@inproceedings{geisert_trajectory_2016a,
  title = {Trajectory {{Generation}} for {{Quadrotor Based Systems Using Numerical Optimal Control}}},
  booktitle = {2016 {{IEEE}} International Conference on Robotics and Automation ({{ICRA}})},
  author = {Geisert, Mathieu and Mansard, Nicolas},
  year = {2016},
  pages = {2958--2964},
  publisher = {IEEE},
  isbn = {1-4673-8026-1}
}

@article{genton_classes_2001,
  title = {Classes of {{Kernels}} for {{Machine Learning}}: {{A Statistics Perspective}}},
  shorttitle = {Classes of {{Kernels}} for {{Machine Learning}}},
  author = {Genton, Marc G.},
  year = {2001},
  journal = {Journal of Machine Learning Research},
  volume = {2},
  number = {Dec},
  pages = {299--312},
  issn = {ISSN 1533-7928},
  urldate = {2021-11-23},
  abstract = {In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HCTBXMT6/Genton - 2001 - Classes of Kernels for Machine Learning A Statist.pdf}
}

@article{gevers_identification_2005,
  title = {Identification for Control: {{From}} the Early Achievements to the Revival of Experiment Design},
  shorttitle = {Identification for Control},
  author = {Gevers, Michel},
  year = {2005},
  journal = {European Journal of Control},
  volume = {11},
  number = {4-5},
  pages = {335--352},
  publisher = {Elsevier}
}

@article{ghorbani_linearized_2020,
  title = {Linearized Two-Layers Neural Networks in High Dimension},
  author = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  year = {2020},
  month = feb,
  journal = {arXiv:1904.12191 [cs, math, stat]},
  eprint = {1904.12191},
  primaryclass = {cs, math, stat},
  urldate = {2021-04-23},
  abstract = {We consider the problem of learning an unknown function \$f\_\{{\textbackslash}star\}\$ on the \$d\$-dimensional sphere with respect to the square loss, given i.i.d. samples \${\textbackslash}\{(y\_i,\{{\textbackslash}boldsymbol x\}\_i){\textbackslash}\}\_\{i{\textbackslash}le n\}\$ where \$\{{\textbackslash}boldsymbol x\}\_i\$ is a feature vector uniformly distributed on the sphere and \$y\_i=f\_\{{\textbackslash}star\}(\{{\textbackslash}boldsymbol x\}\_i)+{\textbackslash}varepsilon\_i\$. We study two popular classes of models that can be regarded as linearizations of two-layers neural networks around a random initialization: the random features model of Rahimi-Recht (RF); the neural tangent kernel model of Jacot-Gabriel-Hongler (NT). Both these approaches can also be regarded as randomized approximations of kernel ridge regression (with respect to different kernels), and enjoy universal approximation properties when the number of neurons \$N\$ diverges, for a fixed dimension \$d\$. We consider two specific regimes: the approximation-limited regime, in which \$n={\textbackslash}infty\$ while \$d\$ and \$N\$ are large but finite; and the sample size-limited regime in which \$N={\textbackslash}infty\$ while \$d\$ and \$n\$ are large but finite. In the first regime we prove that if \$d{\textasciicircum}\{{\textbackslash}ell + {\textbackslash}delta\} {\textbackslash}le N{\textbackslash}le d{\textasciicircum}\{{\textbackslash}ell+1-{\textbackslash}delta\}\$ for small \${\textbackslash}delta {$>$} 0\$, then {\textbackslash}RF{\textbackslash}, effectively fits a degree-\${\textbackslash}ell\$ polynomial in the raw features, and {\textbackslash}NT{\textbackslash}, fits a degree-\$({\textbackslash}ell+1)\$ polynomial. In the second regime, both RF and NT reduce to kernel methods with rotationally invariant kernels. We prove that, if the number of samples is \$d{\textasciicircum}\{{\textbackslash}ell + {\textbackslash}delta\} {\textbackslash}le n {\textbackslash}le d{\textasciicircum}\{{\textbackslash}ell +1-{\textbackslash}delta\}\$, then kernel methods can fit at most a a degree-\${\textbackslash}ell\$ polynomial in the raw features. This lower bound is achieved by kernel ridge regression. Optimal prediction error is achieved for vanishing ridge regularization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Y62J95DW/Ghorbani et al. - 2020 - Linearized two-layers neural networks in high dime.pdf;/Users/antoniohortaribeiro/Zotero/storage/ZCHLHKUJ/1904.html}
}

@article{ghosh_objective_2011,
  title = {Objective {{Priors}}: {{An Introduction}} for {{Frequentists}}},
  shorttitle = {Objective {{Priors}}},
  author = {Ghosh, Malay},
  year = {2011},
  month = may,
  journal = {Statistical Science},
  volume = {26},
  number = {2},
  eprint = {1108.2120},
  pages = {187--202},
  issn = {0883-4237},
  doi = {10.1214/10-STS338},
  urldate = {2018-10-07},
  abstract = {Bayesian methods are increasingly applied in these days in the theory and practice of statistics. Any Bayesian inference depends on a likelihood and a prior. Ideally one would like to elicit a prior from related sources of information or past data. However, in its absence, Bayesian methods need to rely on some "objective" or "default" priors, and the resulting posterior inference can still be quite valuable. Not surprisingly, over the years, the catalog of objective priors also has become prohibitively large, and one has to set some specific criteria for the selection of such priors. Our aim is to review some of these criteria, compare their performance, and illustrate them with some simple examples. While for very large sample sizes, it does not possibly matter what objective prior one uses, the selection of such a prior does influence inference for small or moderate samples. For regular models where asymptotic normality holds, Jeffreys' general rule prior, the positive square root of the determinant of the Fisher information matrix, enjoys many optimality properties in the absence of nuisance parameters. In the presence of nuisance parameters, however, there are many other priors which emerge as optimal depending on the criterion selected. One new feature in this article is that a prior different from Jeffreys' is shown to be optimal under the chi-square divergence criterion even in the absence of nuisance parameters. The latter is also invariant under one-to-one reparameterization.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MJ893VS3/ghosh_objective_2011.pdf;/Users/antoniohortaribeiro/Zotero/storage/9JX2SYLQ/1108.html}
}

@article{gilmer_adversarial_2018,
  title = {Adversarial {{Spheres}}},
  author = {Gilmer, Justin and Metz, Luke and Faghri, Fartash and Schoenholz, Samuel S. and Raghu, Maithra and Wattenberg, Martin and Goodfellow, Ian},
  year = {2018},
  month = sep,
  journal = {arXiv:1801.02774},
  eprint = {1801.02774},
  urldate = {2020-03-20},
  abstract = {Machine learning models with very low test error have been shown to be consistently vulnerable to small, adversarially chosen perturbations of the input. We hypothesize that this counterintuitive behavior is a result of the high-dimensional geometry of the data manifold, and explore this hypothesis on a simple highdimensional dataset. For this dataset we show a fundamental bound relating the classification error rate to the average distance to the nearest misclassification, which is independent of the model. We train different neural network architectures on this dataset and show their error sets approach this theoretical bound. As a result of the theory, the vulnerability of machine learning models to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this foundational synthetic case will point a way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T45,Computer Science - Computer Vision and Pattern Recognition,I.2.6},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SFQZDMN9/Gilmer et al. - 2018 - Adversarial Spheres.pdf}
}

@inproceedings{girshick_fast_2015,
  title = {Fast R-Cnn},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Girshick, Ross},
  year = {2015},
  pages = {1440--1448},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AAWKDQVI/girshick_fast r-cnn_2015.pdf}
}

@inproceedings{girshick_rich_2014,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2014},
  pages = {580--587},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UPQGEABK/girshick_rich_2014.pdf}
}

@inproceedings{globerson_nightmare_2006,
  title = {Nightmare at Test Time: Robust Learning by Feature Deletion},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning  ({{ICML}})},
  author = {Globerson, Amir and Roweis, Sam},
  year = {2006},
  pages = {353--360},
  doi = {10.1145/1143844.1143889},
  abstract = {When constructing a classifier from labeled data, it is important not to assign too much weight to any single input feature, in order to increase the robustness of the classifier. This is particularly important in domains with nonstationary feature distributions or with input sensor failures. A common approach to achieving such robustness is to introduce regularization which spreads the weight more evenly between the features. However, this strategy is very generic, and cannot induce robustness specifically tailored to the classification task at hand. In this work, we introduce a new algorithm for avoiding single feature over-weighting by analyzing robustness using a game theoretic formalization. We develop classifiers which are optimally resilient to deletion of features in a minimax sense, and show how to construct such classifiers using quadratic programming. We illustrate the applicability of our methods on spam filtering and handwritten digit recognition tasks, where feature deletion is indeed a realistic noise model.},
  isbn = {978-1-59593-383-6},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3DE88VBX/Globerson and Roweis - 2006 - Nightmare at test time robust learning by feature.pdf}
}

@article{glorot_apprentissage_2015,
  title = {Apprentissage Des R{\'e}seaux de Neurones Profonds et Applications En Traitement Automatique de La Langue Naturelle},
  author = {Glorot, Xavier},
  year = {2015},
  keywords = {ðŸ”No DOI found}
}

@inproceedings{glorot_understanding_2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  pages = {249--256},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9ZV9RWX2/glorot_understand_2010.pdf}
}

@article{gluch_constructing_,
  title = {Constructing a Provably Adversarially-Robust Classifier from a High Accuracy One},
  author = {G{\l}uch, Grzegorz and Urbanke, R{\"u}diger},
  pages = {10},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/LPL43WK5/GÅ‚uch and Urbanke - Constructing a provably adversarially-robust class.pdf}
}

@article{goethals_identification_2005,
  title = {Identification of {{MIMO Hammerstein}} Models Using Least Squares Support Vector Machines},
  author = {Goethals, Ivan and Pelckmans, Kristiaan and Suykens, Johan A. K. and De Moor, Bart},
  year = {2005},
  month = jul,
  journal = {Automatica},
  volume = {41},
  number = {7},
  pages = {1263--1272},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2005.02.002},
  abstract = {This paper studies a method for the identification of Hammerstein models based on least squares support vector machines (LS-SVMs). The technique allows for the determination of the memoryless static nonlinearity as well as the estimation of the model parameters of the dynamic ARX part. This is done by applying the equivalent of Bai's overparameterization method for identification of Hammerstein systems in an LS-SVM context. The SISO as well as the MIMO identification cases are elaborated. The technique can lead to significant improvements with respect to classical overparameterization methods as illustrated in a number of examples. Another important advantage is that no stringent assumptions on the nature of the nonlinearity need to be imposed except for a certain degree of smoothness.},
  keywords = {ARX,Hammerstein models,Kernel methods,LS-SVM,MIMO systems},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XSHGUIJ6/goethals_identifica_2005.pdf;/Users/antoniohortaribeiro/Zotero/storage/TEQPEFP4/S0005109805000737.html;/Users/antoniohortaribeiro/Zotero/storage/TQ86T9YH/S0005109805000737.html}
}

@article{goethals_subspace_2005,
  title = {Subspace Identification of {{Hammerstein}} Systems Using Least Squares Support Vector Machines},
  author = {Goethals, I. and Pelckmans, K. and Suykens, J. A. K. and Moor, Bart De},
  year = {2005},
  month = oct,
  journal = {IEEE Transactions on Automatic Control},
  volume = {50},
  number = {10},
  pages = {1509--1519},
  issn = {0018-9286},
  doi = {10.1109/TAC.2005.856647},
  abstract = {This paper presents a method for the identification of multiple-input-multiple-output (MIMO) Hammerstein systems for the goal of prediction. The method extends the numerical algorithms for subspace state space system identification (N4SID), mainly by rewriting the oblique projection in the N4SID algorithm as a set of componentwise least squares support vector machines (LS-SVMs) regression problems. The linear model and static nonlinearities follow from a low-rank approximation of a matrix obtained from this regression problem.},
  keywords = {Biological processes,Biological system modeling,control nonlinearities,Hammerstein models,Hammerstein system,identification,Least squares approximation,least squares approximations,Least squares methods,least squares support vector machines,linear model,linear systems,MIMO,MIMO systems,multiple input multiple output system,N4SID,nonlinear systems,numerical algorithms,regression analysis,regression problem,Signal processing algorithms,state-space methods,static nonlinearities,subspace identification,subspace state space identification,support vector machines,system identification},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DS5NW43T/goethals_subspace_2005.pdf;/Users/antoniohortaribeiro/Zotero/storage/WSWFJAJM/goethals_subspace_2005.pdf;/Users/antoniohortaribeiro/Zotero/storage/7K7K77CP/1516254.html;/Users/antoniohortaribeiro/Zotero/storage/AMXPXG69/1516254.html;/Users/antoniohortaribeiro/Zotero/storage/EF3KUK8H/1516254.html;/Users/antoniohortaribeiro/Zotero/storage/H3GDPH9Z/1516254.html}
}

@book{goldberg_genetic_2000,
  title = {Genetic {{Algorithms}}--{{In Search}}, {{Optimization}} \& {{Machine Learning}}, {{Revised}} Ed},
  author = {Goldberg, David E},
  year = {2000},
  publisher = {Addison Wesley, New Delhi}
}

@article{goldberger_physiobank_2000,
  title = {{{PhysioBank}}, {{PhysioToolkit}}, and {{PhysioNet}}: {{Components}} of a {{New Research Resource}} for {{Complex Physiologic Signals}}},
  shorttitle = {{{PhysioBank}}, {{PhysioToolkit}}, and {{PhysioNet}}},
  author = {Goldberger, Ary L. and Amaral, Luis A. N. and Glass, Leon and Hausdorff, Jeffrey M. and Ivanov, Plamen Ch. and Mark, Roger G. and Mietus, Joseph E. and Moody, George B. and Peng, Chung-Kang and Stanley, H. Eugene},
  year = {2000},
  month = jun,
  journal = {Circulation},
  volume = {101},
  number = {23},
  issn = {0009-7322, 1524-4539},
  doi = {10.1161/01.CIR.101.23.e215},
  urldate = {2018-10-21},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6CRIEET2/goldberger_physiobank_2000.pdf}
}

@book{golub_matrix_2012,
  title = {Matrix {{Computations}}},
  author = {Golub, Gene H and Van Loan, Charles F},
  year = {2012},
  volume = {3},
  publisher = {JHU Press},
  file = {/Users/antoniohortaribeiro/Zotero/storage/N6PN4MPR/golub_matrix_2012.pdf}
}

@inproceedings{gong_impact_2018,
  title = {Impact of {{Aliasing}} on {{Deep CNN-Based End-to-End Acoustic Models}}},
  booktitle = {Interspeech},
  author = {Gong, Yuan and Poellabauer, Christian},
  year = {2018},
  pages = {2698--2702},
  doi = {10.21437/Interspeech.2018-1371},
  urldate = {2020-03-23},
  abstract = {A recent trend in audio and speech processing is to learn target labels directly from raw waveforms rather than hand-crafted acoustic features. Previous work has shown that deep convolutional neural networks (CNNs) as front-end can learn effective representations from the raw waveform. However, due to the large dimension of raw audio waveforms, pooling layers are usually used aggressively between temporal convolutional layers. In essence, these pooling layers perform operations that are similar to signal downsampling, which may lead to temporal aliasing according to the Nyquist-Shannon sampling theorem. This paper explores, using a series of experiments, if and how this aliasing effect impacts modern deep CNN-based models.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/J9A7EFH7/Gong and Poellabauer - 2018 - Impact of Aliasing on Deep CNN-Based End-to-End Ac.pdf}
}

@article{gonzalez_nonlinear_2018,
  title = {Non-Linear System Modeling Using {{LSTM}} Neural Networks},
  author = {Gonzalez, Jes{\'u}s and Yu, Wen},
  year = {2018},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {2nd {{IFAC Conference}} on {{Modelling}}, {{Identification}} and {{Control}} of {{Nonlinear Systems MICNON}} 2018},
  volume = {51},
  number = {13},
  pages = {485--489},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2018.07.326},
  urldate = {2022-06-09},
  abstract = {Long-Short Term Memory (LSTM) is a type of Recurrent Neural Networks (RNN). It takes sequences of information and uses recurrent mechanisms and gate techniques. LSTM has many advantages over other feedforward and recurrent NNs in modeling of time series, such as audio and video. However, in non-linear system modeling normal LSTM does not work well(Wang, 2017). In this paper, we combine LSTM with NN, and use the advantages. The novel neural model consists of hierarchical recurrent networks and one multilayer perceptron. We design a special learning algorithm which uses backpropagation, and backpropagation through time methods. We use two non-linear system examples to compare our neural modeling with other well known methods. The results show that for the simulation model (only the test input is used and the past test output is not used), the modified LSTM model proposed in this paper is much better than the other existing neural models.},
  langid = {english},
  keywords = {Black-Box identification,LSTM,neural networks},
  file = {/Users/antoniohortaribeiro/Zotero/storage/F3WEJQNU/Gonzalez_Yu_2018_Non-linear system modeling using LSTM neural networks.pdf;/Users/antoniohortaribeiro/Zotero/storage/D79VE33N/S2405896318310814.html}
}

@book{goodfellow_deep_2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {MIT Press}
}

@inproceedings{goodfellow_explaining_2015,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  year = {2015},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  keywords = {â›” No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/X3QNR7EM/Goodfellow et al. - 2014 - Explaining and Harnessing Adversarial Examples.pdf}
}

@incollection{goodfellow_generative_2014,
  title = {Generative {{Adversarial Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {2672--2680},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-10-04},
  file = {/Users/antoniohortaribeiro/Zotero/storage/25QWJYDJ/goodfellow_generative_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/R8UV9PBM/5423-generative-adversarial-nets.html}
}

@article{gopalakrishnan_combating_2018,
  title = {Combating {{Adversarial Attacks Using Sparse Representations}}},
  author = {Gopalakrishnan, Soorya and Marzi, Zhinus and Madhow, Upamanyu and Pedarsani, Ramtin},
  year = {2018},
  month = jul,
  journal = {arXiv:1803.03880},
  eprint = {1803.03880},
  urldate = {2022-02-03},
  abstract = {It is by now well-known that small adversarial perturbations can induce classification errors in deep neural networks (DNNs). In this paper, we make the case that sparse representations of the input data are a crucial tool for combating such attacks. For linear classifiers, we show that a sparsifying front end is provably effective against \${\textbackslash}ell\_\{{\textbackslash}infty\}\$-bounded attacks, reducing output distortion due to the attack by a factor of roughly \$K / N\$ where \$N\$ is the data dimension and \$K\$ is the sparsity level. We then extend this concept to DNNs, showing that a "locally linear" model can be used to develop a theoretical foundation for crafting attacks and defenses. Experimental results for the MNIST dataset show the efficacy of the proposed sparsifying front end.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VRAS8ZLV/Gopalakrishnan et al. - 2018 - Combating Adversarial Attacks Using Sparse Represe.pdf;/Users/antoniohortaribeiro/Zotero/storage/AK9BSQRX/1803.html}
}

@inproceedings{gorelick_fast_2013,
  title = {Fast Trust Region for Segmentation},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Gorelick, Lena and Schmidt, Frank R and Boykov, Yuri},
  year = {2013},
  pages = {1714--1721}
}

@article{goto_artificial_2019,
  title = {Artificial Intelligence to Predict Needs for Urgent Revascularization from 12-Leads Electrocardiography in Emergency Patients},
  author = {Goto, Shinichi and Kimura, Mai and Katsumata, Yoshinori and Goto, Shinya and Kamatani, Takashi and Ichihara, Genki and Ko, Seien and Sasaki, Junichi and Fukuda, Keiichi and Sano, Motoaki},
  year = {2019},
  month = sep,
  journal = {PLOS ONE},
  volume = {14},
  number = {1},
  pages = {e0210103},
  issn = {1932-6203},
  doi = {10/gf286h},
  urldate = {2019-05-29},
  abstract = {Background Patient with acute coronary syndrome benefits from early revascularization. However, methods for the selection of patients who require urgent revascularization from a variety of patients visiting the emergency room with chest symptoms is not fully established. Electrocardiogram is an easy and rapid procedure, but may contain crucial information not recognized even by well-trained physicians. Objective To make a prediction model for the needs for urgent revascularization from 12-lead electrocardiogram recorded in the emergency room. Method We developed an artificial intelligence model enabling the detection of hidden information from a 12-lead electrocardiogram recorded in the emergency room. Electrocardiograms obtained from consecutive patients visiting the emergency room at Keio University Hospital from January 2012 to April 2018 with chest discomfort was collected. These data were splitted into validation and derivation dataset with no duplication in each dataset. The artificial intelligence model was constructed to select patients who require urgent revascularization within 48 hours. The model was trained with the derivation dataset and tested using the validation dataset. Results Of the consecutive 39,619 patients visiting the emergency room with chest discomfort, 362 underwent urgent revascularization. Of them, 249 were included in the derivation dataset and the remaining 113 were included in validation dataset. For the control, 300 were randomly selected as derivation dataset and another 130 patients were randomly selected for validation dataset from the 39,317 who did not undergo urgent revascularization. On validation, our artificial intelligence model had predictive value of the c-statistics 0.88 (95\% CI 0.84--0.93) for detecting patients who required urgent revascularization. Conclusions Our artificial intelligence model provides information to select patients who need urgent revascularization from only 12-leads electrocardiogram in those visiting the emergency room with chest discomfort.},
  langid = {english},
  keywords = {Artificial intelligence,Convolution,Coronary revascularization,Critical care and emergency medicine,Electrocardiography,Neural networks,Recurrent neural networks,Revascularization},
  file = {/Users/antoniohortaribeiro/Zotero/storage/F3G8RGHG/goto_artificial_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/Q4SPAHJI/article.html}
}

@article{goudis_chargeaf_2022,
  title = {{{CHARGE-AF}}: {{A Useful Score For Atrial Fibrillation Prediction}}?},
  shorttitle = {{{CHARGE-AF}}},
  author = {Goudis, Christos and Daios, Stylianos and Dimitriadis, Fotios and Liu, Tong},
  year = {2022},
  journal = {Current Cardiology Reviews},
  volume = {19},
  number = {2},
  pages = {5--10},
  doi = {10.2174/1573403X18666220901102557},
  urldate = {2024-06-10},
  abstract = {Atrial fibrillation (AF) is the commonest arrhythmia in clinical practice and is associated with increased morbidity and mortality. Various predictive scores for new-onset AF have been proposed, but so far, none have been widely used in clinical practice. CHARGE-AF score was developed from a pooled diverse population from three large cohorts (Atherosclerosis Risk in Communities study, Cardiovascular Health Study and Framingham Heart Study). A simple 5-year predictive model includes the variables of age, race, height, weight, systolic and diastolic blood pressure, current smoking, use of antihypertensive medication, diabetes mellitus, history of myocardial infarction and heart failure. Recent studies report that the CHARGE-AF score has good discrimination for incident AF and seems to be a promising prediction model for this arrhythmia. New screening tools (smartphone apps, smartwatches) are rapidly developing for AF detection. Therefore, the wide application of the CHARGE-AF score in clinical practice and the upcoming usage of mobile health technologies and smartwatches may result in better AF prediction and adequate stroke prevention, especially in high-risk patients.},
  langid = {english}
}

@article{gouk_regularisation_2018,
  title = {Regularisation of {{Neural Networks}} by {{Enforcing Lipschitz Continuity}}},
  author = {Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and Cree, Michael},
  year = {2018},
  month = sep,
  journal = {arXiv:1804.04368 [cs, stat]},
  eprint = {1804.04368},
  primaryclass = {cs, stat},
  urldate = {2020-06-05},
  abstract = {We investigate the effect of explicitly enforcing the Lipschitz continuity of neural networks with respect to their inputs. To this end, we provide a simple technique for computing an upper bound to the Lipschitz constant of a feed forward neural network composed of commonly used layer types and demonstrate inaccuracies in previous work on this topic. Our technique is then used to formulate training a neural network with a bounded Lipschitz constant as a constrained optimisation problem that can be solved using projected stochastic gradient methods. Our evaluation study shows that, in isolation, our method performs comparatively to state-of-the-art regularisation techniques. Moreover, when combined with existing approaches to regularising neural networks the performance gains are cumulative. We also provide evidence that the hyperparameters are intuitive to tune and demonstrate how the choice of norm for computing the Lipschitz constant impacts the resulting model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AE6GZ3GW/Gouk et al. - 2018 - Regularisation of Neural Networks by Enforcing Lip.pdf;/Users/antoniohortaribeiro/Zotero/storage/ENLKVBCB/1804.html}
}

@article{gould_cutest_2015,
  title = {{{CUTEst}}: A Constrained and Unconstrained Testing Environment with Safe Threads for Mathematical Optimization},
  author = {Gould, Nicholas IM and Orban, Dominique and Toint, Philippe L},
  year = {2015},
  journal = {Computational Optimization and Applications},
  volume = {60},
  number = {3},
  pages = {545--557},
  issn = {0926-6003},
  doi = {10.1007/s10589-014-9687-3},
  file = {/Users/antoniohortaribeiro/Zotero/storage/T26ZGQSF/gould_cutest_2015.pdf}
}

@article{gould_cutest_2015a,
  title = {{{CUTEst}}: {{A Constrained}} and {{Unconstrained Testing Environment}} with {{Safe Threads}} for {{Mathematical Optimization}}},
  author = {Gould, Nicholas IM and Orban, Dominique and Toint, Philippe L},
  year = {2015},
  journal = {Computational Optimization and Applications},
  volume = {60},
  number = {3},
  pages = {545--557},
  issn = {0926-6003},
  doi = {10/gfjwmh}
}

@article{gould_solution_2001,
  title = {On the Solution of Equality Constrained Quadratic Programming Problems Arising in Optimization},
  author = {Gould, Nicholas IM and Hribar, Mary E and Nocedal, Jorge},
  year = {2001},
  journal = {SIAM Journal on Scientific Computing},
  volume = {23},
  number = {4},
  pages = {1376--1395},
  issn = {1064-8275},
  doi = {10.1137/S1064827598345667},
  file = {/Users/antoniohortaribeiro/Zotero/storage/334B5WIE/gould_on the_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/DTKB9T9Z/gould_on the_2001.pdf}
}

@article{gould_solving_1999,
  title = {Solving the Trust-Region Subproblem Using the {{Lanczos}} Method},
  author = {Gould, Nicholas I. M. and Lucidi, Stefano and Roma, Massimo and Toint, Philippe L},
  year = {1999},
  journal = {SIAM Journal on Optimization},
  volume = {9},
  number = {2},
  pages = {504--525},
  doi = {10.1137/S1052623497322735}
}

@article{gower_variancereduced_2020,
  title = {Variance-{{Reduced Methods}} for {{Machine Learning}}},
  author = {Gower, R. M. and Schmidt, M. and Bach, F. and Richt{\'a}rik, P.},
  year = {2020},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {108},
  number = {11},
  pages = {1968--1983},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2020.3028013},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IFDBHNMH/Gower et al. - 2020 - Variance-Reduced Methods for Machine Learning.pdf}
}

@book{graeme_operational_1971,
  title = {Operational Amplifiers: Design and Applications},
  author = {Graeme, J.G. and Tobey, G.E. and Huelsman, L.P. and {Burr-Brown Research Corporation}},
  year = {1971},
  series = {{{McGraw-Hill Electrical}} and {{Electronic Engineering Series}}},
  publisher = {McGraw-Hill},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UTZX4K7U/graeme_operationa_1971.pdf}
}

@book{graham_concrete_1994,
  title = {Concrete Mathematics: A Foundation for Computer Science},
  shorttitle = {Concrete Mathematics},
  author = {Graham, Ronald L. and Knuth, Donald Ervin and Patashnik, Oren},
  year = {1994},
  edition = {2nd ed},
  publisher = {Addison-Wesley},
  address = {Reading, Mass},
  isbn = {978-0-201-55802-9},
  lccn = {QA39.2 .G733 1994},
  keywords = {Computer science,Mathematics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VDCCVW6Q/graham_concrete_1994.pdf}
}

@inproceedings{graves_connectionist_2006,
  title = {Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
  shorttitle = {Connectionist Temporal Classification},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning},
  author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2006},
  pages = {369--376},
  publisher = {ACM},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VFNDGZTW/graves_connection_2006.pdf}
}

@inproceedings{graves_hybrid_2013,
  title = {Hybrid Speech Recognition with {{Deep Bidirectional LSTM}}},
  booktitle = {2013 {{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}}},
  author = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
  year = {2013},
  month = dec,
  pages = {273--278},
  publisher = {IEEE},
  address = {Olomouc, Czech Republic},
  doi = {10.1109/ASRU.2013.6707742},
  urldate = {2019-11-22},
  abstract = {Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently been shown to give state-of-the-art performance on the TIMIT speech database. However, the results in that work relied on recurrent-neural-network-specific objective functions, which are difficult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of DBLSTM as an acoustic model in a standard neural network-HMM hybrid system. We find that a DBLSTM-HMM hybrid gives equally good results on TIMIT as the previous work. It also outperforms both GMM and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates.},
  isbn = {978-1-4799-2756-2},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ACT8DVQJ/Graves et al. - 2013 - Hybrid speech recognition with Deep Bidirectional .pdf}
}

@article{greblicki_hammerstein_2017,
  title = {Hammerstein {{System Identification With}} the {{Nearest Neighbor Algorithm}}},
  author = {Greblicki, W. and Pawlak, M.},
  year = {2017},
  month = aug,
  journal = {IEEE Transactions on Information Theory},
  volume = {63},
  number = {8},
  pages = {4746--4757},
  issn = {0018-9448},
  doi = {10.1109/TIT.2017.2694013},
  abstract = {The nonlinear characteristic in a Hammerstein system, i.e., a system in which a nonlinear memoryless subsystem and a linear dynamic are connected in a cascade, is recovered with the nonparametric nearest neighbor regression estimate. The a priori information is nonparametric, both the nonlinear characteristic and the impulse response are completely unknown and can be of any form. Local and global properties of the estimate are examined. Whatever the probability density of the input signal, the estimate converges at every continuity point of the characteristic as well as in the global sense. We derive the asymptotic bias and variance of the proposed estimate. As a result, the optimal rate of convergence is established that additionally is independent of the shape of the input density. Results of numerical simulations are also presented.},
  keywords = {Convergence,dependent data,Estimation,Hammerstein system,Hammerstein system identification,Heuristic algorithms,impulse response,Kernel,linear dynamic,nearest neighbor,nearest neighbor algorithm,nonlinear memoryless subsystem,nonlinear systems,nonparametric nearest neighbor regression estimate,nonparametric regression,numerical analysis,numerical simulations,Random variables,rate of convergence,regression analysis,signal processing,Signal processing algorithms,system identification,transient response},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3MX5N3C4/greblicki_hammerstei_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/EIX57J7W/7902097.html;/Users/antoniohortaribeiro/Zotero/storage/LRNQ4GK6/7902097.html}
}

@book{greene_econometric_2003,
  title = {Econometric Analysis},
  author = {Greene, William H.},
  year = {2003},
  edition = {5th ed},
  publisher = {Prentice Hall},
  address = {Upper Saddle River, N.J},
  isbn = {978-0-13-066189-0},
  lccn = {HB139 .G74 2003},
  keywords = {Econometrics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8TJV9SKG/greene_econometri_2003.pdf}
}

@book{groover_automation_2000,
  title = {Automation, Production Systems, and Computer-Integrated Manufacturing (2nd Edition)},
  author = {Groover, Mikell P.},
  year = {2000},
  edition = {2},
  publisher = {Prentice Hall},
  isbn = {0-13-088978-4 978-0-13-088978-2},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EUGN6NYU/Mikell P. Groover - Automation, Production Systems, and Computer-Integrated Manufacturing (2nd Edition)-Prentice Hall (2000).pdf}
}

@article{grosse_statistical_2017,
  title = {On the ({{Statistical}}) {{Detection}} of {{Adversarial Examples}}},
  author = {Grosse, Kathrin and Manoharan, Praveen and Papernot, Nicolas and Backes, Michael and McDaniel, Patrick},
  year = {2017},
  month = oct,
  journal = {arXiv:1702.06280 [cs, stat]},
  eprint = {1702.06280},
  primaryclass = {cs, stat},
  urldate = {2021-11-16},
  abstract = {Machine Learning (ML) models are applied in a variety of tasks such as network intrusion detection or malware classification. Yet, these models are vulnerable to a class of malicious inputs known as adversarial examples. These are slightly perturbed inputs that are classified incorrectly by the ML model. The mitigation of these adversarial inputs remains an open problem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FX6HGD6L/Grosse et al. - 2017 - On the (Statistical) Detection of Adversarial Exam.pdf}
}

@article{grosse_statistical_2017a,
  title = {On the ({{Statistical}}) {{Detection}} of {{Adversarial Examples}}},
  author = {Grosse, Kathrin and Manoharan, Praveen and Papernot, Nicolas and Backes, Michael and McDaniel, Patrick},
  year = {2017},
  month = oct,
  journal = {arXiv:1702.06280 [cs, stat]},
  eprint = {1702.06280},
  primaryclass = {cs, stat},
  urldate = {2021-11-16},
  abstract = {Machine Learning (ML) models are applied in a variety of tasks such as network intrusion detection or malware classification. Yet, these models are vulnerable to a class of malicious inputs known as adversarial examples. These are slightly perturbed inputs that are classified incorrectly by the ML model. The mitigation of these adversarial inputs remains an open problem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YL9EK6G7/Grosse et al. - 2017 - On the (Statistical) Detection of Adversarial Exam.pdf}
}

@book{grune_nonlinear_2017,
  title = {Nonlinear {{Model Predictive Control}}},
  author = {Gr{\"u}ne, Lars and Pannek, J{\"u}rgen},
  year = {2017},
  series = {Communications and {{Control Engineering}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-46024-6},
  urldate = {2018-04-27},
  isbn = {978-3-319-46023-9 978-3-319-46024-6},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3ZWNAQQX/grÃ¼ne_nonlinear_2017.pdf}
}

@article{guarner_chagas_2019,
  title = {Chagas Disease as Example of a Reemerging Parasite},
  author = {Guarner, Jeannette},
  year = {2019},
  month = may,
  journal = {Seminars in Diagnostic Pathology},
  series = {Emerging Infections Issue},
  volume = {36},
  number = {3},
  pages = {164--169},
  issn = {0740-2570},
  doi = {10.1053/j.semdp.2019.04.008},
  urldate = {2021-11-25},
  abstract = {Trypanosoma cruzi, the protozoan that causes Chagas disease, is primarily transmitted by three main Triatomine vectors in endemic areas. However, the infection has become a potential emerging disease because the vector is found in non-endemic areas, there is migration of infected asymptomatic people that can infect the vector, become blood donors, or pass the disease vertically (congenital infections). Lastly, the disease can be acquired through contaminated food (oral transmission). This review will present the different transmission pathways, clinical manifestations, diagnostic modalities and treatment considerations of Chagas disease.},
  langid = {english},
  keywords = {Chagas disease,Trypanosoma cruzi}
}

@misc{guimaraes_super_,
  title = {{Super Partituras - Partitura da m{\'u}sica Forr{\'o} no Escuro v.2 (Luiz Gonzaga).}},
  author = {Guimaraes, Jonatas},
  journal = {SuperPartituras},
  urldate = {2023-11-28},
  abstract = {P{\'a}gina que cont{\'e}m a partitura da m{\'u}sica Forr{\'o} no Escuro v.2 (Luiz Gonzaga).},
  howpublished = {https://www.superpartituras.com.br/luiz-gonzaga/forro-no-escuro-v-2},
  langid = {brazilian},
  file = {/Users/antoniohortaribeiro/Zotero/storage/98DHSJR5/forro-no-escuro-v-2.html}
}

@article{gunasekar_implicit_2017,
  title = {Implicit {{Regularization}} in {{Matrix Factorization}}},
  author = {Gunasekar, Suriya and Woodworth, Blake and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  year = {2017},
  month = may,
  journal = {arXiv:1705.09280 [cs, stat]},
  eprint = {1705.09280},
  primaryclass = {cs, stat},
  urldate = {2020-08-09},
  abstract = {We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix \$X\$ with gradient descent on a factorization of \$X\$. We conjecture and provide empirical and theoretical evidence that with small enough step sizes and initialization close enough to the origin, gradient descent on a full dimensional factorization converges to the minimum nuclear norm solution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CX328X76/Gunasekar et al. - 2017 - Implicit Regularization in Matrix Factorization.pdf;/Users/antoniohortaribeiro/Zotero/storage/PBGG3YMA/1705.html}
}

@article{guo_calibration_2017,
  title = {On {{Calibration}} of {{Modern Neural Networks}}},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  year = {2017},
  month = aug,
  journal = {arXiv:1706.04599 [cs]},
  eprint = {1706.04599},
  primaryclass = {cs},
  urldate = {2020-11-10},
  abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a singleparameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KZWSN68G/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf}
}

@inproceedings{guo_sparse_2018,
  title = {Sparse {{DNNs}} with {{Improved Adversarial Robustness}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Guo, Yiwen and Zhang, Chao and Zhang, Changshui and Chen, Yurong},
  year = {2018},
  volume = {31},
  urldate = {2022-02-03},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FN2959Z6/Guo et al. - 2018 - Sparse DNNs with Improved Adversarial Robustness.pdf}
}

@article{gupta_feedback_2018,
  title = {Feedback {{GAN}} ({{FBGAN}}) for {{DNA}}: A {{Novel Feedback-Loop Architecture}} for {{Optimizing Protein Functions}}},
  shorttitle = {Feedback {{GAN}} ({{FBGAN}}) for {{DNA}}},
  author = {Gupta, Anvita and Zou, James},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.01694 [cs, q-bio]},
  eprint = {1804.01694},
  primaryclass = {cs, q-bio},
  urldate = {2019-04-01},
  abstract = {Generative Adversarial Networks (GANs) represent an attractive and novel approach to generate realistic data, such as genes, proteins, or drugs, in synthetic biology. Here, we apply GANs to generate synthetic DNA sequences encoding for proteins of variable length. We propose a novel feedback-loop architecture, called Feedback GAN (FBGAN), to optimize the synthetic gene sequences for desired properties using an external function analyzer. The proposed architecture also has the advantage that the analyzer need not be differentiable. We apply the feedback-loop mechanism to two examples: 1) generating synthetic genes coding for antimicrobial peptides, and 2) optimizing synthetic genes for the secondary structure of their resulting peptides. A suite of metrics demonstrate that the GAN generated proteins have desirable biophysical properties. The FBGAN architecture can also be used to optimize GAN-generated datapoints for useful properties in domains beyond genomics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Genomics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XFFVSEDT/gupta_feedback_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/Y28X6RSF/1804.html}
}

@article{gupta_note_2012,
  title = {A Note on the Asymptotic Distribution of {{LASSO}} Estimator for Correlated Data},
  author = {Gupta, Shuva},
  year = {2012},
  journal = {Sankhya A},
  volume = {74},
  number = {1},
  pages = {10--28},
  issn = {0976-836X},
  doi = {10.1007/s13171-012-0006-8},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GZH4XWN9/gupta_a note on_2012.pdf}
}

@misc{gurbuzbalaban_heavytail_2021,
  title = {The {{Heavy-Tail Phenomenon}} in {{SGD}}},
  author = {Gurbuzbalaban, Mert and {\c S}im{\c s}ekli, Umut and Zhu, Lingjiong},
  year = {2021},
  month = jun,
  number = {arXiv:2006.04740},
  eprint = {2006.04740},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.04740},
  urldate = {2023-11-06},
  abstract = {In recent years, various notions of capacity and complexity have been proposed for characterizing the generalization properties of stochastic gradient descent (SGD) in deep learning. Some of the popular notions that correlate well with the performance on unseen data are (i) the `flatness' of the local minimum found by SGD, which is related to the eigenvalues of the Hessian, (ii) the ratio of the stepsize \${\textbackslash}eta\$ to the batch-size \$b\$, which essentially controls the magnitude of the stochastic gradient noise, and (iii) the `tail-index', which measures the heaviness of the tails of the network weights at convergence. In this paper, we argue that these three seemingly unrelated perspectives for generalization are deeply linked to each other. We claim that depending on the structure of the Hessian of the loss at the minimum, and the choices of the algorithm parameters \${\textbackslash}eta\$ and \$b\$, the SGD iterates will converge to a {\textbackslash}emph\{heavy-tailed\} stationary distribution. We rigorously prove this claim in the setting of quadratic optimization: we show that even in a simple linear regression problem with independent and identically distributed data whose distribution has finite moments of all order, the iterates can be heavy-tailed with infinite variance. We further characterize the behavior of the tails with respect to algorithm parameters, the dimension, and the curvature. We then translate our results into insights about the behavior of SGD in deep learning. We support our theory with experiments conducted on synthetic data, fully connected, and convolutional neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Statistics Theory},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MJFSSUVP/Gurbuzbalaban et al. - 2021 - The Heavy-Tail Phenomenon in SGD.pdf;/Users/antoniohortaribeiro/Zotero/storage/WJUPI3TG/2006.html}
}

@article{gurushewal_robust_2012,
  title = {Robust {{Stability}} of {{LTI}} ({{SISO}}) {{System}} with {{System Gain}} ({{A}}) under Uncertainty Set Using {{PID}} Controller},
  author = {Gurushewal, Singh},
  year = {2012},
  journal = {International Journal of Advances in Computing and Information Technology},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3DI2UV85/gurushewal_robust_2012.pdf}
}

@article{gustafsson_development_2022,
  title = {Development and Validation of Deep Learning {{ECG-based}} Prediction of Myocardial Infarction in Emergency Department Patients},
  author = {Gustafsson, Stefan and Gedon, Daniel and Lampa, Erik and Ribeiro, Ant{\^o}nio H. and Holzmann, Martin J. and Sch{\"o}n, Thomas B. and Sundstr{\"o}m, Johan},
  year = {2022},
  month = nov,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {19615},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-24254-x},
  urldate = {2022-11-17},
  abstract = {Myocardial infarction diagnosis is a common challenge in the emergency department. In managed settings, deep learning-based models and especially convolutional deep models have shown promise in electrocardiogram (ECG) classification, but there is a lack of high-performing models for the diagnosis of myocardial infarction in real-world scenarios. We aimed to train and validate a deep learning model using ECGs to predict myocardial infarction in real-world emergency department patients. We studied emergency department patients in the Stockholm region between 2007 and 2016 that had an ECG obtained because of their presenting complaint. We developed a deep neural network based on convolutional layers similar to a residual network. Inputs to the model were ECG tracing, age, and sex; and outputs were the probabilities of three mutually exclusive classes: non-ST-elevation myocardial infarction (NSTEMI), ST-elevation myocardial infarction (STEMI), and control status, as registered in the SWEDEHEART and other registries. We used an ensemble of five models. Among 492,226 ECGs in 214,250 patients, 5,416 were recorded with an NSTEMI, 1,818 a STEMI, and 485,207 without a myocardial infarction. In a random test set, our model could discriminate STEMIs/NSTEMIs from controls with a C-statistic of 0.991/0.832 and had a Brier score of 0.001/0.008. The model obtained a similar performance in a temporally separated test set of the study sample, and achieved a C-statistic of 0.985 and a Brier score of 0.002 in discriminating STEMIs from controls in an external test set. We developed and validated a deep learning model with excellent performance in discriminating between control, STEMI, and NSTEMI on the presenting ECG of a real-world sample of the important population of all-comers to the emergency department. Hence, deep learning models for ECG decision support could be valuable in the emergency department.},
  copyright = {2022 The Author(s)},
  keywords = {Cardiology,Cardiovascular diseases,Diseases,Health care,Machine learning,Medical research},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3NG3RQFE/Gustafsson et al. - 2022 - Development and validation of deep learning ECG-ba.pdf;/Users/antoniohortaribeiro/Zotero/storage/B86Z5P3V/s41598-022-24254-x.html}
}

@article{guzman_use_2017,
  title = {The {{Use}} of {{NARX Neural Networks}} to {{Forecast Daily Groundwater Levels}}},
  author = {Guzman, Sandra M and Paz, Joel O and Tagert, Mary Love M},
  year = {2017},
  journal = {Water Resources Management},
  volume = {31},
  number = {5},
  pages = {1591--1603},
  doi = {10.1007/s11269-017-1598-5}
}

@article{guzman_use_2017a,
  title = {The {{Use}} of {{NARX Neural Networks}} to {{Forecast Daily Groundwater Levels}}},
  author = {Guzman, Sandra M and Paz, Joel O and Tagert, Mary Love M},
  year = {2017},
  journal = {Water Resources Management},
  volume = {31},
  number = {5},
  pages = {1591--1603},
  doi = {10/f92hfc}
}

@inproceedings{ha_model_2015,
  title = {Model Order Selection for Continuous Time Instrumental Variable Methods Using Regularization},
  booktitle = {2015 54th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Ha, H. and Welsh, J. S.},
  year = {2015},
  month = dec,
  pages = {771--776},
  doi = {10.1109/CDC.2015.7402323},
  abstract = {The aim of this paper is to propose a new method to select the model order in continuous time system identification, instrumental variable methods. The idea is to over-parameterize the model and utilize regularization based on the l1 norm to obtain a sparse estimate. The model order of the identified system is then determined by the rank of the Hankel matrix of the estimated parameter. Simulation results show that the proposed method works very effectively. For low signal to noise ratio (SNR), it offers a significant improvement to existing model order selection methods with the performance at high SNR comparable to the existing methods.},
  keywords = {Computational modeling,Continuous time identification,continuous time system identification,continuous time systems,Data models,Estimation,Hankel matrices,Hankel matrix,identification,instrumental variable methods,Instruments,l1 norm,Mathematical model,model order selection methods,order selection,Regularization,signal to noise ratio,SNR,sparse estimation,Transfer functions,Yttrium},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q2TNH4WX/ha_model_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/7J4TJEZZ/7402323.html}
}

@article{haagerup_new_2005,
  title = {A New Application of Random Matrices: {{Ext}}({{C}} {\textsubscript{red}} {\textsuperscript{{$\ast$}}} ({{F}} {\textsubscript{2}} )) Is Not a Group},
  shorttitle = {A New Application of Random Matrices},
  author = {Haagerup, Uffe and Thorbj{\o}rnsen, Steen},
  year = {2005},
  month = sep,
  journal = {Annals of Mathematics},
  volume = {162},
  number = {2},
  pages = {711--775},
  issn = {0003-486X},
  doi = {10.4007/annals.2005.162.711},
  urldate = {2020-12-21},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/I3TSFA9K/Haagerup and ThorbjÃ¸rnsen - 2005 - A new application of random matrices Ext(C r.pdf}
}

@article{habineza_endtoend_2023,
  title = {End-to-End {{Risk Prediction}} of {{Atrial Fibrillation}} from the 12-{{Lead ECG}} by {{Deep Neural Networks}}},
  author = {Habineza, Theogene and Ribeiro, Ant{\^o}nio H. and Gedon, Daniel and Behar, Joachim A. and Ribeiro, Antonio Luiz P. and Sch{\"o}n, Thomas B.},
  year = {2023},
  journal = {Journal of Electrocardiology},
  doi = {10.1016/j.jelectrocard.2023.09.011},
  copyright = {All rights reserved}
}

@article{hagan_training_1994,
  title = {Training Feedforward Networks with the {{Marquardt}} Algorithm},
  author = {Hagan, Martin T and Menhaj, Mohammad B},
  year = {1994},
  journal = {Neural Networks, IEEE Transactions on},
  volume = {5},
  number = {6},
  pages = {989--993},
  doi = {10.1109/72.329697},
  keywords = {Acceleration,Approximation algorithms,backpropagation,Backpropagation algorithms,Convergence,feedforward network training,feedforward neural nets,feedforward neural networks,Function approximation,learning,Least squares approximation,least squares approximations,Least squares methods,Marquardt algorithm,Neural networks,nonlinear least squares,Testing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KJ2262NZ/hagan_training_1994.pdf;/Users/antoniohortaribeiro/Zotero/storage/SDNB9RJW/329697.html;/Users/antoniohortaribeiro/Zotero/storage/XVBGJ4SJ/329697.html}
}

@article{hager_new_2005,
  title = {A New Conjugate Gradient Method with Guaranteed Descent and an Efficient Line Search},
  author = {Hager, William W and Zhang, Hongchao},
  year = {2005},
  journal = {SIAM Journal on Optimization},
  volume = {16},
  number = {1},
  pages = {170--192},
  doi = {10.1137/030601880}
}

@article{han_deep_2020,
  title = {Deep Learning Models for Electrocardiograms Are Susceptible to Adversarial Attack},
  author = {Han, Xintian and Hu, Yuxuan and Foschini, Luca and Chinitz, Larry and Jankelson, Lior and Ranganath, Rajesh},
  year = {2020},
  month = mar,
  journal = {Nature Medicine},
  volume = {26},
  number = {3},
  pages = {360--363},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-020-0791-x},
  urldate = {2020-10-28},
  abstract = {Electrocardiogram (ECG) acquisition is increasingly widespread in medical and commercial devices, necessitating the development of automated interpretation strategies. Recently, deep neural networks have been used to automatically analyze ECG tracings and outperform physicians in detecting certain rhythm irregularities1. However, deep learning classifiers are susceptible to adversarial examples, which are created from raw data to fool the classifier such that it assigns the example to the wrong class, but which are undetectable to the human eye2,3. Adversarial examples have also been created for medical-related tasks4,5. However, traditional attack methods to create adversarial examples do not extend directly to ECG signals, as such methods introduce square-wave artefacts that are not physiologically plausible. Here we develop a method to construct smoothed adversarial examples for ECG tracings that are invisible to human expert evaluation and show that a deep learning model for arrhythmia detection from single-lead ECG6 is vulnerable to this type of attack. Moreover, we provide a general technique for collating and perturbing known adversarial examples to create multiple new ones. The susceptibility of deep learning ECG algorithms to adversarial misclassification implies that care should be taken when evaluating these models on ECGs that may have been altered, particularly when incentives for causing misclassification exist.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/L3P8S94I/s41591-020-0791-x.html}
}

@article{hanin_which_2018,
  title = {Which {{Neural Net Architectures Give Rise To Exploding}} and {{Vanishing Gradients}}?},
  author = {Hanin, Boris},
  year = {2018},
  month = jan,
  urldate = {2018-12-10},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/876VSZ7Z/hanin_which_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/VFNIVJZ9/1801.html}
}

@article{hannun_cardiologistlevel_2019,
  title = {Cardiologist-Level Arrhythmia Detection and Classification in Ambulatory Electrocardiograms Using a Deep Neural Network},
  author = {Hannun, Awni Y. and Rajpurkar, Pranav and Haghpanahi, Masoumeh and Tison, Geoffrey H. and Bourn, Codie and Turakhia, Mintu P. and Ng, Andrew Y.},
  year = {2019},
  month = jan,
  journal = {Nature Medicine},
  volume = {25},
  number = {1},
  pages = {65--69},
  issn = {1546-170X},
  doi = {10/gftc8p},
  abstract = {Computerized electrocardiogram (ECG) interpretation plays a critical role in the clinical ECG workflow1. Widely available digital ECG data and the algorithmic paradigm of deep learning2 present an opportunity to substantially improve the accuracy and scalability of automated ECG analysis. However, a comprehensive evaluation of an end-to-end deep learning approach for ECG analysis across a wide variety of diagnostic classes has not been previously reported. Here, we develop a deep neural network (DNN) to classify 12 rhythm classes using 91,232 single-lead ECGs from 53,549 patients who used a single-lead ambulatory ECG monitoring device. When validated against an independent test dataset annotated by a consensus committee of board-certified practicing cardiologists, the DNN achieved an average area under the receiver operating characteristic curve (ROC) of 0.97. The average F1 score, which is the harmonic mean of the positive predictive value and sensitivity, for the DNN (0.837) exceeded that of average cardiologists (0.780). With specificity fixed at the average specificity achieved by cardiologists, the sensitivity of the DNN exceeded the average cardiologist sensitivity for all rhythm classes. These findings demonstrate that an end-to-end deep learning approach can classify a broad range of distinct arrhythmias from single-lead ECGs with high diagnostic performance similar to that of cardiologists. If confirmed in clinical settings, this approach could reduce the rate of misdiagnosed computerized ECG interpretations and improve the efficiency of expert human ECG interpretation by accurately triaging or prioritizing the most urgent conditions.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EUSFMGEU/hannun_cardiologi_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/WYCELFFB/Hannun et al. - 2019 - Cardiologist-level arrhythmia detection and classi.pdf}
}

@article{hara_can_2017,
  title = {Can {{Spatiotemporal 3D CNNs Retrace}} the {{History}} of {{2D CNNs}} and {{ImageNet}}?},
  author = {Hara, Kensho and Kataoka, Hirokatsu and Satoh, Yutaka},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.09577 [cs]},
  eprint = {1711.09577},
  primaryclass = {cs},
  abstract = {The purpose of this study is to determine whether current video datasets have sufficient data for training very deep convolutional neural networks (CNNs) with spatio-temporal three-dimensional (3D) kernels. Recently, the performance levels of 3D CNNs in the field of action recognition have improved significantly. However, to date, conventional research has only explored relatively shallow 3D architectures. We examine the architectures of various 3D CNNs from relatively shallow to very deep ones on current video datasets. Based on the results of those experiments, the following conclusions could be obtained: (i) ResNet-18 training resulted in significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics. (ii) The Kinetics dataset has sufficient data for training of deep 3D CNNs, and enables training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet. ResNeXt-101 achieved 78.4\% average accuracy on the Kinetics test set. (iii) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures, and the pretrained ResNeXt-101 achieved 94.5\% and 70.2\% on UCF-101 and HMDB-51, respectively. The use of 2D CNNs trained on ImageNet has produced significant progress in various tasks in image. We believe that using deep 3D CNNs together with Kinetics will retrace the successful history of 2D CNNs and ImageNet, and stimulate advances in computer vision for videos. The codes and pretrained models used in this study are publicly available. https://github.com/kenshohara/3D-ResNets-PyTorch},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TP96H8TP/hara_can_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/93DB4XH3/1711.html}
}

@article{hardt_identity_2018,
  title = {Identity {{Matters}} in {{Deep Learning}}},
  author = {Hardt, Moritz and Ma, Tengyu},
  year = {2018},
  month = jul,
  journal = {arXiv:1611.04231 [cs, stat]},
  eprint = {1611.04231},
  primaryclass = {cs, stat},
  urldate = {2020-08-27},
  abstract = {An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as batch normalization, but was also key to the immense success of residual networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UTLMMDHX/Hardt and Ma - 2018 - Identity Matters in Deep Learning.pdf}
}

@inproceedings{hassan_statistical_2015,
  title = {Statistical Design Centering of {{RF}} Cavity Linear Accelerator via Non-Derivative Trust Region Optimization},
  booktitle = {Numerical {{Electromagnetic}} and {{Multiphysics Modeling}} and {{Optimization}} ({{NEMO}}), 2015 {{IEEE MTT-S International Conference}} On},
  author = {Hassan, Abdel-Karim SO and {Abdel-Malek}, Hany L and Mohamed, Ahmed SA and Abuelfadl, Tamer M and Elqenawy, Ahmed E},
  year = {2015},
  pages = {1--3},
  publisher = {IEEE}
}

@article{hassani_curse_2022,
  title = {The Curse of Overparametrization in Adversarial Training: {{Precise}} Analysis of Robust Generalization for Random Features Regression},
  shorttitle = {The Curse of Overparametrization in Adversarial Training},
  author = {Hassani, Hamed and Javanmard, Adel},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.05149},
  eprint = {2201.05149},
  urldate = {2022-02-25},
  abstract = {Successful deep learning models often involve training neural network architectures that contain more parameters than the number of training samples. Such overparametrized models have been extensively studied in recent years, and the virtues of overparametrization have been established from both the statistical perspective, via the double-descent phenomenon, and the computational perspective via the structural properties of the optimization landscape. Despite the remarkable success of deep learning architectures in the overparametrized regime, it is also well known that these models are highly vulnerable to small adversarial perturbations in their inputs. Even when adversarially trained, their performance on perturbed inputs (robust generalization) is considerably worse than their best attainable performance on benign inputs (standard generalization). It is thus imperative to understand how overparametrization fundamentally affects robustness. In this paper, we will provide a precise characterization of the role of overparametrization on robustness by focusing on random features regression models (two-layer neural networks with random first layer weights). We consider a regime where the sample size, the input dimension and the number of parameters grow in proportion to each other, and derive an asymptotically exact formula for the robust generalization error when the model is adversarially trained. Our developed theory reveals the nontrivial effect of overparametrization on robustness and indicates that for adversarially trained random features models, high overparametrization can hurt robust generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5LAT958M/Hassani_Javanmard_2022_The curse of overparametrization in adversarial training.pdf;/Users/antoniohortaribeiro/Zotero/storage/M6WE74LT/2201.html}
}

@article{hassani_survey_2014,
  title = {A Survey on Hysteresis Modeling, Identification and Control},
  author = {Hassani, Vahid and Tjahjowidodo, Tegoeh and Do, Thanh Nho},
  year = {2014},
  month = dec,
  journal = {Mechanical Systems and Signal Processing},
  volume = {49},
  number = {1},
  pages = {209--233},
  issn = {0888-3270},
  doi = {10.1016/j.ymssp.2014.04.012},
  keywords = {Hysteresis characterization,Hysteresis control,Hysteresis nonlinearity,Parameter estimation,Smart materials,system identification}
}

@inproceedings{hassibi_optimal_1993,
  title = {Optimal Brain Surgeon and General Network Pruning},
  booktitle = {Neural {{Networks}}, 1993., {{IEEE International Conference}} On},
  author = {Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  year = {1993},
  pages = {293--299},
  publisher = {IEEE}
}

@book{hastie_elements_2009,
  title = {Elements of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  edition = {Second},
  publisher = {Springer Science \& Business Media}
}

@article{hastie_forward_2007,
  title = {Forward Stagewise Regression and the Monotone Lasso},
  author = {Hastie, Trevor and Taylor, Jonathan and Tibshirani, Robert and Walther, Guenther},
  year = {2007},
  journal = {Electronic Journal of Statistics},
  volume = {1},
  number = {0},
  pages = {1--29},
  issn = {1935-7524},
  doi = {10.1214/07-EJS004},
  urldate = {2017-09-12},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AZTV52ZG/hastie_forward_2007.pdf}
}

@book{hastie_statistical_2015,
  title = {Statistical Learning with Sparsity: The Lasso and Generalizations},
  shorttitle = {Statistical Learning with Sparsity},
  author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
  year = {2015},
  publisher = {CRC press},
  urldate = {2017-09-20},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q3WBV9HK/hastie_statistica_2015.pdf}
}

@article{hastie_surprises_2019,
  title = {Surprises in {{High-Dimensional Ridgeless Least Squares Interpolation}}},
  author = {Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J.},
  year = {2019},
  month = nov,
  journal = {arXiv:1903.08560},
  eprint = {1903.08560},
  urldate = {2020-07-23},
  abstract = {Interpolators---estimators that achieve zero training error---have attracted growing attention in machine learning, mainly because state-of-the art neural networks appear to be models of this type. In this paper, we study minimum \${\textbackslash}ell\_2\$ norm ("ridgeless") interpolation in high-dimensional least squares regression. We consider two different models for the feature distribution: a linear model, where the feature vectors \$x\_i {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}p\$ are obtained by applying a linear transform to a vector of i.i.d. entries, \$x\_i = {\textbackslash}Sigma{\textasciicircum}\{1/2\} z\_i\$ (with \$z\_i {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}p\$); and a nonlinear model, where the feature vectors are obtained by passing the input through a random one-layer neural network, \$x\_i = {\textbackslash}varphi(W z\_i)\$ (with \$z\_i {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}d\$, \$W {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{p {\textbackslash}times d\}\$ a matrix of i.i.d. entries, and \${\textbackslash}varphi\$ an activation function acting componentwise on \$W z\_i\$). We recover---in a precise quantitative way---several phenomena that have been observed in large-scale neural networks and kernel machines, including the "double descent" behavior of the prediction risk, and the potential benefits of overparametrization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q2C4CTGJ/1903.html}
}

@article{hastie_surprises_2022,
  title = {Surprises in High-Dimensional Ridgeless Least Squares Interpolation},
  author = {Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J.},
  year = {2022},
  journal = {The Annals of Statistics},
  volume = {50},
  number = {2},
  pages = {949--986},
  publisher = {Institute of Mathematical Statistics},
  doi = {10.1214/21-AOS2133},
  keywords = {interpolation,overparametrization,Random matrix theory,regression,Ridge regression},
  file = {/Users/antoniohortaribeiro/Zotero/storage/E9S236HB/Hastie et al. - 2019 - Surprises in High-Dimensional Ridgeless Least Squa.pdf}
}

@article{he_control_2017,
  title = {Control {{Design}} for {{Nonlinear Flexible Wings}} of a {{Robotic Aircraft}}},
  author = {He, Wei and Zhang, Shuang},
  year = {2017},
  month = jan,
  journal = {IEEE Transactions on Control Systems Technology},
  volume = {25},
  number = {1},
  pages = {351--357},
  issn = {2374-0159},
  doi = {10.1109/TCST.2016.2536708},
  abstract = {In this brief, the control problem for flexible wings of a robotic aircraft is addressed by using boundary control schemes. Inspired by birds and bats, the wing with flexibility and articulation is modeled as a distributed parameter system described by hybrid partial differential equations and ordinary differential equations. Boundary control for both wing twist and bending is proposed on the original coupled dynamics, and bounded stability is proved by introducing a proper Lyapunov function. The effectiveness of the proposed control is verified by simulations.},
  keywords = {aerospace components,Aerospace control,aerospace simulation,Aircraft,articulation,autonomous aerial vehicles,bats,bending,birds,Boundary conditions,Boundary control,boundary control schemes,bounded stability,control design,Control design,control system synthesis,distributed parameter system,distributed parameter systems,Distributed parameter systems,flexible wings,hybrid partial differential equations,Lyapunov function,Lyapunov methods,Mathematical model,microaerial vehicle (MAV),nonlinear flexible wings,nonlinear systems,ordinary differential equations,original coupled dynamics,partial differential equations,robot,robotic aircraft,Robots,simulations,stability,vehicle dynamics,vibration control,wing twist},
  file = {/Users/antoniohortaribeiro/Zotero/storage/D54UX379/7476820.html}
}

@inproceedings{he_deep_2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  eprint = {1512.03385},
  pages = {770--778},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/antoniohortaribeiro/Zotero/storage/C25IGGDK/he_deep_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/7GS859V7/1512.html}
}

@inproceedings{he_delving_2015,
  title = {Delving Deep into Rectifiers: {{Surpassing}} Human-Level Performance on Imagenet Classification},
  shorttitle = {Delving Deep into Rectifiers},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  pages = {1026--1034},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WZVXZWK9/he_delving_2015.pdf}
}

@article{he_delving_2015a,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = feb,
  journal = {arXiv:1502.01852 [cs]},
  eprint = {1502.01852},
  primaryclass = {cs},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZV7IZH72/he_delving_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/3FPUWBR6/1502.html}
}

@inproceedings{he_identity_2016,
  title = {Identity {{Mappings}} in {{Deep Residual Networks}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2016},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  pages = {630--645},
  publisher = {Springer International Publishing},
  abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62~\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.},
  isbn = {978-3-319-46493-0},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/936FJX82/he_identity_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/Z55P6K9Z/1603.html}
}

@article{he_model_2017,
  title = {Model Identification and Control Design for a Humanoid Robot},
  author = {He, Wei and Ge, Weiliang and Li, Yunchuan and Liu, Yan-Jun and Yang, Chenguang and Sun, Changyin},
  year = {2017},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {47},
  number = {1},
  pages = {45--57},
  doi = {10.1109/TSMC.2016.2557227}
}

@article{he_model_2017a,
  title = {Model {{Identification}} and {{Control Design}} for a {{Humanoid Robot}}},
  author = {He, Wei and Ge, Weiliang and Li, Yunchuan and Liu, Yan-Jun and Yang, Chenguang and Sun, Changyin},
  year = {2017},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {47},
  number = {1},
  pages = {45--57},
  doi = {10/f9kc4j},
  annotation = {00054}
}

@inproceedings{he_new_1993,
  title = {A New Method for Identifying Orders of Input-Output Models for Nonlinear Dynamic Systems},
  booktitle = {American {{Control Conference}}, 1993},
  author = {He, Xiangdong and Asada, Haruhiko},
  year = {1993},
  pages = {2520--2523},
  publisher = {IEEE}
}

@article{he_twosided_2016,
  title = {Two-Sided Coupled Generalized {{Sylvester}} Matrix Equations Solving Using a Simultaneous Decomposition for Fifteen Matrices},
  author = {He, Zhuo-Heng and Agudelo, Oscar Mauricio and Wang, Qing-Wen and De Moor, Bart},
  year = {2016},
  month = may,
  journal = {Linear Algebra and its Applications},
  volume = {496},
  pages = {549--593},
  issn = {0024-3795},
  doi = {10.1016/j.laa.2016.02.013},
  abstract = {In this paper, we investigate and analyze in detail the structure and properties of a simultaneous decomposition for fifteen matrices: Ai{$\in$}Cpi{\texttimes}ti, Bi{$\in$}Csi{\texttimes}qi, Ci{$\in$}Cpi{\texttimes}ti+1, Di{$\in$}Csi+1{\texttimes}qi, and Ei{$\in$}Cpi{\texttimes}qi (i=1,2,3). We show that from this simultaneous decomposition we can derive some necessary and sufficient conditions for the existence of a solution to the system of two-sided coupled generalized Sylvester matrix equations with four unknowns AiXiBi+CiXi+1Di=Ei (i=1,2,3). Apart from proving an expression for the general solutions to this system, we derive the range of ranks of these solutions using the ranks of the given matrices Ai, Bi, Ci, Di, and Ei. We provide some numerical examples to illustrate our results. Moreover, we present a similar approach to consider the simultaneous decomposition for 5k matrices and the system of k two-sided coupled generalized Sylvester matrix equations with k+1 unknowns AiXiBi+CiXi+1Di=Ei (i=1,{\dots},k, k{$\geq$}4). The main results are also valid over the real number field and the real quaternion algebra.},
  keywords = {General solution,Generalized Sylvester matrix equations,Matrix decompositions,Rank,Solvability},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5XRQT2UD/he_two-sided_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/2RLDS5E8/S0024379516001142.html;/Users/antoniohortaribeiro/Zotero/storage/Q5JWG6G9/S0024379516001142.html}
}

@inproceedings{heikkila_fourstep_1997,
  title = {A Four-Step Camera Calibration Procedure with Implicit Image Correction},
  booktitle = {Computer {{Vision}} and {{Pattern Recognition}}, 1997. {{Proceedings}}., 1997 {{IEEE Computer Society Conference}} On},
  author = {Heikkila, Janne and Silv{\'e}n, Olli},
  year = {1997},
  pages = {1106--1112},
  publisher = {IEEE}
}

@inproceedings{helfrich_orthogonal_2018,
  title = {Orthogonal Recurrent Neural Networks with Scaled {{Cayley}} Transform},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Helfrich, Kyle and Willmott, Devin and Ye, Qiang},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  series = {Proceedings of Machine Learning Research},
  volume = {80},
  pages = {1969--1978},
  publisher = {PMLR},
  address = {Stockholmsm{\"a}ssan, Stockholm Sweden},
  abstract = {Recurrent Neural Networks (RNNs) are designed to handle sequential data but suffer from vanishing or exploding gradients. Recent work on Unitary Recurrent Neural Networks (uRNNs) have been used to address this issue and in some cases, exceed the capabilities of Long Short-Term Memory networks (LSTMs). We propose a simpler and novel update scheme to maintain orthogonal recurrent weight matrices without using complex valued matrices. This is done by parametrizing with a skew-symmetric matrix using the Cayley transform; such a parametrization is unable to represent matrices with negative one eigenvalues, but this limitation is overcome by scaling the recurrent weight matrix by a diagonal matrix consisting of ones and negative ones. The proposed training scheme involves a straightforward gradient calculation and update step. In several experiments, the proposed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves superior results with fewer trainable parameters than other unitary RNNs.},
  pdf = {http://proceedings.mlr.press/v80/helfrich18a/helfrich18a.pdf}
}

@inproceedings{henaff_geodesics_2016,
  title = {Geodesics of Learned Representations},
  booktitle = {Proceedings of the {{International Conference}} for {{Learning Representations}} ({{ICLR}})},
  author = {H{\'e}naff, Olivier J. and Simoncelli, Eero P.},
  year = {2016},
  month = feb,
  eprint = {1511.06394},
  urldate = {2020-03-23},
  abstract = {We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a "representational geodesic"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DFEQ52YX/HÃ©naff and Simoncelli - 2016 - Geodesics of learned representations.pdf;/Users/antoniohortaribeiro/Zotero/storage/YGHGQQ7Z/1511.html}
}

@article{hendriks_deep_2021,
  title = {Deep {{Energy-Based NARX Models}}},
  author = {Hendriks, Johannes N. and Gustafsson, Fredrik K. and Ribeiro, Ant{\^o}nio H. and Wills, Adrian G. and Sch{\"o}n, Thomas B.},
  year = {2021},
  journal = {IFAC Symposium on System Identification (SYSID)},
  volume = {54},
  number = {7},
  eprint = {2012.04136},
  pages = {505--510},
  doi = {10.1016/j.ifacol.2021.08.410},
  abstract = {This paper is directed towards the problem of learning nonlinear ARX models based on system input--output data. In particular, our interest is in learning a conditional distribution of the current output based on a finite window of past inputs and outputs. To achieve this, we consider the use of so-called energy-based models, which have been developed in allied fields for learning unknown distributions based on data. This energy-based model relies on a general function to describe the distribution, and here we consider a deep neural network for this purpose. The primary benefit of this approach is that it is capable of learning both simple and highly complex noise models, which we demonstrate on simulated and experimental data.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NC6RLIHX/Hendriks et al. - 2020 - Deep Energy-Based NARX Models.pdf;/Users/antoniohortaribeiro/Zotero/storage/VPJ9CCN9/2012.html}
}

@article{hendriks_deep_2021a,
  title = {Deep {{Energy-Based NARX Models}}},
  author = {Hendriks, Johannes N. and Gustafsson, Fredrik K. and Ribeiro, Ant{\^o}nio H. and Wills, Adrian G. and Sch{\"o}n, Thomas B.},
  year = {2021},
  journal = {Workshop on Nonlinear System Identification},
  abstract = {This paper is directed towards the problem of learning nonlinear ARX models based on system input--output data. In particular, our interest is in learning a conditional distribution of the current output based on a finite window of past inputs and outputs. To achieve this, we consider the use of so-called energy-based models, which have been developed in allied fields for learning unknown distributions based on data. This energy-based model relies on a general function to describe the distribution, and here we consider a deep neural network for this purpose. The primary benefit of this approach is that it is capable of learning both simple and highly complex noise models, which we demonstrate on simulated and experimental data.},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/antoniohortaribeiro/Zotero/storage/R56K9SKH/Hendriks et al. - 2020 - Deep Energy-Based NARX Models.pdf;/Users/antoniohortaribeiro/Zotero/storage/X2TGVS6G/2012.html}
}

@article{hendrycks_benchmarking_2019,
  title = {Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
  author = {Hendrycks, Dan and Dietterich, Thomas},
  year = {2019},
  journal = {Proceedings of the International Conference on Learning Representations}
}

@misc{hendrycks_unsolved_2022,
  title = {Unsolved {{Problems}} in {{ML Safety}}},
  author = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
  year = {2022},
  month = jun,
  number = {arXiv:2109.13916},
  eprint = {2109.13916},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.13916},
  urldate = {2022-10-24},
  abstract = {Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards ("Robustness"), identifying hazards ("Monitoring"), reducing inherent model hazards ("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout, we clarify each problem's motivation and provide concrete research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KSHKY3KM/Hendrycks et al. - 2022 - Unsolved Problems in ML Safety.pdf;/Users/antoniohortaribeiro/Zotero/storage/3AG4QWU9/2109.html}
}

@article{hendrycks_using_2019,
  title = {Using {{Pre-Training Can Improve Model Robustness}} and {{Uncertainty}}},
  author = {Hendrycks, Dan and Lee, Kimin and Mazeika, Mantas},
  year = {2019},
  eprint = {1901.09960},
  urldate = {2020-03-24},
  abstract = {He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on adversarial examples, label corruption, class imbalance, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We introduce adversarial pre-training and show approximately a 10\% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DX7QMEYL/Hendrycks et al. - 2019 - Using Pre-Training Can Improve Model Robustness an.pdf;/Users/antoniohortaribeiro/Zotero/storage/6QVPHU6F/1901.html}
}

@book{hennessy_computer_2012,
  title = {Computer Architecture: A Quantitative Approach},
  shorttitle = {Computer Architecture},
  author = {Hennessy, John L. and Patterson, David A. and Asanovi{\'c}, Krste},
  year = {2012},
  edition = {5th ed},
  publisher = {Morgan Kaufmann/Elsevier},
  address = {Waltham, MA},
  isbn = {978-0-12-383872-8},
  lccn = {QA76.9.A73 P377 2012},
  keywords = {Computer architecture},
  annotation = {OCLC: ocn755102367},
  file = {/Users/antoniohortaribeiro/Zotero/storage/79T2BWXU/hennessy_computer_2012.pdf}
}

@inproceedings{hereid_hybrid_2015,
  title = {Hybrid Zero Dynamics Based Multiple Shooting Optimization with Applications to Robotic Walking},
  booktitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Hereid, A. and Hubicki, C. M. and Cousineau, E. A. and Hurst, J. W. and Ames, A. D.},
  year = {2015},
  month = may,
  pages = {5734--5740},
  doi = {10.1109/ICRA.2015.7140002},
  abstract = {Hybrid zero dynamics (HZD) has emerged as a popular framework for the stable control of bipedal robotic gaits, but typically designing a gait's virtual constraints is a slow and undependable optimization process. To expedite and boost the reliability of HZD gait generation, we borrow methods from trajectory optimization to formulate a smoother and more linear optimization problem. We present a multiple-shooting formulation for the optimization of virtual constraints, combining the stability-friendly properties of HZD with an optimization-conducive problem formulation. To showcase the implications of this recipe for improving gait generation, we use the same process to generate periodic planar walking gaits on two different robot models, and in one case, demonstrate stable walking on the hardware prototype, DURUS-R.},
  keywords = {Control systems,DURUS-R,Foot,hybrid zero dynamics,HZD gait generation,Joints,legged locomotion,linear optimization problem,multiple shooting optimization,optimisation,Optimization,optimization-conducive problem formulation,periodic planar walking gaits,Reliability,robotic walking,stability,stability-friendly properties,trajectory optimization},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FW7VGKFR/hereid_hybrid_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/8HGFUT48/7140002.html;/Users/antoniohortaribeiro/Zotero/storage/EG6D6BPK/7140002.html}
}

@article{hernan_causal_2020,
  title = {Causal {{Inference}}: {{What If}}},
  author = {Hernan, Miguel A and Robins, James M},
  year = {2020},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EG7Z4MJM/Hernan and Robins - Causal Inference What If.pdf}
}

@article{hernan_data_2018,
  title = {Data Science Is Science's Second Chance to Get Causal Inference Right: {{A}} Classification of Data Science Tasks},
  shorttitle = {Data Science Is Science's Second Chance to Get Causal Inference Right},
  author = {Hern{\'a}n, Miguel A. and Hsu, John and Healy, Brian},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.10846 [cs, stat]},
  eprint = {1804.10846},
  primaryclass = {cs, stat},
  urldate = {2018-12-13},
  abstract = {Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term "data science" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: description, prediction, and causal inference. An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WY55HVYP/hernÃ¡n_data_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/4LXXS6LH/1804.html}
}

@book{hestenes_methods_1952,
  title = {Methods of Conjugate Gradients for Solving Linear Systems},
  author = {Hestenes, Magnus Rudolph and Stiefel, Eduard},
  year = {1952},
  volume = {49},
  publisher = {NBS}
}

@article{hinton_deep_2012,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}: {{The Shared Views}} of {{Four Research Groups}}},
  shorttitle = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  author = {Hinton, G. and Deng, L. and Yu, D. and Dahl, G. E. and Mohamed, A. and Jaitly, N. and Senior, A. and Vanhoucke, V. and Nguyen, P. and Sainath, T. N. and Kingsbury, B.},
  year = {2012},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {82--97},
  issn = {1053-5888},
  doi = {10/gc8z3r},
  abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.},
  keywords = {acoustic modeling,Acoustics,Automatic speech recognition,Data models,deep neural networks,feed-forward neural network,feedforward neural nets,Gaussian mixture models,Gaussian processes,hidden Markov models,Hidden Markov models,HMM states,Neural networks,posterior probabilities,speech recognition,Speech recognition,temporal variability,Training},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FABZRDPP/hinton_deep_2012.pdf;/Users/antoniohortaribeiro/Zotero/storage/DYD3L6DZ/citations.html}
}

@article{hinton_deep_2018,
  title = {Deep Learning---a Technology with the Potential to Transform Health Care},
  author = {Hinton, Geoffrey},
  year = {2018},
  month = sep,
  journal = {JAMA},
  volume = {320},
  number = {11},
  pages = {1101--1102},
  issn = {0098-7484},
  doi = {10/gfkhr6},
  abstract = {Widespread application of artificial intelligence in health care has been anticipated for half a century. For most of that time, the dominant approach to artificial intelligence was inspired by logic: researchers assumed that the essence of intelligence was manipulating symbolic expressions, using rules of inference. This approach produced expert systems and graphical models that attempted to automate the reasoning processes of experts. In the last decade, however, a radically different approach to artificial intelligence, called deep learning, has produced major breakthroughs and is now used on billions of digital devices for complex tasks such as speech recognition, image interpretation, and language translation. The purpose of this Viewpoint is to give health care professionals an intuitive understanding of the technology underlying deep learning. In an accompanying Viewpoint, Naylor1 outlines some of the factors propelling adoption of this technology in medicine and health care.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YMGTM3F7/hinton_with the_2018.pdf}
}

@article{hinton_fast_2006,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  year = {2006},
  month = jul,
  journal = {Neural Computation},
  volume = {18},
  number = {7},
  pages = {1527--1554},
  issn = {0899-7667, 1530-888X},
  doi = {10/cjnhxz},
  urldate = {2019-02-27},
  abstract = {We show how to use ``complementary priors'' to eliminate the explaining away effects that make inference difficult in densely-connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modelled by long ravines in the free-energy landscape of the top-level associative memory and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/C59T52NT/Hinton et al. - 2006 - A Fast Learning Algorithm for Deep Belief Nets.pdf}
}

@inproceedings{hinton_learning_1986,
  title = {Learning Distributed Representations of Concepts},
  booktitle = {Proceedings of the Eighth Annual Conference of the Cognitive Science Society},
  author = {Hinton, Geoffrey E},
  year = {1986},
  volume = {1},
  pages = {12},
  publisher = {Amherst, MA}
}

@article{hinton_potential_2018,
  title = {With the {{Potential}} to {{Transform Health Care}}},
  author = {Hinton, Geoffrey},
  year = {2018},
  pages = {2},
  langid = {english},
  keywords = {ðŸ”No DOI found}
}

@inproceedings{hirschmuller_accurate_2005,
  title = {Accurate and Efficient Stereo Processing by Semi-Global Matching and Mutual Information},
  booktitle = {Computer {{Vision}} and {{Pattern Recognition}}, 2005. {{CVPR}} 2005. {{IEEE Computer Society Conference}} On},
  author = {Hirschm{\"u}ller, Heiko},
  year = {2005},
  volume = {2},
  pages = {807--814},
  publisher = {IEEE}
}

@article{hirschmuller_stereo_2005,
  title = {Stereo Vision Based Reconstruction of Huge Urban Areas from an Airborne Pushbroom Camera ({{HRSC}})},
  author = {Hirschm{\"u}ller, Heiko and Scholten, Frank and Hirzinger, Gerd},
  year = {2005},
  journal = {Lecture notes in computer science},
  volume = {3663},
  pages = {58},
  doi = {10.1007/11550518_8},
  urldate = {2017-08-20},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3XUZZPPJ/hirschmÃ¼ller_stereo_2005.pdf}
}

@article{hirschmuller_stereo_2008,
  title = {Stereo Processing by Semiglobal Matching and Mutual Information},
  author = {Hirschm{\"u}ller, Heiko},
  year = {2008},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  volume = {30},
  number = {2},
  pages = {328--341},
  doi = {10.1109/TPAMI.2007.1166}
}

@article{hlatky_criteria_2009,
  title = {Criteria for {{Evaluation}} of {{Novel Markers}} of {{Cardiovascular Risk}}: {{A Scientific Statement From}} the {{American Heart Association}}},
  shorttitle = {Criteria for {{Evaluation}} of {{Novel Markers}} of {{Cardiovascular Risk}}},
  author = {Hlatky, Mark A. and Greenland, Philip and Arnett, Donna K. and Ballantyne, Christie M. and Criqui, Michael H. and Elkind, Mitchell S.V. and Go, Alan S. and Harrell, Frank E. and Hong, Yuling and Howard, Barbara V. and Howard, Virginia J. and Hsue, Priscilla Y. and Kramer, Christopher M. and McConnell, Joseph P. and Normand, Sharon-Lise T. and O'Donnell, Christopher J. and Smith, Sidney C. and Wilson, Peter W.F.},
  year = {2009},
  month = may,
  journal = {Circulation},
  volume = {119},
  number = {17},
  pages = {2408--2416},
  issn = {0009-7322, 1524-4539},
  doi = {10.1161/CIRCULATIONAHA.109.192278},
  urldate = {2023-08-30},
  abstract = {There is increasing interest in utilizing novel markers of cardiovascular disease risk, and consequently, there is a need to assess the value of their use. This scientific statement reviews current concepts of risk evaluation and proposes standards for the critical appraisal of risk assessment methods. An adequate evaluation of a novel risk marker requires a sound research design, a representative at-risk population, and an adequate number of outcome events. Studies of a novel marker should report the degree to which it adds to the prognostic information provided by standard risk markers. No single statistical measure provides all the information needed to assess a novel marker, so measures of both discrimination and accuracy should be reported. The clinical value of a marker should be assessed by its effect on patient management and outcomes. In general, a novel risk marker should be evaluated in several phases, including initial proof of concept, prospective validation in independent populations, documentation of incremental information when added to standard risk markers, assessment of effects on patient management and outcomes, and ultimately, cost-effectiveness.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JRTNKTEL/Hlatky et al_2009_Criteria for Evaluation of Novel Markers of Cardiovascular Risk.pdf}
}

@article{hochreiter_long_1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  journal = {Neural computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EPFVFIRS/hochreiter_long_1997.pdf}
}

@article{hochreiter_vanishing_1998,
  title = {The {{Vanishing Gradient Problem During Learning Recurrent Neural Nets}} and {{Problem Solutions}}},
  author = {Hochreiter, Sepp},
  year = {1998},
  month = apr,
  journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  volume = {06},
  number = {02},
  pages = {107--116},
  issn = {0218-4885, 1793-6411},
  doi = {10.1142/S0218488598000094},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6PMVUGRF/hochreiter_the_1998.pdf}
}

@article{hodgkin_quantitative_1952,
  title = {A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve},
  author = {Hodgkin, A. L. and Huxley, A. F.},
  year = {1952},
  journal = {The Journal of Physiology},
  volume = {117},
  number = {4},
  pages = {500--544},
  issn = {1469-7793},
  doi = {10.1113/jphysiol.1952.sp004764},
  urldate = {2022-01-12},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/K499XTH6/Hodgkin and Huxley - 1952 - A quantitative description of membrane current and.pdf;/Users/antoniohortaribeiro/Zotero/storage/HIMGEWYZ/jphysiol.1952.html}
}

@book{hoffer_modern_2011,
  title = {Modern Database Management},
  author = {Hoffer, Jeffrey A. and Ramesh, V. and Topi, Heikki},
  year = {2011},
  edition = {10th ed},
  publisher = {Prentice Hall},
  address = {Upper Saddle River, N.J},
  isbn = {978-0-13-608839-4},
  lccn = {QA76.9.D3 M395 2011},
  keywords = {Database management},
  annotation = {OCLC: ocn613293263},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DCHKZ47G/hoffer_modern_2011.pdf}
}

@article{hoffman_nouturn_2011,
  title = {The {{No-U-Turn Sampler}}: {{Adaptively Setting Path Lengths}} in {{Hamiltonian Monte Carlo}}},
  shorttitle = {The {{No-U-Turn Sampler}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  year = {2011},
  month = nov,
  journal = {arXiv:1111.4246 [cs, stat]},
  eprint = {1111.4246},
  primaryclass = {cs, stat},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{{\textbackslash}epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{{\textbackslash}epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Machine Learning,Statistics - Computation},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3DN24GTI/hoffman_the_2011.pdf;/Users/antoniohortaribeiro/Zotero/storage/J68DR9GG/1111.html}
}

@article{hoffman_unsupervised_2012,
  title = {Unsupervised Pattern Discovery in Human Chromatin Structure through Genomic Segmentation},
  author = {Hoffman, Michael M. and Buske, Orion J. and Wang, Jie and Weng, Zhiping and Bilmes, Jeff A. and Noble, William Stafford},
  year = {2012},
  month = may,
  journal = {Nature Methods},
  volume = {9},
  number = {5},
  pages = {473--476},
  issn = {1548-7105},
  doi = {10/gfxrbn},
  urldate = {2019-04-01},
  abstract = {We trained Segway, a dynamic Bayesian network method, simultaneously on chromatin data from multiple experiments, including positions of histone modifications, transcription-factor binding and open chromatin, all derived from a human chronic myeloid leukemia cell line. In an unsupervised fashion, we identified patterns associated with transcription start sites, gene ends, enhancers, transcriptional regulator CTCF-binding regions and repressed regions. Software and genome browser tracks are at http://noble.gs.washington.edu/proj/segway/.},
  copyright = {2012 Nature Publishing Group},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TWHQ3IZH/hoffman_unsupervis_2012.pdf;/Users/antoniohortaribeiro/Zotero/storage/QKKF3W3V/nmeth.html}
}

@inproceedings{hong_encase_2017,
  title = {{{ENCASE}}: An {{ENsemble ClASsifiEr}} for {{ECG Classification Using Expert Features}} and {{Deep Neural Networks}}},
  shorttitle = {{{ENCASE}}},
  booktitle = {2017 {{Computing}} in {{Cardiology Conference}}},
  author = {Hong, Shenda and Wu, Meng and Zhou, Yuxi and Wang, Qingyun and Shang, Junyuan and Li, Hongyan and Xie, Junqing},
  year = {2017},
  month = sep,
  doi = {10/gftsxw},
  urldate = {2019-01-24},
  abstract = {We propose ENCASE to combine expert features and DNNs (Deep Neural Networks) together for ECG classification. We first explore and implement expert features from statistical area, signal processing area and medical area. Then, we build DNNs to automatically extract deep features. Besides, we propose a new algorithm to find the most representative wave (called centerwave) among long ECG record, and extract features from centerwave. Finally, we combine these features together and put them into ensemble classifiers. Experiment on 4-class ECG data classification reports 0.84 F1 score, which is much better than any of the single model.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZUPP83PW/Hong et al. - 2017 - ENCASE an ENsemble ClASsifiEr for ECG Classificat.pdf}
}

@article{hooke_direct_1961,
  title = {``{{Direct Search}}'' {{Solution}} of {{Numerical}} and {{Statistical Problems}}},
  author = {Hooke, Robert and Jeeves, Terry A},
  year = {1961},
  journal = {Journal of the ACM (JACM)},
  volume = {8},
  number = {2},
  pages = {212--229},
  doi = {10.1145/321062.321069}
}

@article{horbelt_identifying_2001,
  title = {Identifying Physical Properties of a {{CO2 LASER}} by Dynamical Modeling of Measured Time Series},
  author = {Horbelt, W and Timmer, J and B{\"u}nner, {\relax MJ} and Meucci, R and Ciofini, M},
  year = {2001},
  journal = {Physical Review E},
  volume = {64},
  number = {1},
  pages = {016222},
  doi = {10.1103/PhysRevE.64.016222}
}

@article{hornik_multilayer_1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  keywords = {â“Multiple DOI}
}

@article{howard_universal_2018,
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  author = {Howard, Jeremy and Ruder, Sebastian},
  year = {2018},
  month = jan,
  journal = {arXiv:1801.06146 [cs, stat]},
  eprint = {1801.06146},
  primaryclass = {cs, stat},
  urldate = {2019-06-10},
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BW93EXTK/howard_universal_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/54PX9Q55/1801.html}
}

@inproceedings{hsu_proliferation_2021,
  title = {On the Proliferation of Support Vectors in High Dimensions},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Hsu, Daniel and Muthukumar, Vidya and Xu, Ji},
  year = {2021},
  month = mar,
  pages = {91--99},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2022-11-16},
  abstract = {The support vector machine (SVM) is a well-established classification method whose name refers to the particular training examples, called support vectors, that determine the maximum margin separating hyperplane. The SVM classifier is known to enjoy good generalization properties when the number of support vectors is small compared to the number of training examples. However, recent research has shown that in sufficiently high-dimensional linear classification problems, the SVM can generalize well despite a proliferation of support vectors where all training examples are support vectors. In this paper, we identify new deterministic equivalences for this phenomenon of support vector proliferation, and use them to (1) substantially broaden the conditions under which the phenomenon occurs in high-dimensional settings, and (2) prove a nearly matching converse result.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ND9J9J2J/Hsu et al. - 2021 - On the proliferation of support vectors in high di.pdf;/Users/antoniohortaribeiro/Zotero/storage/XBGSF7LM/Hsu et al. - 2021 - On the proliferation of support vectors in high di.pdf}
}

@article{hsu_subset_2008,
  title = {Subset Selection for Vector Autoregressive Processes Using {{Lasso}}},
  author = {Hsu, Nan-Jung and Hung, Hung-Lin and Chang, Ya-Mei},
  year = {2008},
  month = mar,
  journal = {Computational Statistics \& Data Analysis},
  volume = {52},
  number = {7},
  pages = {3645--3657},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2007.12.004},
  abstract = {A subset selection method is proposed for vector autoregressive (VAR) processes using the Lasso [Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B 58, 267--288] technique. Simply speaking, Lasso is a shrinkage method in a regression setup which selects the model and estimates the parameters simultaneously. Compared to the conventional information-based methods such as AIC and BIC, the Lasso approach avoids computationally intensive and exhaustive search. On the other hand, compared to the existing subset selection methods with parameter constraints such as the top-down and bottom-up strategies, the Lasso method is computationally efficient and its result is robust to the order of series included in the autoregressive model. We derive the asymptotic theorem for the Lasso estimator under VAR processes. Simulation results demonstrate that the Lasso method performs better than several conventional subset selection methods for small samples in terms of prediction mean squared errors and estimation errors under various settings. The methodology is applied to modeling U.S. macroeconomic data for illustration.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SZJFU46I/hsu_subset_2008.pdf;/Users/antoniohortaribeiro/Zotero/storage/58HQHHHT/S0167947307004549.html;/Users/antoniohortaribeiro/Zotero/storage/D74969DK/S0167947307004549.html}
}

@misc{hu_infinitely_2020,
  title = {Infinitely {{Wide Graph Convolutional Networks}}: {{Semi-supervised Learning}} via {{Gaussian Processes}}},
  shorttitle = {Infinitely {{Wide Graph Convolutional Networks}}},
  author = {Hu, Jilin and Shen, Jianbing and Yang, Bin and Shao, Ling},
  year = {2020},
  month = feb,
  number = {arXiv:2002.12168},
  eprint = {2002.12168},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.12168},
  urldate = {2022-07-26},
  abstract = {Graph convolutional neural networks{\textasciitilde}(GCNs) have recently demonstrated promising results on graph-based semi-supervised classification, but little work has been done to explore their theoretical properties. Recently, several deep neural networks, e.g., fully connected and convolutional neural networks, with infinite hidden units have been proved to be equivalent to Gaussian processes{\textasciitilde}(GPs). To exploit both the powerful representational capacity of GCNs and the great expressive power of GPs, we investigate similar properties of infinitely wide GCNs. More specifically, we propose a GP regression model via GCNs{\textasciitilde}(GPGC) for graph-based semi-supervised learning. In the process, we formulate the kernel matrix computation of GPGC in an iterative analytical form. Finally, we derive a conditional distribution for the labels of unobserved nodes based on the graph structure, labels for the observed nodes, and the feature matrix of all the nodes. We conduct extensive experiments to evaluate the semi-supervised classification performance of GPGC and demonstrate that it outperforms other state-of-the-art methods by a clear margin on all the datasets while being efficient.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZJVYJ8D3/Hu et al. - 2020 - Infinitely Wide Graph Convolutional Networks Semi.pdf;/Users/antoniohortaribeiro/Zotero/storage/CMDFR2EV/2002.html}
}

@misc{hua_causal_2022,
  title = {Causal {{Information Bottleneck Boosts Adversarial Robustness}} of {{Deep Neural Network}}},
  author = {Hua, Huan and Yan, Jun and Fang, Xi and Huang, Weiquan and Yin, Huilin and Ge, Wancheng},
  year = {2022},
  month = oct,
  number = {arXiv:2210.14229},
  eprint = {2210.14229},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.14229},
  urldate = {2023-04-14},
  abstract = {The information bottleneck (IB) method is a feasible defense solution against adversarial attacks in deep learning. However, this method suffers from the spurious correlation, which leads to the limitation of its further improvement of adversarial robustness. In this paper, we incorporate the causal inference into the IB framework to alleviate such a problem. Specifically, we divide the features obtained by the IB method into robust features (content information) and non-robust features (style information) via the instrumental variables to estimate the causal effects. With the utilization of such a framework, the influence of non-robust features could be mitigated to strengthen the adversarial robustness. We make an analysis of the effectiveness of our proposed method. The extensive experiments in MNIST, FashionMNIST, and CIFAR-10 show that our method exhibits the considerable robustness against multiple adversarial attacks. Our code would be released.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8BEE3N5H/Hua et al. - 2022 - Causal Information Bottleneck Boosts Adversarial R.pdf;/Users/antoniohortaribeiro/Zotero/storage/57IJBHXZ/2210.html}
}

@article{huang_densely_2016,
  title = {Densely {{Connected Convolutional Networks}}},
  author = {Huang, Gao and Liu, Zhuang and {van der Maaten}, Laurens and Weinberger, Kilian Q.},
  year = {2016},
  month = aug,
  journal = {arXiv:1608.06993 [cs]},
  eprint = {1608.06993},
  primaryclass = {cs},
  urldate = {2019-06-13},
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/M8QULTPP/huang_densely_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/ULVSI9WH/1608.html}
}

@article{huang_generalization_2024,
  title = {Generalization Challenges in Electrocardiogram Deep Learning: Insights from Dataset Characteristics and Attention Mechanism},
  shorttitle = {Generalization Challenges in Electrocardiogram Deep Learning},
  author = {Huang, Zhaojing and MacLachlan, Sarisha and Yu, Leping and Herbozo Contreras, Luis Fernando and Truong, Nhan Duy and Ribeiro, Antonio Horta and Kavehei, Omid},
  year = {2024},
  journal = {Future Cardiology},
  volume = {0},
  number = {0},
  pages = {1--12},
  issn = {1479-6678},
  doi = {10.1080/14796678.2024.2354082},
  urldate = {2024-06-12},
  abstract = {Aim: Deep learning's widespread use prompts heightened scrutiny, particularly in the biomedical fields, with a specific focus on model generalizability. This study delves into the influence of training data characteristics on the generalization performance of models, specifically in cardiac abnormality detection. Materials \& methods: Leveraging diverse electrocardiogram datasets, models are trained on subsets with varying characteristics and subsequently compared for performance. Additionally, the introduction of the attention mechanism aims to improve generalizability. Results: Experiments reveal that using a balanced dataset, just 1\% of a large dataset, leads to equal performance in generalization tasks, notably in detecting cardiology abnormalities. Conclusion: This balanced training data notably enhances model generalizability, while the integration of the attention mechanism further refines the model's ability to generalize effectively. This study tackles a common problem for deep learning models: they often struggle when faced with new, unfamiliar data that they have not~been trained on. This phenomenon is also known as performance drop in out-of-distribution generalization. This reduced performance on out-of-distribution generalization is a key focus of the research, aiming to improve the models'~ability to handle diverse data sets beyond their training data. The study examines how the characteristics of the dataset used to train deep learning models affect their ability to detect abnormal heart activities when applied to new, unseen data. Researchers trained these models using various sets of electrocardiogram~(ECG)~data and then evaluated their performance in identifying abnormalities. They also introduced an attention mechanism to enhance the models'~learning capabilities. The attention mechanism in deep learning is like a spotlight that helps the model focus on important information while ignoring less relevant details. The findings were particularly noteworthy. Despite being trained on a small, well-balanced subset of a larger dataset, the models excelled in detecting heart abnormalities in new, unfamiliar data. This training method significantly improved the models'~generalization and performance with unseen data. Furthermore, integrating the attention mechanism substantially enhanced the models'~ability to generalize effectively on new information. Investigate the impact of training data characteristics and attention mechanism on deep learning model generalizability in cardiac abnormality detection. Balanced dataset (1\% of the total) improves model performance in generalization tasks, especially in detecting cardiology abnormalities. The attention mechanism further enhances the model's capacity to comprehend and utilize out-of-distribution data effectively. Utilized multiple electrocardiogram datasets for the study. Trained models on subsets with varying characteristics and evaluated performance. Added attention mechanism to enhance learning capabilities. Balanced training data significantly enhances model generalizability. Attention mechanism improves the model's ability to generalize on out-of-distribution data. Lack of clinical user information in datasets due to privacy and ethical considerations. Future research may consider patient-specific models for improved generalization in biomedical machine learning. Balanced and curated datasets are crucial for training high-performing models in cardiac abnormality detection using deep learning. Attention mechanisms show promise in enhancing model accuracy and generalization.},
  copyright = {All rights reserved},
  keywords = {attention mechanism,cardiac abnormality detection,dataset characteristics,deep learning,electrocardiogram,generalization},
  annotation = {https://www.medrxiv.org/content/10.1101/2023.07.05.23292238v2},
  file = {/Users/antoniohortaribeiro/Zotero/storage/94FTKEU9/Huang et al. - 2024 - Generalization challenges in electrocardiogram dee.pdf}
}

@article{huang_importance_2021,
  title = {On the {{Importance}} of {{Gradients}} for {{Detecting Distributional Shifts}} in the {{Wild}}},
  author = {Huang, Rui and Geng, Andrew and Li, Yixuan},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.00218 [cs]},
  eprint = {2110.00218},
  primaryclass = {cs},
  urldate = {2021-11-24},
  abstract = {Detecting out-of-distribution (OOD) data has become a critical component in ensuring the safe deployment of machine learning models in the real world. Existing OOD detection approaches primarily rely on the output or feature space for deriving OOD scores, while largely overlooking information from the gradient space. In this paper, we present GradNorm, a simple and effective approach for detecting OOD inputs by utilizing information extracted from the gradient space. GradNorm directly employs the vector norm of gradients, backpropagated from the KL divergence between the softmax output and a uniform probability distribution. Our key idea is that the magnitude of gradients is higher for in-distribution (ID) data than that for OOD data, making it informative for OOD detection. GradNorm demonstrates superior performance, reducing the average FPR95 by up to 16.33\% compared to the previous best method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VKKAJEKY/Huang et al. - 2021 - On the Importance of Gradients for Detecting Distr.pdf;/Users/antoniohortaribeiro/Zotero/storage/DUGPCJ88/2110.html}
}

@article{huang_learning_2016,
  title = {Learning with a {{Strong Adversary}}},
  author = {Huang, Ruitong and Xu, Bing and Schuurmans, Dale and Szepesvari, Csaba},
  year = {2016},
  month = jan,
  journal = {arXiv:1511.03034},
  eprint = {1511.03034},
  urldate = {2022-04-29},
  abstract = {The robustness of neural networks to intended perturbations has recently attracted significant attention. In this paper, we propose a new method, {\textbackslash}emph\{learning with a strong adversary\}, that learns robust classifiers from supervised data. The proposed method takes finding adversarial examples as an intermediate step. A new and simple way of finding adversarial examples is presented and experimentally shown to be efficient. Experimental results demonstrate that resulting learning method greatly improves the robustness of the classification models produced.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GAU9RQVB/Huang et al_2016_Learning with a Strong Adversary.pdf;/Users/antoniohortaribeiro/Zotero/storage/YMZ7D3RT/1511.html}
}

@book{huber_robust_1981,
  title = {Robust Statistics},
  author = {Huber, Peter J.},
  year = {1981},
  series = {Wiley Series in Probability and Mathematical Statistics},
  publisher = {Wiley},
  address = {New York},
  isbn = {978-0-471-41805-4},
  langid = {english},
  lccn = {QA276 .H785},
  keywords = {Robust statistics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RDTV4RZ8/Huber - 1981 - Robust statistics.pdf}
}

@article{hughes_using_2018,
  title = {Using {{Multitask Learning}} to {{Improve}} 12-{{Lead Electrocardiogram Classification}}},
  author = {Hughes, J. Weston and MD, Taylor Sittler and Joseph, Anthony D. and MD, Jeffrey E. Olgin and Gonzalez, Joseph E. and MD, Geoffrey H. Tison},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.00497 [cs, stat]},
  eprint = {1812.00497},
  primaryclass = {cs, stat},
  urldate = {2018-12-04},
  abstract = {We develop a multi-task convolutional neural network (CNN) to classify multiple diagnoses from 12-lead electrocardiograms (ECGs) using a dataset comprised of over 40,000 ECGs, with labels derived from cardiologist clinical interpretations. Since many clinically important classes can occur in low frequencies, approaches are needed to improve performance on rare classes. We compare the performance of several single-class classifiers on rare classes to a multi-headed classifier across all available classes. We demonstrate that the addition of common classes can significantly improve CNN performance on rarer classes when compared to a model trained on the rarer class in isolation. Using this method, we develop a model with high performance as measured by F1 score on multiple clinically relevant classes compared against the gold-standard cardiologist interpretation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DZV26G2Q/hughes_using_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/PC52FZ4G/1812.html}
}

@inproceedings{huijben_deep_2019,
  title = {Deep Probabilistic Subsampling for Task-Adaptive Compressed Sensing},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Huijben, Iris A. M. and Veeling, Bastiaan S. and van Sloun, Ruud J. G.},
  year = {2019},
  month = sep,
  urldate = {2020-07-20},
  abstract = {The field of deep learning is commonly concerned with optimizing predictive models using large pre-acquired datasets of densely sampled datapoints or signals. In this work, we demonstrate that the...},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HF2Z8VFT/Huijben et al. - 2019 - Deep probabilistic subsampling for task-adaptive c.pdf;/Users/antoniohortaribeiro/Zotero/storage/AHWMAA4E/forum.html}
}

@article{hurvich_impact_1990,
  title = {The {{Impact}} of {{Model Selection}} on {{Inference}} in {{Linear Regression}}},
  author = {Hurvich, Clifford M. and Tsai, Chih---Ling},
  year = {1990},
  month = aug,
  journal = {The American Statistician},
  volume = {44},
  number = {3},
  pages = {214--217},
  issn = {0003-1305},
  doi = {10.1080/00031305.1990.10475722},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WAT9JDFN/hurvich_the_1990.pdf}
}

@article{hussain_autonomous_2019,
  title = {Autonomous {{Cars}}: {{Research Results}}, {{Issues}}, and {{Future Challenges}}},
  shorttitle = {Autonomous {{Cars}}},
  author = {Hussain, Rasheed and Zeadally, Sherali},
  year = {2019},
  journal = {IEEE Communications Surveys \& Tutorials},
  volume = {21},
  number = {2},
  pages = {1275--1313},
  issn = {1553-877X},
  doi = {10.1109/COMST.2018.2869360},
  abstract = {Throughout the last century, the automobile industry achieved remarkable milestones in manufacturing reliable, safe, and affordable vehicles. Because of significant recent advances in computation and communication technologies, autonomous cars are becoming a reality. Already autonomous car prototype models have covered millions of miles in test driving. Leading technical companies and car manufacturers have invested a staggering amount of resources in autonomous car technology, as they prepare for autonomous cars' full commercialization in the coming years. However, to achieve this goal, several technical and nontechnical issues remain: software complexity, real-time data analytics, and testing and verification are among the greater technical challenges; and consumer stimulation, insurance management, and ethical/moral concerns rank high among the nontechnical issues. Tackling these challenges requires thoughtful solutions that satisfy consumers, industry, and governmental requirements, regulations, and policies. Thus, here we present a comprehensive review of state-of-the-art results for autonomous car technology. We discuss current issues that hinder autonomous cars' development and deployment on a large scale. We also highlight autonomous car applications that will benefit consumers and many other sectors. Finally, to enable cost-effective, safe, and efficient autonomous cars, we discuss several challenges that must be addressed (and provide helpful suggestions for adoption) by designers, implementers, policymakers, regulatory organizations, and car manufacturers.},
  keywords = {Automobiles,Autonomous automobiles,Autonomous cars,Companies,connected cars,driverless cars,Industries,policy,privacy,Roads,security,simulation,Tutorials,Vehicular ad hoc networks},
  file = {/Users/antoniohortaribeiro/Zotero/storage/K9SMHJ68/Hussain and Zeadally - 2019 - Autonomous Cars Research Results, Issues, and Fut.pdf;/Users/antoniohortaribeiro/Zotero/storage/HS8UQFZ2/8457076.html}
}

@inproceedings{huster_limitations_2019,
  title = {Limitations of the {{Lipschitz Constant}} as a {{Defense Against Adversarial Examples}}},
  booktitle = {{{ECML PKDD}} 2018 {{Workshops}}},
  author = {Huster, Todd and Chiang, Cho-Yu Jason and Chadha, Ritu},
  editor = {Alzate, Carlos and Monreale, Anna and Assem, Haytham and Bifet, Albert and Buda, Teodora Sandra and Caglayan, Bora and Drury, Brett and {Garc{\'i}a-Mart{\'i}n}, Eva and Gavald{\`a}, Ricard and Koprinska, Irena and Kramer, Stefan and Lavesson, Niklas and Madden, Michael and Molloy, Ian and Nicolae, Maria-Irina and Sinn, Mathieu},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {16--29},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-13453-2_2},
  abstract = {Several recent papers have discussed utilizing Lipschitz constants to limit the susceptibility of neural networks to adversarial examples. We analyze recently proposed methods for computing the Lipschitz constant. We show that the Lipschitz constant may indeed enable adversarially robust neural networks. However, the methods currently employed for computing it suffer from theoretical and practical limitations. We argue that addressing this shortcoming is a promising direction for future research into certified adversarial defenses.},
  isbn = {978-3-030-13453-2},
  langid = {english},
  keywords = {Adversarial examples,Lipschitz constant},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6XD4CRLB/Huster et al. - 2019 - Limitations of the Lipschitz Constant as a Defense.pdf}
}

@inproceedings{hutchison_evaluation_2010,
  title = {Evaluation of {{Pooling Operations}} in {{Convolutional Architectures}} for {{Object Recognition}}},
  booktitle = {Artificial {{Neural Networks}} -- {{ICANN}} 2010},
  author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Scherer, Dominik and M{\"u}ller, Andreas and Behnke, Sven},
  editor = {Diamantaras, Konstantinos and Duch, Wlodek and Iliadis, Lazaros S.},
  year = {2010},
  volume = {6354},
  pages = {92--101},
  doi = {10.1007/978-3-642-15825-4_10},
  urldate = {2019-02-13},
  abstract = {A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant properties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57\% on the NORB normalized-uniform dataset and 5.6\% on the NORB jittered-cluttered dataset.},
  isbn = {978-3-642-15824-7 978-3-642-15825-4},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XGHQELT4/Hutchison et al. - 2010 - Evaluation of Pooling Operations in Convolutional .pdf}
}

@article{huval_empirical_2015,
  title = {An Empirical Evaluation of Deep Learning on Highway Driving},
  author = {Huval, Brody and Wang, Tao and Tandon, Sameep and Kiske, Jeff and Song, Will and Pazhayampallil, Joel and Andriluka, Mykhaylo and Rajpurkar, Pranav and Migimatsu, Toki and {Cheng-Yue}, Royce and Mujica, Fernando A. and Coates, Adam and Ng, Andrew Y.},
  year = {2015},
  journal = {CoRR},
  volume = {abs/1504.01716},
  eprint = {1504.01716},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/HuvalWTKSPARMCM15.bib},
  timestamp = {Mon, 11 Mar 2019 09:54:20 +0100}
}

@article{huyck_identification_2011,
  title = {Identification of a {{Pilot Scale Distillation Column}}: {{A Kernel Based Approach}}},
  shorttitle = {Identification of a {{Pilot Scale Distillation Column}}},
  author = {Huyck, B. and De Brabanter, K. and Logist, F. and De Brabanter, J. and Van Impe, J. and De Moor, B.},
  year = {2011},
  month = jan,
  journal = {IFAC Proceedings Volumes},
  series = {18th {{IFAC World Congress}}},
  volume = {44},
  number = {1},
  pages = {471--476},
  issn = {1474-6670},
  doi = {10.3182/20110828-6-IT-1002.01512},
  abstract = {This paper describes the identification of a binary distillation column with Least-Squares Support Vector Machines (LS-SVM). It is our intention to investigate whether a kernel based model, particularly an LS-SVM, can be used for the simulation of the top and bottom temperature of a binary distillation column. Furthermore, we compare the latter model with standard linear models by means of mean-squared error (MSE). It will be demonstrated that this nonlinear model class achieves higher performances in MSE than linear models in the presence of nonlinear distortions. When the system is close to linear, the performance of the LS-SVM is only slightly better than the linear models.},
  keywords = {Chemical Industry,Distillation columns,kernel based system identification},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EELXRIVQ/S1474667016436542.html;/Users/antoniohortaribeiro/Zotero/storage/ZPIDNSXU/S1474667016436542.html}
}

@article{hwang_simulated_1988,
  title = {Simulated Annealing: Theory and Applications},
  author = {Hwang, Chii-Ruey},
  year = {1988},
  journal = {Acta Applicandae Mathematicae},
  volume = {12},
  number = {1},
  pages = {108--111},
  keywords = {ðŸ”No DOI found}
}

@inproceedings{iangoodfellow_maxout_2013,
  title = {Maxout {{Networks}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {{Ian Goodfellow} and {David Warde-Farley} and {Mehdi Mirza} and {Aaron Courville} and {Yoshua Bengio}},
  editor = {{Sanjoy Dasgupta} and {David McAllester}},
  year = {2013},
  month = feb,
  pages = {1319--1327},
  publisher = {PMLR},
  abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.}
}

@inproceedings{ifju_flexiblewingbased_2002,
  title = {Flexible-Wing-Based Micro Air Vehicles},
  booktitle = {40th {{AIAA}} Aerospace Sciences Meeting \& Exhibit},
  author = {Ifju, P and Jenkins, D and Ettinger, Scott and Lian, Yongsheng and Shyy, Wei and Waszak, M},
  year = {2002},
  pages = {705}
}

@article{ilyas_adversarial_2019,
  title = {Adversarial {{Examples Are Not Bugs}}, {{They Are Features}}},
  author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  year = {2019},
  journal = {Advances in Neural Information Processing Systems},
  volume = {32},
  eprint = {1905.02175},
  urldate = {2020-05-18},
  abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WB5MXBLB/Ilyas et al. - 2019 - Adversarial Examples Are Not Bugs, They Are Featur.pdf;/Users/antoniohortaribeiro/Zotero/storage/8IR22RGX/1905.html}
}

@techreport{instruments_ni_2015,
  title = {{{NI USB-6008}}/6009 {{User Guide}}},
  author = {Instruments, National},
  year = {2015},
  institution = {National Instruments}
}

@inproceedings{ioffe_batch_2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = jun,
  pages = {448--456},
  publisher = {PMLR},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
  keywords = {ðŸ”No DOI found,Computer Science - Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4QI95APQ/ioffe_batch_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/C9VFEKJ7/1502.html}
}

@article{iravanian_novel_2002,
  title = {A Novel Algorithm for Cardiac Biosignal Filtering Based on Filtered Residue Method},
  author = {Iravanian, S. and Tung, L.},
  year = {2002},
  month = nov,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {49},
  number = {11},
  pages = {1310--1317},
  issn = {0018-9294},
  doi = {10.1109/TBME.2002.804589},
  abstract = {In this paper, a new algorithm is presented for the filtering (de-noising) of cardiac bioelectrical signals. The primary target of this algorithm is the class of cardiac action potentials recorded using voltage-sensitive dyes, although the method is also applied to electrocardiographic signals High periodicity is one of the main features of cardiac biosignals. The proposed algorithm exploits this feature in filtering signals with a minimum amount of distortion. The basic idea is to use signal averaging in time to rind the stationary portion of the signal. The residue is found by subtracting the signal average from the corresponding points of the input. After passing through a low-pass filter, the filtered residue (FR) is added back to the signal average to reconstruct the output. The practical implementation of the filter residue algorithm is discussed. Stretching and shrinking operations are the basis for the conversion of quasi-periodic signals into periodic signals, which can then be subjected to the FR algorithm. Various examples are presented, and error estimation is performed to guide the selection of optimal parameters for the algorithm. The ability of the algorithm to reconstruct the variation among beats is demonstrated, and its limitations are discussed.},
  keywords = {{Animals, Newborn},{Models, Cardiovascular},{Models, Statistical},{Signal Processing, Computer-Assisted},Action Potentials,adaptive filtering,adaptive filters,Algorithms,Animals,AWGN,Bioelectric phenomena,bioelectric potentials,cardiac action potentials,cardiac biosignal filtering,cellular potentials,Computer Simulation,denoising,electrocardiography,Electrophysiology,error estimation,filtered residue method,Filtering algorithms,Finite impulse response filter,FIR filters,Low pass filters,low-pass filter,low-pass filters,medical signal processing,Membrane Potentials,Muscle Cells,Noise reduction,Optical distortion,Optical filters,optical mapping,Optical recording,optimal parameters,Quality Control,Rats,Reproducibility of Results,residue number systems,Sensitivity and Specificity,signal denoising,stationary portion,stretching/shrinking operation,transmembrane voltage recordings,Voltage,voltage-sensitive dyes,white Gaussian random noise},
  file = {/Users/antoniohortaribeiro/Zotero/storage/A5V3BUZF/iravanian_a novel_2002.pdf;/Users/antoniohortaribeiro/Zotero/storage/TR6TPGXQ/1046939.html}
}

@article{irigoyen_narx_2013,
  title = {A {{NARX}} Neural Network Model for Enhancing Cardiovascular Rehabilitation Therapies},
  author = {Irigoyen, Eloy and Mi{\~n}ano, Gorka},
  year = {2013},
  journal = {Neurocomputing},
  volume = {109},
  pages = {9--15},
  doi = {10.1016/j.neucom.2012.07.031}
}

@article{isaksson_using_2015,
  title = {Using Horizon Estimation and Nonlinear Optimization for Grey-Box Identification},
  author = {Isaksson, Alf J and Sj{\"o}berg, Johan and T{\"o}rnqvist, David and Ljung, Lennart and Kok, Manon},
  year = {2015},
  journal = {Journal of Process Control},
  volume = {30},
  pages = {69--79},
  issn = {0959-1524},
  doi = {10.1016/j.jprocont.2014.12.008}
}

@article{jacobs_sparse_2018,
  title = {Sparse {{Bayesian Nonlinear System Identification}} Using {{Variational Inference}}},
  author = {Jacobs, W. R. and Baldacchino, T. and Dodd, T. J. and Anderson, S. R.},
  year = {2018},
  journal = {IEEE Transactions on Automatic Control},
  pages = {1--1},
  issn = {0018-9286},
  doi = {10.1109/TAC.2018.2813004},
  abstract = {Bayesian nonlinear system identification for one of the major classes of dynamic model, the nonlinear autoregressive with exogenous input (NARX) model, has not been widely studied to date. Markov chain Monte Carlo (MCMC) methods have been developed, which tend to be accurate but can also be slow to converge. In this contribution, we present a novel, computationally efficient solution to sparse Bayesian identification of the NARX model using variational inference, which is orders of magnitude faster than MCMC methods. A sparsity-inducing hyper-prior is used to solve the structure detection problem. Key results include: 1. successful demonstration of the method on low signal-to-noise ratio signals (down to 2dB); 2. successful benchmarking in terms of speed and accuracy against a number of other algorithms: Bayesian LASSO, reversible jump MCMC, forward regression orthogonalisation, LASSO and simulation error minimisation with pruning; 3. accurate identification of a real world system, an electroactive polymer; and 4. demonstration for the first time of numerically propagating the estimated nonlinear time-domain model parameter uncertainty into the frequency-domain.},
  keywords = {Analytical models,Bayes methods,Bayesian estimation,Computational modeling,Data models,NARX model,nonlinear systems,Numerical models,system identification,variational inference},
  file = {/Users/antoniohortaribeiro/Zotero/storage/P8E6UDV5/jacobs_sparse_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/SF22UVJ7/8307489.html}
}

@article{jacot_implicit_,
  title = {Implicit {{Regularization}} of {{Random Feature Models}}},
  author = {Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Cl{\'e}ment and Gabriel, Franck},
  pages = {10},
  abstract = {Random Feature (RF) models are used as efficient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel Ridge Regression (KRR). For a Gaussian RF model with P features, N data points, and a ridge {$\lambda$}, we show that the average (i.e. expected) RF predictor is close to a KRR predictor with an effective ridge {$\lambda$}{\texttildelow}. We show that {$\lambda$}{\texttildelow} {$>$} {$\lambda$} and {$\lambda$}{\texttildelow} {$\lambda$} monotonically as P grows, thus revealing the implicit regularization effect of finite RF sampling. We then compare the risk (i.e. test error) of the {$\lambda$}{\texttildelow}KRR predictor with the average risk of the {$\lambda$}-RF predictor and obtain a precise and explicit bound on their difference. Finally, we empirically find an extremely good agreement between the test errors of the average {$\lambda$}-RF predictor and {$\lambda$}{\texttildelow}-KRR predictor.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QAZZAQI4/Jacot et al. - Implicit Regularization of Random Feature Models.pdf}
}

@article{jacot_neural_2018,
  title = {Neural {{Tangent Kernel}}: {{Convergence}} and {{Generalization}} in {{Neural Networks}}},
  shorttitle = {Neural {{Tangent Kernel}}},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  year = {2018},
  journal = {Advances in Neural Information Processing Systems 31},
  eprint = {1806.07572},
  urldate = {2020-07-27},
  abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function \$f\_{\textbackslash}theta\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_{\textbackslash}theta\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Probability,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5D9A47EU/Jacot et al. - 2020 - Neural Tangent Kernel Convergence and Generalizat.pdf;/Users/antoniohortaribeiro/Zotero/storage/G9SP9GM6/1806.html}
}

@article{jaeger_echo_,
  title = {The ``Echo State'' Approach to Analysing and Training Recurrent Neural Networks -- with an {{Erratum}} Note},
  author = {Jaeger, Herbert},
  pages = {48},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XFTBRYYQ/Jaeger - The â€œecho stateâ€ approach to analysing and trainin.pdf}
}

@article{jaeger_harnessing_2008,
  title = {Harnessing {{Nonlinearity}}: {{Predicting Chaotic Systems}} and {{Saving Energy}} in {{Wireless Communication}}},
  author = {Jaeger, Herbert and Haas, Harald},
  year = {2008},
  journal = {Science},
  volume = {304},
  pages = {78--80},
  keywords = {Image resolution,Nonlinear system,Web of Science},
  file = {/Users/antoniohortaribeiro/Zotero/storage/L63L6MWP/Jaeger - Supporting Online Material.pdf;/Users/antoniohortaribeiro/Zotero/storage/N2HJXZ52/Jaeger and Haas - 2008 - Herbert Jaeger , Communication Systems and Saving .pdf}
}

@article{jaeger_optimization_2007,
  title = {Optimization and Applications of Echo State Networks with Leaky- Integrator Neurons},
  author = {Jaeger, Herbert and Luko{\v s}evi{\v c}ius, Mantas and Popovici, Dan and Siewert, Udo},
  year = {2007},
  month = apr,
  journal = {Neural Networks},
  volume = {20},
  number = {3},
  pages = {335--352},
  issn = {08936080},
  doi = {10.1016/j.neunet.2007.04.016},
  urldate = {2019-10-28},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/V9PYREN4/Jaeger et al. - 2007 - Optimization and applications of echo state networ.pdf}
}

@book{jaeger_reservoir_2009,
  title = {Reservoir Computing Approaches to Recurrent Neural Network Training},
  author = {Jaeger, Herbert},
  year = {2009},
  abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both: current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current ``brand-names'' of reservoir methods, and thus aims to help unifying the field and providing the reader with a detailed ``map'' of it.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/D7HECPDG/Jaeger - 2009 - Reservoir computing approaches to recurrent neural.pdf;/Users/antoniohortaribeiro/Zotero/storage/MRGKWICT/summary.html}
}

@inproceedings{jakubovitz_improving_2018,
  title = {Improving {{DNN}} Robustness to Adversarial Attacks Using Jacobian Regularization},
  booktitle = {European Conference on Computer Vision},
  author = {Jakubovitz, Daniel and Giryes, Raja},
  year = {2018}
}

@inproceedings{jambukia_classification_2015,
  title = {Classification of {{ECG}} Signals Using Machine Learning Techniques: {{A}} Survey},
  shorttitle = {Classification of {{ECG}} Signals Using Machine Learning Techniques},
  booktitle = {Proceedings of the {{International Conference}} on {{Advances}} in {{Computer Engineering}} and {{Applications}} ({{ICACEA}})},
  author = {Jambukia, Shweta H. and Dabhi, Vipul K. and Prajapati, Harshadkumar B.},
  year = {2015},
  month = mar,
  pages = {714--721},
  publisher = {IEEE},
  doi = {10.1109/ICACEA.2015.7164783},
  urldate = {2018-10-21},
  abstract = {Classification of electrocardiogram (ECG) signals plays an important role in diagnoses of heart diseases. An accurate ECG classification is a challenging problem. A survey of ECG classification into arrhythmia types is presented in this paper. Early and accurate detection of arrhythmia types is important in detecting heart diseases and finding treatment of a patient. Different classifiers are available for ECG classification. Amongst all classifiers, artificial neural networks have become very popular and most widely used for ECG classification. In this paper a detailed survey of preprocessing techniques, ECG databases, feature extraction techniques, classifiers and performance measures are presented. This paper also discusses issues in ECG classification, analysis of input beat selection, and output of classifiers.},
  isbn = {978-1-4673-6911-4},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YVIB7ZCP/jambukia_classifica_2015.pdf}
}

@article{james_global_2018,
  title = {Global, Regional, and National Incidence, Prevalence, and Years Lived with Disability for 354 Diseases and Injuries for 195 Countries and Territories, 1990--2017: A Systematic Analysis for the {{Global Burden}} of {{Disease Study}} 2017},
  shorttitle = {Global, Regional, and National Incidence, Prevalence, and Years Lived with Disability for 354 Diseases and Injuries for 195 Countries and Territories, 1990--2017},
  author = {James, Spencer L and Abate, Degu and Abate, Kalkidan Hassen and Abay, Solomon M and Abbafati, Cristiana and Abbasi, Nooshin and Abbastabar, Hedayat and {Abd-Allah}, Foad and Abdela, Jemal and Abdelalim, Ahmed and Abdollahpour, Ibrahim and Abdulkader, Rizwan Suliankatchi and Abebe, Zegeye and Abera, Semaw F and Abil, Olifan Zewdie and Abraha, Haftom Niguse and {Abu-Raddad}, Laith Jamal and {Abu-Rmeileh}, Niveen M E and Accrombessi, Manfred Mario Kokou and Acharya, Dilaram and Acharya, Pawan and Ackerman, Ilana N and Adamu, Abdu A and Adebayo, Oladimeji M and Adekanmbi, Victor and Adetokunboh, Olatunji O and Adib, Mina G and Adsuar, Jose C and Afanvi, Kossivi Agbelenko and Afarideh, Mohsen and Afshin, Ashkan and Agarwal, Gina and Agesa, Kareha M and Aggarwal, Rakesh and Aghayan, Sargis Aghasi and Agrawal, Sutapa and Ahmadi, Alireza and Ahmadi, Mehdi and Ahmadieh, Hamid and Ahmed, Muktar Beshir and Aichour, Amani Nidhal and Aichour, Ibtihel and Aichour, Miloud Taki Eddine and Akinyemiju, Tomi and Akseer, Nadia and {Al-Aly}, Ziyad and {Al-Eyadhy}, Ayman and {Al-Mekhlafi}, Hesham M and {Al-Raddadi}, Rajaa M and Alahdab, Fares and Alam, Khurshid and Alam, Tahiya and Alashi, Alaa and Alavian, Seyed Moayed and Alene, Kefyalew Addis and Alijanzadeh, Mehran and {Alizadeh-Navaei}, Reza and Aljunid, Syed Mohamed and Alkerwi, Ala'a and Alla, Fran{\c c}ois and Allebeck, Peter and Alouani, Mohamed M L and Altirkawi, Khalid and {Alvis-Guzman}, Nelson and Amare, Azmeraw T and Aminde, Leopold N and Ammar, Walid and Amoako, Yaw Ampem and Anber, Nahla Hamed and Andrei, Catalina Liliana and Androudi, Sofia and Animut, Megbaru Debalkie and Anjomshoa, Mina and Ansha, Mustafa Geleto and Antonio, Carl Abelardo T and Anwari, Palwasha and Arabloo, Jalal and Arauz, Antonio and Aremu, Olatunde and Ariani, Filippo and Armoon, Bahroom and {\"A}rnl{\"o}v, Johan and Arora, Amit and Artaman, Al and Aryal, Krishna K and Asayesh, Hamid and Asghar, Rana Jawad and Ataro, Zerihun and Atre, Sachin R and Ausloos, Marcel and {Avila-Burgos}, Leticia and Avokpaho, Euripide F G A and Awasthi, Ashish and Ayala Quintanilla, Beatriz Paulina and Ayer, Rakesh and Azzopardi, Peter S and Babazadeh, Arefeh and Badali, Hamid and Badawi, Alaa and Bali, Ayele Geleto and Ballesteros, Katherine E and Ballew, Shoshana H and Banach, Maciej and Banoub, Joseph Adel Mattar and Banstola, Amrit and Barac, Aleksandra and Barboza, Miguel A and {Barker-Collo}, Suzanne Lyn and B{\"a}rnighausen, Till Winfried and Barrero, Lope H and Baune, Bernhard T and {Bazargan-Hejazi}, Shahrzad and Bedi, Neeraj and Beghi, Ettore and Behzadifar, Masoud and Behzadifar, Meysam and B{\'e}jot, Yannick and Belachew, Abate Bekele and Belay, Yihalem Abebe and Bell, Michelle L and Bello, Aminu K and Bensenor, Isabela M and Bernabe, Eduardo and Bernstein, Robert S and Beuran, Mircea and Beyranvand, Tina and Bhala, Neeraj and Bhattarai, Suraj and Bhaumik, Soumyadeep and Bhutta, Zulfiqar A and Biadgo, Belete and Bijani, Ali and Bikbov, Boris and Bilano, Ver and Bililign, Nigus and Bin Sayeed, Muhammad Shahdaat and Bisanzio, Donal and Blacker, Brigette F and Blyth, Fiona M and {Bou-Orm}, Ibrahim R and Boufous, Soufiane and Bourne, Rupert and Brady, Oliver J and Brainin, Michael and Brant, Luisa C and Brazinova, Alexandra and Breitborde, Nicholas J K and Brenner, Hermann and Briant, Paul Svitil and Briggs, Andrew M and Briko, Andrey Nikolaevich and Britton, Gabrielle and Brugha, Traolach and Buchbinder, Rachelle and Busse, Reinhard and Butt, Zahid A and {Cahuana-Hurtado}, Lucero and Cano, Jorge and C{\'a}rdenas, Rosario and Carrero, Juan J and Carter, Austin and Carvalho, F{\'e}lix and {Casta{\~n}eda-Orjuela}, Carlos A and Castillo Rivas, Jacqueline and Castro, Franz and {Catal{\'a}-L{\'o}pez}, Ferr{\'a}n and Cercy, Kelly M and Cerin, Ester and Chaiah, Yazan and Chang, Alex R and Chang, Hsing-Yi and Chang, Jung-Chen and Charlson, Fiona J and Chattopadhyay, Aparajita and Chattu, Vijay Kumar and Chaturvedi, Pankaj and Chiang, Peggy Pei-Chia and Chin, Ken Lee and Chitheer, Abdulaal and Choi, Jee-Young J and Chowdhury, Rajiv and Christensen, Hanne and Christopher, Devasahayam J and Cicuttini, Flavia M and Ciobanu, Liliana G and Cirillo, Massimo and Claro, Rafael M and {Collado-Mateo}, Daniel and Cooper, Cyrus and Coresh, Josef and Cortesi, Paolo Angelo and Cortinovis, Monica and Costa, Megan and Cousin, Ewerton and Criqui, Michael H and Cromwell, Elizabeth A and Cross, Marita and Crump, John A and Dadi, Abel Fekadu and Dandona, Lalit and Dandona, Rakhi and Dargan, Paul I and Daryani, Ahmad and Das Gupta, Rajat and Das Neves, Jos{\'e} and Dasa, Tamirat Tesfaye and Davey, Gail and Davis, Adrian C and Davitoiu, Dragos Virgil and De Courten, Barbora and De La Hoz, Fernando Pio and De Leo, Diego and De Neve, Jan-Walter and Degefa, Meaza Girma and Degenhardt, Louisa and Deiparine, Selina and Dellavalle, Robert P and Demoz, Gebre Teklemariam and Deribe, Kebede and Dervenis, Nikolaos and Des Jarlais, Don C and Dessie, Getenet Ayalew and Dey, Subhojit and Dharmaratne, Samath Dhamminda and Dinberu, Mesfin Tadese and Dirac, M Ashworth and Djalalinia, Shirin and Doan, Linh and Dokova, Klara and Doku, David Teye and Dorsey, E Ray and Doyle, Kerrie E and Driscoll, Tim Robert and Dubey, Manisha and Dubljanin, Eleonora and Duken, Eyasu Ejeta and Duncan, Bruce B and Duraes, Andre R and Ebrahimi, Hedyeh and Ebrahimpour, Soheil and Echko, Michelle Marie and Edvardsson, David and Effiong, Andem and Ehrlich, Joshua R and El Bcheraoui, Charbel and El Sayed Zaki, Maysaa and {El-Khatib}, Ziad and Elkout, Hajer and Elyazar, Iqbal R F and Enayati, Ahmadali and Endries, Aman Yesuf and Er, Benjamin and Erskine, Holly E and Eshrati, Babak and Eskandarieh, Sharareh and Esteghamati, Alireza and Esteghamati, Sadaf and Fakhim, Hamed and Fallah Omrani, Vahid and Faramarzi, Mahbobeh and Fareed, Mohammad and Farhadi, Farzaneh and Farid, Talha A and s{\'a} Farinha, Carla Sofia E and Farioli, Andrea and Faro, Andre and Farvid, Maryam S and Farzadfar, Farshad and Feigin, Valery L and Fentahun, Netsanet and Fereshtehnejad, Seyed-Mohammad and Fernandes, Eduarda and Fernandes, Joao C and Ferrari, Alize J and Feyissa, Garumma Tolu and Filip, Irina and Fischer, Florian and Fitzmaurice, Christina and Foigt, Nataliya A and Foreman, Kyle J and Fox, Jack and Frank, Tahvi D and Fukumoto, Takeshi and Fullman, Nancy and F{\"u}rst, Thomas and Furtado, Jo{\~a}o M and Futran, Neal D and Gall, Seana and Ganji, Morsaleh and Gankpe, Fortune Gbetoho and {Garcia-Basteiro}, Alberto L and Gardner, William M and Gebre, Abadi Kahsu and Gebremedhin, Amanuel Tesfay and Gebremichael, Teklu Gebrehiwo and Gelano, Tilayie Feto and Geleijnse, Johanna M and {Genova-Maleras}, Ricard and Geramo, Yilma Chisha Dea and Gething, Peter W and Gezae, Kebede Embaye and Ghadiri, Keyghobad and Ghasemi Falavarjani, Khalil and {Ghasemi-Kasman}, Maryam and Ghimire, Mamata and Ghosh, Rakesh and Ghoshal, Aloke Gopal and Giampaoli, Simona and Gill, Paramjit Singh and Gill, Tiffany K and Ginawi, Ibrahim Abdelmageed and Giussani, Giorgia and Gnedovskaya, Elena V and Goldberg, Ellen M and Goli, Srinivas and {G{\'o}mez-Dant{\'e}s}, Hector and Gona, Philimon N and Gopalani, Sameer Vali and Gorman, Taren M and Goulart, Alessandra C and Goulart, B{\'a}rbara Niegia Garcia and Grada, Ayman and Grams, Morgan E and Grosso, Giuseppe and Gugnani, Harish Chander and Guo, Yuming and Gupta, Prakash C and Gupta, Rahul and Gupta, Rajeev and Gupta, Tanush and Gyawali, Bishal and Haagsma, Juanita A and Hachinski, Vladimir and {Hafezi-Nejad}, Nima and Haghparast Bidgoli, Hassan and Hagos, Tekleberhan B and Hailu, Gessessew Bugssa and {Haj-Mirzaian}, Arvin and {Haj-Mirzaian}, Arya and Hamadeh, Randah R and Hamidi, Samer and Handal, Alexis J and Hankey, Graeme J and Hao, Yuantao and Harb, Hilda L and Harikrishnan, Sivadasanpillai and Haro, Josep Maria and Hasan, Mehedi and Hassankhani, Hadi and Hassen, Hamid Yimam and Havmoeller, Rasmus and Hawley, Caitlin N and Hay, Roderick J and Hay, Simon I and {Hedayatizadeh-Omran}, Akbar and Heibati, Behzad and Hendrie, Delia and Henok, Andualem and Herteliu, Claudiu and Heydarpour, Sousan and Hibstu, Desalegn Tsegaw and Hoang, Huong Thanh and Hoek, Hans W and Hoffman, Howard J and Hole, Michael K and Homaie Rad, Enayatollah and Hoogar, Praveen and Hosgood, H Dean and Hosseini, Seyed Mostafa and Hosseinzadeh, Mehdi and Hostiuc, Mihaela and Hostiuc, Sorin and Hotez, Peter J and Hoy, Damian G and Hsairi, Mohamed and Htet, Aung Soe and Hu, Guoqing and Huang, John J and Huynh, Chantal K and Iburg, Kim Moesgaard and Ikeda, Chad Thomas and Ileanu, Bogdan and Ilesanmi, Olayinka Stephen and Iqbal, Usman and Irvani, Seyed Sina Naghibi and Irvine, Caleb Mackay Salpeter and Islam, Sheikh Mohammed Shariful and Islami, Farhad and Jacobsen, Kathryn H and Jahangiry, Leila and Jahanmehr, Nader and Jain, Sudhir Kumar and Jakovljevic, Mihajlo and Javanbakht, Mehdi and Jayatilleke, Achala Upendra and Jeemon, Panniyammakal and Jha, Ravi Prakash and Jha, Vivekanand and Ji, John S and Johnson, Catherine O and Jonas, Jost B and Jozwiak, Jacek Jerzy and Jungari, Suresh Banayya and J{\"u}risson, Mikk and Kabir, Zubair and Kadel, Rajendra and Kahsay, Amaha and Kalani, Rizwan and Kanchan, Tanuj and Karami, Manoochehr and Karami Matin, Behzad and Karch, Andr{\'e} and Karema, Corine and Karimi, Narges and Karimi, Seyed M and Kasaeian, Amir and Kassa, Dessalegn H and Kassa, Getachew Mullu and Kassa, Tesfaye Dessale and Kassebaum, Nicholas J and Katikireddi, Srinivasa Vittal and Kawakami, Norito and Karyani, Ali Kazemi and Keighobadi, Masoud Masoud and Keiyoro, Peter Njenga and Kemmer, Laura and Kemp, Grant Rodgers and Kengne, Andre Pascal and Keren, Andre and Khader, Yousef Saleh and Khafaei, Behzad and Khafaie, Morteza Abdullatif and Khajavi, Alireza and Khalil, Ibrahim A and Khan, Ejaz Ahmad and Khan, Muhammad Shahzeb and Khan, Muhammad Ali and Khang, Young-Ho and Khazaei, Mohammad and Khoja, Abdullah T and Khosravi, Ardeshir and Khosravi, Mohammad Hossein and Kiadaliri, Aliasghar A and Kiirithio, Daniel N and Kim, Cho-Il and Kim, Daniel and Kim, Pauline and Kim, Young-Eun and Kim, Yun Jin and Kimokoti, Ruth W and Kinfu, Yohannes and Kisa, Adnan and {Kissimova-Skarbek}, Katarzyna and Kivim{\"a}ki, Mika and Knudsen, Ann Kristin Skrindo and Kocarnik, Jonathan M and Kochhar, Sonali and Kokubo, Yoshihiro and Kolola, Tufa and Kopec, Jacek A and Kosen, Soewarta and Kotsakis, Georgios A and Koul, Parvaiz A and Koyanagi, Ai and Kravchenko, Michael A and Krishan, Kewal and Krohn, Kristopher J and Kuate Defo, Barthelemy and Kucuk Bicer, Burcu and Kumar, G Anil and Kumar, Manasi and Kyu, Hmwe Hmwe and Lad, Deepesh P and Lad, Sheetal D and Lafranconi, Alessandra and Lalloo, Ratilal and Lallukka, Tea and Lami, Faris Hasan and Lansingh, Van C and Latifi, Arman and Lau, Kathryn Mei-Ming and Lazarus, Jeffrey V and Leasher, Janet L and Ledesma, Jorge R and Lee, Paul H and Leigh, James and Leung, Janni and Levi, Miriam and Lewycka, Sonia and Li, Shanshan and Li, Yichong and Liao, Yu and Liben, Misgan Legesse and Lim, Lee-Ling and Lim, Stephen S and Liu, Shiwei and Lodha, Rakesh and Looker, Katharine J and Lopez, Alan D and Lorkowski, Stefan and Lotufo, Paulo A and Low, Nicola and Lozano, Rafael and Lucas, Tim C D and Lucchesi, Lydia R and Lunevicius, Raimundas and Lyons, Ronan A and Ma, Stefan and Macarayan, Erlyn Rachelle King and Mackay, Mark T and Madotto, Fabiana and Magdy Abd El Razek, Hassan and Magdy Abd El Razek, Muhammed and Maghavani, Dhaval P and Mahotra, Narayan Bahadur and Mai, Hue Thi and Majdan, Marek and Majdzadeh, Reza and Majeed, Azeem and Malekzadeh, Reza and Malta, Deborah Carvalho and Mamun, Abdullah A and Manda, Ana-Laura and Manguerra, Helena and Manhertz, Treh and Mansournia, Mohammad Ali and Mantovani, Lorenzo Giovanni and Mapoma, Chabila Christopher and Maravilla, Joemer C and Marcenes, Wagner and Marks, Ashley and {Martins-Melo}, Francisco Rogerl{\^a}ndio and Martopullo, Ira and M{\"a}rz, Winfried and Marzan, Melvin B and {Mashamba-Thompson}, Tivani Phosa and Massenburg, Benjamin Ballard and Mathur, Manu Raj and Matsushita, Kunihiro and Maulik, Pallab K and Mazidi, Mohsen and McAlinden, Colm and McGrath, John J and McKee, Martin and Mehndiratta, Man Mohan and Mehrotra, Ravi and Mehta, Kala M and Mehta, Varshil and {Mejia-Rodriguez}, Fabiola and Mekonen, Tesfa and Melese, Addisu and Melku, Mulugeta and Meltzer, Michele and Memiah, Peter T N and Memish, Ziad A and Mendoza, Walter and Mengistu, Desalegn Tadese and Mengistu, Getnet and Mensah, George A and Mereta, Seid Tiku and Meretoja, Atte and Meretoja, Tuomo J and Mestrovic, Tomislav and Mezerji, Naser Mohammad Gholi and Miazgowski, Bartosz and Miazgowski, Tomasz and Millear, Anoushka I and Miller, Ted R and Miltz, Benjamin and Mini, G K and Mirarefin, Mojde and Mirrakhimov, Erkin M and Misganaw, Awoke Temesgen and Mitchell, Philip B and Mitiku, Habtamu and Moazen, Babak and Mohajer, Bahram and Mohammad, Karzan Abdulmuhsin and Mohammadifard, Noushin and {Mohammadnia-Afrouzi}, Mousa and Mohammed, Mohammed A and Mohammed, Shafiu and Mohebi, Farnam and Moitra, Modhurima and Mokdad, Ali H and Molokhia, Mariam and Monasta, Lorenzo and Moodley, Yoshan and Moosazadeh, Mahmood and Moradi, Ghobad and {Moradi-Lakeh}, Maziar and Moradinazar, Mehdi and Moraga, Paula and Morawska, Lidia and Moreno Vel{\'a}squez, Ilais and {Morgado-Da-Costa}, Joana and Morrison, Shane Douglas and Moschos, Marilita M and {Mountjoy-Venning}, W Cliff and Mousavi, Seyyed Meysam and Mruts, Kalayu Brhane and Muche, Achenef Asmamaw and Muchie, Kindie Fentahun and Mueller, Ulrich Otto and Muhammed, Oumer Sada and Mukhopadhyay, Satinath and Muller, Kate and Mumford, John Everett and Murhekar, Manoj and Musa, Jonah and Musa, Kamarul Imran and Mustafa, Ghulam and Nabhan, Ashraf F and Nagata, Chie and Naghavi, Mohsen and Naheed, Aliya and Nahvijou, Azin and Naik, Gurudatta and Naik, Nitish and Najafi, Farid and Naldi, Luigi and Nam, Hae Sung and Nangia, Vinay and Nansseu, Jobert Richie and Nascimento, Bruno Ramos and Natarajan, Gopalakrishnan and Neamati, Nahid and Negoi, Ionut and Negoi, Ruxandra Irina and Neupane, Subas and Newton, Charles Richard James and Ngunjiri, Josephine W and Nguyen, Anh Quynh and Nguyen, Ha Thu and Nguyen, Huong Lan Thi and Nguyen, Huong Thanh and Nguyen, Long Hoang and Nguyen, Minh and Nguyen, Nam Ba and Nguyen, Son Hoang and Nichols, Emma and Ningrum, Dina Nur Anggraini and Nixon, Molly R and Nolutshungu, Nomonde and Nomura, Shuhei and Norheim, Ole F and Noroozi, Mehdi and Norrving, Bo and Noubiap, Jean Jacques and Nouri, Hamid Reza and Nourollahpour Shiadeh, Malihe and Nowroozi, Mohammad Reza and Nsoesie, Elaine O and Nyasulu, Peter S and Odell, Christopher M and {Ofori-Asenso}, Richard and Ogbo, Felix Akpojene and Oh, In-Hwan and Oladimeji, Olanrewaju and Olagunju, Andrew T and Olagunju, Tinuke O and Olivares, Pedro R and Olsen, Helen Elizabeth and Olusanya, Bolajoko Olubukunola and Ong, Kanyin L and Ong, Sok King and Oren, Eyal and Ortiz, Alberto and Ota, Erika and Otstavnov, Stanislav S and {\O}verland, Simon and Owolabi, Mayowa Ojo and {P a}, Mahesh and Pacella, Rosana and Pakpour, Amir H and Pana, Adrian and {Panda-Jonas}, Songhomitra and Parisi, Andrea and Park, Eun-Kee and Parry, Charles D H and Patel, Shanti and Pati, Sanghamitra and Patil, Snehal T and Patle, Ajay and Patton, George C and Paturi, Vishnupriya Rao and Paulson, Katherine R and Pearce, Neil and Pereira, David M and Perico, Norberto and Pesudovs, Konrad and Pham, Hai Quang and Phillips, Michael R and Pigott, David M and Pillay, Julian David and Piradov, Michael A and Pirsaheb, Meghdad and Pishgar, Farhad and {Plana-Ripoll}, Oleguer and Plass, Dietrich and Polinder, Suzanne and Popova, Svetlana and Postma, Maarten J and Pourshams, Akram and Poustchi, Hossein and Prabhakaran, Dorairaj and Prakash, Swayam and Prakash, V and Purcell, Caroline A and Purwar, Manorama B and Qorbani, Mostafa and Quistberg, D Alex and Radfar, Amir and Rafay, Anwar and Rafiei, Alireza and Rahim, Fakher and Rahimi, Kazem and {Rahimi-Movaghar}, Afarin and {Rahimi-Movaghar}, Vafa and Rahman, Mahfuzar and ur Rahman, Mohammad Hifz and Rahman, Muhammad Aziz and Rahman, Sajjad Ur and Rai, Rajesh Kumar and Rajati, Fatemeh and Ram, Usha and Ranjan, Prabhat and Ranta, Anna and Rao, Puja C and Rawaf, David Laith and Rawaf, Salman and Reddy, K Srinath and Reiner, Robert C and Reinig, Nickolas and Reitsma, Marissa Bettay and Remuzzi, Giuseppe and Renzaho, Andre M N and Resnikoff, Serge and Rezaei, Satar and Rezai, Mohammad Sadegh and Ribeiro, Antonio Luiz P and Roberts, Nicholas L S and Robinson, Stephen R and Roever, Leonardo and Ronfani, Luca and Roshandel, Gholamreza and Rostami, Ali and Roth, Gregory A and Roy, Ambuj and Rubagotti, Enrico and Sachdev, Perminder S and Sadat, Nafis and Saddik, Basema and Sadeghi, Ehsan and Saeedi Moghaddam, Sahar and Safari, Hosein and Safari, Yahya and {Safari-Faramani}, Roya and Safdarian, Mahdi and Safi, Sare and Safiri, Saeid and Sagar, Rajesh and Sahebkar, Amirhossein and Sahraian, Mohammad Ali and Sajadi, Haniye Sadat and Salam, Nasir and Salama, Joseph S and Salamati, Payman and Saleem, Komal and Saleem, Zikria and Salimi, Yahya and Salomon, Joshua A and Salvi, Sundeep Santosh and Salz, Inbal and Samy, Abdallah M and Sanabria, Juan and Sang, Yingying and Santomauro, Damian Francesco and Santos, Itamar S and Santos, Jo{\~a}o Vasco and Santric Milicevic, Milena M and Sao Jose, Bruno Piassi and Sardana, Mayank and Sarker, Abdur Razzaque and Sarrafzadegan, Nizal and Sartorius, Benn and Sarvi, Shahabeddin and Sathian, Brijesh and Satpathy, Maheswar and Sawant, Arundhati R and Sawhney, Monika and Saxena, Sonia and Saylan, Mete and Schaeffner, Elke and Schmidt, Maria In{\^e}s and Schneider, Ione J C and Sch{\"o}ttker, Ben and Schwebel, David C and Schwendicke, Falk and Scott, James G and Sekerija, Mario and Sepanlou, Sadaf G and {Serv{\'a}n-Mori}, Edson and Seyedmousavi, Seyedmojtaba and Shabaninejad, Hosein and Shafieesabet, Azadeh and Shahbazi, Mehdi and Shaheen, Amira A and Shaikh, Masood Ali and {Shams-Beyranvand}, Mehran and Shamsi, Mohammadbagher and Shamsizadeh, Morteza and Sharafi, Heidar and Sharafi, Kiomars and Sharif, Mehdi and {Sharif-Alhoseini}, Mahdi and Sharma, Meenakshi and Sharma, Rajesh and She, Jun and Sheikh, Aziz and Shi, Peilin and Shibuya, Kenji and Shigematsu, Mika and Shiri, Rahman and Shirkoohi, Reza and Shishani, Kawkab and Shiue, Ivy and Shokraneh, Farhad and Shoman, Haitham and Shrime, Mark G and Si, Si and Siabani, Soraya and Siddiqi, Tariq J and Sigfusdottir, Inga Dora and Sigurvinsdottir, Rannveig and Silva, Jo{\~a}o Pedro and Silveira, Dayane Gabriele Alves and Singam, Narayana Sarma Venkata and Singh, Jasvinder A and Singh, Narinder Pal and Singh, Virendra and Sinha, Dhirendra Narain and Skiadaresi, Eirini and Slepak, Erica Leigh N and Sliwa, Karen and Smith, David L and Smith, Mari and Soares Filho, Adauto Martins and Sobaih, Badr Hasan and Sobhani, Soheila and Sobngwi, Eug{\`e}ne and Soneji, Samir S and Soofi, Moslem and Soosaraei, Masoud and Sorensen, Reed J D and Soriano, Joan B and Soyiri, Ireneous N and Sposato, Luciano A and Sreeramareddy, Chandrashekhar T and Srinivasan, Vinay and Stanaway, Jeffrey D and Stein, Dan J and Steiner, Caitlyn and Steiner, Timothy J and Stokes, Mark A and Stovner, Lars Jacob and Subart, Michelle L and Sudaryanto, Agus and Sufiyan, Mu'awiyyah Babale and Sunguya, Bruno F and Sur, Patrick John and Sutradhar, Ipsita and Sykes, Bryan L and Sylte, Dillon O and {Tabar{\'e}s-Seisdedos}, Rafael and Tadakamadla, Santosh Kumar and Tadesse, Birkneh Tilahun and Tandon, Nikhil and Tassew, Segen Gebremeskel and Tavakkoli, Mohammad and Taveira, Nuno and Taylor, Hugh R and {Tehrani-Banihashemi}, Arash and Tekalign, Tigist Gashaw and Tekelemedhin, Shishay Wahdey and Tekle, Merhawi Gebremedhin and Temesgen, Habtamu and Temsah, Mohamad-Hani and Temsah, Omar and Terkawi, Abdullah Sulieman and Teweldemedhin, Mebrahtu and Thankappan, Kavumpurathu Raman and Thomas, Nihal and Tilahun, Binyam and To, Quyen G and Tonelli, Marcello and {Topor-Madry}, Roman and Topouzis, Fotis and Torre, Anna E and {Tortajada-Girb{\'e}s}, Miguel and Touvier, Mathilde and {Tovani-Palone}, Marcos Roberto and Towbin, Jeffrey A and Tran, Bach Xuan and Tran, Khanh Bao and Troeger, Christopher E and Truelsen, Thomas Clement and Tsilimbaris, Miltiadis K and Tsoi, Derrick and Tudor Car, Lorainne and Tuzcu, E Murat and Ukwaja, Kingsley N and Ullah, Irfan and Undurraga, Eduardo A and Unutzer, Jurgen and Updike, Rachel L and Usman, Muhammad Shariq and Uthman, Olalekan A and Vaduganathan, Muthiah and Vaezi, Afsane and Valdez, Pascual R and Varughese, Santosh and Vasankari, Tommi Juhani and Venketasubramanian, Narayanaswamy and Villafaina, Santos and Violante, Francesco S and Vladimirov, Sergey Konstantinovitch and Vlassov, Vasily and Vollset, Stein Emil and Vosoughi, Kia and Vujcic, Isidora S and Wagnew, Fasil Shiferaw and Waheed, Yasir and Waller, Stephen G and Wang, Yafeng and Wang, Yuan-Pang and Weiderpass, Elisabete and Weintraub, Robert G and Weiss, Daniel J and Weldegebreal, Fitsum and Weldegwergs, Kidu Gidey and Werdecker, Andrea and West, T Eoin and Whiteford, Harvey A and Widecka, Justyna and Wijeratne, Tissa and Wilner, Lauren B and Wilson, Shadrach and Winkler, Andrea Sylvia and Wiyeh, Alison B and Wiysonge, Charles Shey and Wolfe, Charles D A and Woolf, Anthony D and Wu, Shouling and Wu, Yun-Chun and Wyper, Grant M A and Xavier, Denis and Xu, Gelin and Yadgir, Simon and Yadollahpour, Ali and Yahyazadeh Jabbari, Seyed Hossein and Yamada, Tomohide and Yan, Lijing L and Yano, Yuichiro and Yaseri, Mehdi and Yasin, Yasin Jemal and Yeshaneh, Alex and Yimer, Ebrahim M and Yip, Paul and Yisma, Engida and Yonemoto, Naohiro and Yoon, Seok-Jun and Yotebieng, Marcel and Younis, Mustafa Z and Yousefifard, Mahmoud and Yu, Chuanhua and Zadnik, Vesna and Zaidi, Zoubida and Zaman, Sojib Bin and Zamani, Mohammad and Zare, Zohreh and Zeleke, Ayalew Jejaw and Zenebe, Zerihun Menlkalew and Zhang, Kai and Zhao, Zheng and Zhou, Maigeng and Zodpey, Sanjay and Zucker, Inbar and Vos, Theo and Murray, Christopher J L},
  year = {2018},
  month = nov,
  journal = {The Lancet},
  volume = {392},
  number = {10159},
  pages = {1789--1858},
  issn = {0140-6736},
  doi = {10.1016/S0140-6736(18)32279-7},
  urldate = {2021-11-25},
  abstract = {Background The Global Burden of Diseases, Injuries, and Risk Factors Study 2017 (GBD 2017) includes a comprehensive assessment of incidence, prevalence, and years lived with disability (YLDs) for 354 causes in 195 countries and territories from 1990 to 2017. Previous GBD studies have shown how the decline of mortality rates from 1990 to 2016 has led to an increase in life expectancy, an ageing global population, and an expansion of the non-fatal burden of disease and injury. These studies have also shown how a substantial portion of the world's population experiences non-fatal health loss with considerable heterogeneity among different causes, locations, ages, and sexes. Ongoing objectives of the GBD study include increasing the level of estimation detail, improving analytical strategies, and increasing the amount of high-quality data. Methods We estimated incidence and prevalence for 354 diseases and injuries and 3484 sequelae. We used an updated and extensive body of literature studies, survey data, surveillance data, inpatient admission records, outpatient visit records, and health insurance claims, and additionally used results from cause of death models to inform estimates using a total of 68\hphantom{,}781 data sources. Newly available clinical data from India, Iran, Japan, Jordan, Nepal, China, Brazil, Norway, and Italy were incorporated, as well as updated claims data from the USA and new claims data from Taiwan (province of China) and Singapore. We used DisMod-MR 2.1, a Bayesian meta-regression tool, as the main method of estimation, ensuring consistency between rates of incidence, prevalence, remission, and cause of death for each condition. YLDs were estimated as the product of a prevalence estimate and a disability weight for health states of each mutually exclusive sequela, adjusted for comorbidity. We updated the Socio-demographic Index (SDI), a summary development indicator of income per capita, years of schooling, and total fertility rate. Additionally, we calculated differences between male and female YLDs to identify divergent trends across sexes. GBD 2017 complies with the Guidelines for Accurate and Transparent Health Estimates Reporting. Findings Globally, for females, the causes with the greatest age-standardised prevalence were oral disorders, headache disorders, and haemoglobinopathies and haemolytic anaemias in both 1990 and 2017. For males, the causes with the greatest age-standardised prevalence were oral disorders, headache disorders, and tuberculosis including latent tuberculosis infection in both 1990 and 2017. In terms of YLDs, low back pain, headache disorders, and dietary iron deficiency were the leading Level 3 causes of YLD counts in 1990, whereas low back pain, headache disorders, and depressive disorders were the leading causes in 2017 for both sexes combined. All-cause age-standardised YLD rates decreased by 3{$\cdot$}9\% (95\% uncertainty interval [UI] 3{$\cdot$}1--4{$\cdot$}6) from 1990 to 2017; however, the all-age YLD rate increased by 7{$\cdot$}2\% (6{$\cdot$}0--8{$\cdot$}4) while the total sum of global YLDs increased from 562 million (421--723) to 853 million (642--1100). The increases for males and females were similar, with increases in all-age YLD rates of 7{$\cdot$}9\% (6{$\cdot$}6--9{$\cdot$}2) for males and 6{$\cdot$}5\% (5{$\cdot$}4--7{$\cdot$}7) for females. We found significant differences between males and females in terms of age-standardised prevalence estimates for multiple causes. The causes with the greatest relative differences between sexes in 2017 included substance use disorders (3018 cases [95\% UI 2782--3252] per 100\hphantom{,}000 in males vs s1400 [1279--1524] per 100\hphantom{,}000 in females), transport injuries (3322 [3082--3583] vs 2336 [2154--2535]), and self-harm and interpersonal violence (3265 [2943--3630] vs 5643 [5057--6302]). Interpretation Global all-cause age-standardised YLD rates have improved only slightly over a period spanning nearly three decades. However, the magnitude of the non-fatal disease burden has expanded globally, with increasing numbers of people who have a wide spectrum of conditions. A subset of conditions has remained globally pervasive since 1990, whereas other conditions have displayed more dynamic trends, with different ages, sexes, and geographies across the globe experiencing varying burdens and trends of health loss. This study emphasises how global improvements in premature mortality for select conditions have led to older populations with complex and potentially expensive diseases, yet also highlights global achievements in certain domains of disease and injury. Funding Bill \& Melinda Gates Foundation.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3LKXJ2R3/James et al_2018_Global, regional, and national incidence, prevalence, and years lived with.pdf}
}

@article{jansson_input_2005,
  title = {Input Design via {{LMIs}} Admitting Frequency-Wise Model Specifications in Confidence Regions},
  author = {Jansson, H. and Hjalmarsson, H.},
  year = {2005},
  month = oct,
  journal = {IEEE Transactions on Automatic Control},
  volume = {50},
  number = {10},
  pages = {1534--1549},
  issn = {0018-9286},
  doi = {10.1109/TAC.2005.856652},
  abstract = {A framework for reformulating input design problems in prediction error identification as convex optimization problems is presented. For linear time-invariant single input/single output systems, this framework unifies and extends existing results on open-loop input design that are based on the finite dimensional asymptotic covariance matrix of the parameter estimates. Basic methods for parametrizing the input spectrum are provided and conditions on these parametrizations that guarantee that all possible covariance matrices for the asymptotic distribution of the parameter estimates can be generated are provided. A wide range of model quality constraints can be handled. In particular, different frequency-by-frequency constraints can be used. This opens up new applications of input design in areas such as robust control. Furthermore, quality specifications can be imposed on all models in a confidence region. Thus, allowing for statements such as "with at least 99\% probability the model quality specifications will be satisfied".},
  keywords = {confidence regions,convex optimization,covariance matrices,covariance matrix,Design optimization,Eigenvalues and eigenfunctions,finite dimensional asymptotic covariance matrix,Frequency,frequency wise model specification,input design,linear matrix inequalities,linear systems,linear time invariant single input single output system,LMI,multidimensional systems,open loop input design,open loop systems,parameter estimation,prediction error identification,Predictive models,robust control,system identification,Transfer functions,Vectors},
  file = {/Users/antoniohortaribeiro/Zotero/storage/F43MW23N/jansson_input_2005.pdf;/Users/antoniohortaribeiro/Zotero/storage/VC4RS53B/1516256.html}
}

@inproceedings{javanmard_precise_2020,
  title = {Precise Tradeoffs in Adversarial Training for Linear Regression},
  booktitle = {Proceedings of the {{Conference}} on {{Learning Theory}}},
  author = {Javanmard, Adel and Soltanolkotabi, Mahdi and Hassani, Hamed},
  year = {2020-07-09/2020-07-12},
  volume = {125},
  pages = {2034--2078},
  abstract = {Despite breakthrough performance, modern learning models are known to be highly vulnerable to small adversarial perturbations in their inputs. While a wide variety of recent \emph{adversarial training} methods have been effective at improving robustness to perturbed inputs (robust accuracy), often this benefit is accompanied by a decrease in accuracy on benign inputs (standard accuracy), leading to a tradeoff between often competing objectives. Complicating matters further, recent empirical evidence suggest that a variety of other factors (size and quality of training data, model size, etc.) affect this tradeoff in somewhat surprising ways. In this paper we provide a precise and comprehensive understanding of the role of adversarial training in the context of linear regression with Gaussian features. In particular, we characterize the fundamental tradeoff between the accuracies achievable by any algorithm regardless of computational power or size of the training data. Furthermore, we precisely characterize the standard/robust accuracy and the corresponding tradeoff achieved by a contemporary mini-max adversarial training approach in a high-dimensional regime where the number of data points and the parameters of the model grow in proportion to each other. Our theory for adversarial training algorithms also facilitates the rigorous study of how a variety of factors (size and quality of training data, model overparametrization etc.) affect the tradeoff between these two competing accuracies.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/M9HXRAWG/Javanmard et al. - 2020 - Precise Tradeoffs in Adversarial Training for Line.pdf}
}

@article{javanmard_precise_2022,
  title = {Precise Statistical Analysis of Classification Accuracies for Adversarial Training},
  author = {Javanmard, Adel and Soltanolkotabi, Mahdi},
  year = {2022},
  journal = {The Annals of Statistics},
  volume = {50},
  number = {4},
  pages = {2127--2156},
  publisher = {Institute of Mathematical Statistics},
  doi = {10.1214/22-AOS2180},
  keywords = {adversarial training,Binary classification,Precise high-dimensional asymptotics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CHCGE2C3/Javanmard_Soltanolkotabi_2020_Precise Statistical Analysis of Classification Accuracies for Adversarial.pdf}
}

@article{jeffreys_invariant_1946,
  title = {An {{Invariant Form}} for the {{Prior Probability}} in {{Estimation Problems}}},
  author = {Jeffreys, H.},
  year = {1946},
  month = sep,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {186},
  number = {1007},
  pages = {453--461},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.1946.0056},
  urldate = {2018-10-07},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TPZP8BXI/jeffreys_an_1946.pdf}
}

@article{jidling_screening_2023,
  title = {Screening for {{Chagas}} Disease from the Electrocardiogram Using a Deep Neural Network},
  author = {Jidling, Carl and Gedon, Daniel and Sch{\"o}n, Thomas B. and Oliveira, Claudia Di Lorenzo and Cardos, Clareci Silva and Ferreira, Ariela Mota and Giatti, Luana and Barreto, Sandhi Maria and Sabino, Ester C. and Ribeiro, Ant{\^o}nio L. P. and Ribeiro, Ant{\^o}nio H.},
  year = {2023},
  journal = {Plos Neglected Tropical Diseases},
  volume = {17},
  number = {7},
  doi = {10.1371/journal.pntd.0011118},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZZG8GWG9/queue.html}
}

@article{jin_fpga_2010,
  title = {{{FPGA}} Design and Implementation of a Real-Time Stereo Vision System},
  author = {Jin, Seunghun and Cho, Junguk and Pham, Xuan Dai and Lee, Kyoung Mu and Park, S-K and Kim, Munsang and Jeon, Jae Wook},
  year = {2010},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  volume = {20},
  number = {1},
  pages = {15--26},
  keywords = {ðŸ”No DOI found}
}

@article{jing_tunable_2017,
  title = {Tunable {{Efficient Unitary Neural Networks}} ({{EUNN}}) and Their Application to {{RNNs}}},
  author = {Jing, Li and Shen, Yichen and Dubcek, Tena and Peurifoy, John and Skirlo, Scott and LeCun, Yann and Tegmark, Max and Soljacic, Marin},
  year = {2017},
  journal = {Proceedings of the 34 th International Conference on Machine Learning},
  pages = {9},
  abstract = {Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merely O(1) per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixelpermuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.},
  langid = {english},
  keywords = {â›” No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7XDYGBKJ/Jing et al. - Tunable Efficient Unitary Neural Networks (EUNN) a.pdf}
}

@article{johnstone_pca_2018,
  title = {{{PCA}} in {{High Dimensions}}: {{An Orientation}}},
  shorttitle = {{{PCA}} in {{High Dimensions}}},
  author = {Johnstone, Iain M. and Paul, Debashis},
  year = {2018},
  month = aug,
  journal = {Proceedings of the IEEE},
  volume = {106},
  number = {8},
  pages = {1277--1292},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2018.2846730},
  abstract = {When the data are high dimensional, widely used multivariate statistical methods such as principal component analysis can behave in unexpected ways. In settings where the dimension of the observations is comparable to the sample size, upward bias in sample eigenvalues and inconsistency of sample eigenvectors are among the most notable phenomena that appear. These phenomena, and the limiting behavior of the rescaled extreme sample eigenvalues, have recently been investigated in detail under the spiked covariance model. The behavior of the bulk of the sample eigenvalues under weak distributional assumptions on the observations has been described. These results have been exploited to develop new estimation and hypothesis testing methods for the population covariance matrix. Furthermore, partly in response to these phenomena, alternative classes of estimation procedures have been developed by exploiting sparsity of the eigenvectors or the covariance matrix. This paper gives an orientation to these areas.},
  keywords = {Covariance matrices,Eigenvalues and eigenfunctions,Estimation,Marcenko--Pastur distribution,Matrix decomposition,phase transition phenomena,Principal component analysis,principal component analysis (PCA),random matrix theory,spiked covariance model,Statistical analysis,Tracy--Widom law},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9B7M2W23/Johnstone and Paul - 2018 - PCA in High Dimensions An Orientation.pdf;/Users/antoniohortaribeiro/Zotero/storage/WV5JIW32/8412585.html}
}

@book{jones_scipy_2001,
  title = {{{SciPy}}: {{Open}} Source Scientific Tools for {{Python}}},
  author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu and others},
  year = {2001/},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5RITEDCW/scholar (14).ris;/Users/antoniohortaribeiro/Zotero/storage/9X3SVV55/scholar (14).ris}
}

@book{jung_op_2006,
  title = {Op {{Amp}} Applications Handbook},
  editor = {Jung, Walter G.},
  year = {2006},
  series = {Analog {{Devices}} Series},
  publisher = {Newnes},
  address = {Burlington, MA},
  isbn = {978-0-7506-7844-5},
  lccn = {TK7871.58.O6 O62 2006},
  keywords = {{Handbooks, manuals, etc},Operational amplifiers},
  annotation = {OCLC: ocm55633774},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9377J5BU/jung_op amp_2006.pdf}
}

@article{kadlec_datadriven_2009,
  title = {Data-Driven {{Soft Sensors}} in the Process Industry},
  author = {Kadlec, Petr and Gabrys, Bogdan and Strandt, Sibylle},
  year = {2009},
  month = apr,
  journal = {Computers \& Chemical Engineering},
  volume = {33},
  number = {4},
  pages = {795--814},
  issn = {00981354},
  doi = {10.1016/j.compchemeng.2008.12.012},
  urldate = {2017-12-24},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VXZ7ETBG/kadlec_data-drive_2009.pdf}
}

@inproceedings{kalarot_comparison_2010,
  title = {Comparison of {{FPGA}} and {{GPU}} Implementations of Real-Time Stereo Vision},
  booktitle = {Computer {{Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}}), 2010 {{IEEE Computer Society Conference}} On},
  author = {Kalarot, Ratheesh and Morris, John},
  year = {2010},
  pages = {9--15},
  publisher = {IEEE}
}

@article{kalchbrenner_neural_2016,
  title = {Neural {{Machine Translation}} in {{Linear Time}}},
  author = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and van den Oord, Aaron and Graves, Alex and Kavukcuoglu, Koray},
  year = {2016},
  month = oct,
  journal = {arXiv:1610.10099 [cs]},
  eprint = {1610.10099},
  primaryclass = {cs},
  urldate = {2019-03-13},
  abstract = {We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {â›” No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/A3QBCMLK/Kalchbrenner et al. - 2016 - Neural Machine Translation in Linear Time.pdf}
}

@article{kamaleswaran_robust_2018,
  title = {A Robust Deep Convolutional Neural Network for the Classification of Abnormal Cardiac Rhythm Using Single Lead Electrocardiograms of Variable Length},
  author = {Kamaleswaran, Rishikesan and Mahajan, Ruhi and Akbilgic, Oguz},
  year = {2018},
  month = mar,
  journal = {Physiological Measurement},
  volume = {39},
  number = {3},
  pages = {035006},
  issn = {1361-6579},
  doi = {10/gf2594},
  abstract = {OBJECTIVE: Atrial fibrillation (AF) is a major cause of hospitalization and death in the United States. Moreover, as the average age of individuals increases around the world, early detection and diagnosis of AF become even more pressing. In this paper, we introduce a novel deep learning architecture for the detection of normal sinus rhythm, AF, other abnormal rhythms, and noise. APPROACH: We have demonstrated through a systematic approach many hyperparameters, input sets, and optimization methods that yielded influence in both training time and performance accuracy. We have focused on these properties to identify an optimal 13-layer convolutional neural network (CNN) model which was trained on 8528 short single-lead ECG recordings and evaluated on a test dataset of 3658 recordings. MAIN RESULTS: The proposed CNN architecture achieved a state-of-the-art performance in identifying normal, AF and other rhythms with an average F 1-score of 0.83. SIGNIFICANCE: We have presented a robust deep learning-based architecture that can identify abnormal cardiac rhythms using short single-lead ECG recordings. The proposed architecture is computationally fast and can also be used in real-time cardiac arrhythmia detection applications.},
  langid = {english},
  pmid = {29369044},
  keywords = {{Arrhythmias, Cardiac},{Signal Processing, Computer-Assisted},Electrocardiography,Heart Rate,Humans,Neural Networks (Computer)}
}

@article{kaminnski_genetic_1996,
  title = {Genetic Algorithms and Artificial Neural Networks for Description of Thermal Deterioration Processes},
  author = {Kami{\'n}nski, W and Strumitto, P and Tomczak, E},
  year = {1996},
  journal = {Drying Technology},
  volume = {14},
  number = {9},
  pages = {2117--2133},
  doi = {10.1080/07373939608917198}
}

@article{kanuparthi_hdetach_2019,
  title = {H-{{DETACH}}: {{Modifying}} the {{LSTM Gradient Towards Better Optimization}}},
  author = {Kanuparthi, Bhargav and Arpit, Devansh and Kerg, Giancarlo and Ke, Nan Rosemary and Mitliagkas, Ioannis and Bengio, Yoshua},
  year = {2019},
  journal = {Proceedings of the International Conference for Learning Representations (ICLR)},
  pages = {19},
  abstract = {Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP). This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps. We introduce a simple stochastic algorithm (h-detach) that is specific to LSTM optimization and targeted towards addressing this problem. Specifically, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them. Our algorithm1 prevents gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better. We show significant improvements over vanilla LSTM gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization using our modification of LSTM gradient on various benchmark datasets.},
  langid = {english},
  keywords = {â›” No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/E9BQ46DJ/Kanuparthi et al. - 2019 - h-DETACH MODIFYING THE LSTM GRADIENT TO- WARDS BE.pdf}
}

@article{karikov_construction_2013,
  title = {Construction of a {{Dynamic Neural Network Model}} as a {{Stage}} of {{Grate Cooler Automation}}},
  author = {Karikov, Evgeny Borisovich and Rubanov, Vasily Grigorievich and Klassen, Victor Korneevich},
  year = {2013},
  journal = {World Applied Sciences Journal},
  volume = {25},
  number = {2},
  pages = {227--232},
  keywords = {ðŸ”No DOI found}
}

@article{karki_understanding_1998,
  title = {Understanding Operational Amplifier Specifications},
  author = {Karki, Jim},
  year = {1998},
  journal = {Mixed Signal and Analog Operational Amplifiers. Digital Signal Processing Solutions, no. White Paper: SLOA011},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/79P6CFQ4/karki_understand_1998.pdf}
}

@misc{karpathy_unreasonable_2015,
  title = {The {{Unreasonable Effectiveness}} of {{Recurrent Neural Networks}}},
  author = {Karpathy, Andrej},
  year = {2015},
  month = may,
  journal = {Andrej Karpathy blog},
  urldate = {2020-01-27},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VWYQW22A/rnn-effectiveness.html}
}

@article{kashef_legacy_2016,
  title = {Legacy Effect of Statins: 20-Year Follow up of the {{West}} of {{Scotland Coronary Prevention Study}} ({{WOSCOPS}})},
  shorttitle = {Legacy Effect of Statins},
  author = {Kashef, Mohammed Amin and Giugliano, Gregory},
  year = {2016},
  journal = {Global Cardiology Science and Practice},
  volume = {2016},
  number = {4},
  issn = {2305-7823},
  doi = {10.21542/gcsp.2016.35},
  urldate = {2024-08-19},
  abstract = {The West of Scotland Coronary Prevention Study (WOSCOPS) was a randomized, placebo- controlled, primary prevention trial of pravastatin in men aged 45 to 64 (mean age of 55 years) with no history of myocardial infarction at randomization. A total of 6,595 men, with a mean (SD) plasma cholesterol level of 272 (23) mg/dL and mean (SD) low density lipoprotein cholesterol (LDL-C) of 192 (17) mg/dL were randomly assigned to receive pravastatin 40 mg daily or placebo for five years. The primary outcome was a composite of death from coronary heart disease (CHD) and nonfatal myocardial infarction. There was a 31\% relative reduction in the primary outcome with pravastatin. There was similar reduction in risk of nonfatal myocardial infarction, death from CHD and death from all cardiovascular causes with no increased risk of death from non-cardiovascular causes nor an increase in incident cancers.},
  copyright = {Copyright (c) 2017 Mohammed Amin Kashef, Gregory Giugliano},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9G2EFGLL/Kashef and Giugliano - 2016 - Legacy effect of statins 20-year follow up of the.pdf}
}

@article{kassam_robust_1985,
  title = {Robust Techniques for Signal Processing: {{A}} Survey},
  author = {Kassam, Saleem A and Poor, H Vincent},
  year = {1985},
  journal = {Proceedings of the IEEE},
  volume = {73},
  number = {3},
  pages = {433--481},
  publisher = {IEEE}
}

@article{katharopoulos_transformers_2020,
  title = {Transformers Are {{RNNs}}: {{Fast Autoregressive Transformers}} with {{Linear Attention}}},
  shorttitle = {Transformers Are {{RNNs}}},
  author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c c}ois},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.16236 [cs, stat]},
  eprint = {2006.16236},
  primaryclass = {cs, stat},
  urldate = {2020-07-04},
  abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textasciicircum}2{\textbackslash}right)\$ to \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textbackslash}right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4JRQB7WJ/Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf;/Users/antoniohortaribeiro/Zotero/storage/CXFE58HQ/2006.html}
}

@article{katzman_deepsurv_2018,
  title = {{{DeepSurv}}: {{Personalized Treatment Recommender System Using A Cox Proportional Hazards Deep Neural Network}}},
  shorttitle = {{{DeepSurv}}},
  author = {Katzman, Jared and Shaham, Uri and Bates, Jonathan and Cloninger, Alexander and Jiang, Tingting and Kluger, Yuval},
  year = {2018},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {18},
  number = {1},
  eprint = {1606.00931},
  primaryclass = {cs, stat},
  pages = {24},
  issn = {1471-2288},
  doi = {10.1186/s12874-018-0482-1},
  urldate = {2023-07-06},
  abstract = {Medical practitioners use survival models to explore and understand the relationships between patients' covariates (e.g. clinical and genetic features) and the effectiveness of various treatment options. Standard survival models like the linear Cox proportional hazards model require extensive feature engineering or prior medical knowledge to model treatment interaction at an individual level. While nonlinear survival methods, such as neural networks and survival forests, can inherently model these high-level interaction terms, they have yet to be shown as effective treatment recommender systems. We introduce DeepSurv, a Cox proportional hazards deep neural network and state-of-the-art survival method for modeling interactions between a patient's covariates and treatment effectiveness in order to provide personalized treatment recommendations. We perform a number of experiments training DeepSurv on simulated and real survival data. We demonstrate that DeepSurv performs as well as or better than other state-of-the-art survival models and validate that DeepSurv successfully models increasingly complex relationships between a patient's covariates and their risk of failure. We then show how DeepSurv models the relationship between a patient's features and effectiveness of different treatment options to show how DeepSurv can be used to provide individual treatment recommendations. Finally, we train DeepSurv on real clinical studies to demonstrate how it's personalized treatment recommendations would increase the survival time of a set of patients. The predictive and modeling capabilities of DeepSurv will enable medical researchers to use deep neural networks as a tool in their exploration, understanding, and prediction of the effects of a patient's characteristics on their risk of failure.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2XRIZF2X/Katzman et al. - 2018 - DeepSurv Personalized Treatment Recommender Syste.pdf;/Users/antoniohortaribeiro/Zotero/storage/W7FKBPNM/1606.html}
}

@article{katzman_deepsurv_2018a,
  title = {{{DeepSurv}}: Personalized Treatment Recommender System Using a {{Cox}} Proportional Hazards Deep Neural Network},
  shorttitle = {{{DeepSurv}}},
  author = {Katzman, Jared L. and Shaham, Uri and Cloninger, Alexander and Bates, Jonathan and Jiang, Tingting and Kluger, Yuval},
  year = {2018},
  month = feb,
  journal = {BMC Medical Research Methodology},
  volume = {18},
  number = {1},
  pages = {24},
  issn = {1471-2288},
  doi = {10.1186/s12874-018-0482-1},
  urldate = {2023-07-06},
  abstract = {Medical practitioners use survival models to explore and understand the relationships between patients' covariates (e.g. clinical and genetic features) and the effectiveness of various treatment options. Standard survival models like the linear Cox proportional hazards model require extensive feature engineering or prior medical knowledge to model treatment interaction at an individual level. While nonlinear survival methods, such as neural networks and survival forests, can inherently model these high-level interaction terms, they have yet to be shown as effective treatment recommender systems.},
  keywords = {Deep learning,Survival analysis,Treatment recommendations},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2E65KR8N/Katzman et al. - 2018 - DeepSurv personalized treatment recommender syste.pdf;/Users/antoniohortaribeiro/Zotero/storage/IAMB4R8B/s12874-018-0482-1.html}
}

@incollection{kawaguchi_deep_2016,
  title = {Deep {{Learning}} without {{Poor Local Minima}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Kawaguchi, Kenji},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {586--594},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-08-27},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VYY6V593/Kawaguchi - 2016 - Deep Learning without Poor Local Minima.pdf;/Users/antoniohortaribeiro/Zotero/storage/CVJ3EWZ5/6112-deep-learning-without-poor-local-minima.html}
}

@inproceedings{kawaguchi_elimination_2020,
  title = {Elimination of {{All Bad Local Minima}} in {{Deep Learning}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Kawaguchi, Kenji and Kaelbling, Leslie},
  year = {2020},
  month = jun,
  pages = {853--863},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2020-08-27},
  abstract = {In this paper, we theoretically prove that adding one special neuron per output unit eliminates all suboptimal local minima of any deep neural network, for multi-class classification, binary classi...},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/II38DZE9/Kawaguchi and Kaelbling - 2020 - Elimination of All Bad Local Minima in Deep Learni.pdf;/Users/antoniohortaribeiro/Zotero/storage/7ZGYAQ6S/kawaguchi20b.html}
}

@book{kay_intuitive_2006,
  title = {Intuitive Probability and Random Processes Using {{MATLAB}}},
  author = {Kay, Steven M.},
  year = {2006},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-24157-9 978-0-387-24158-6},
  lccn = {QA273 .K326 2006},
  keywords = {Computer Simulation,MATLAB,Probabilities,stochastic processes,Textbooks},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZNX96TH8/kay_intuitive_2006.pdf}
}

@article{kayacan_identification_2015,
  title = {Identification of Nonlinear Dynamic Systems Using Type-2 Fuzzy Neural Networks---{{A}} Novel Learning Algorithm and a Comparative Study},
  author = {Kayacan, Erkan and Kayacan, Erdal and Khanesar, Mojtaba Ahmadieh},
  year = {2015},
  journal = {IEEE Transactions on Industrial Electronics},
  volume = {62},
  number = {3},
  pages = {1716--1724},
  issn = {0278-0046},
  doi = {10.1109/TIE.2014.2345353}
}

@incollection{kennedy_particle_2011,
  title = {Particle Swarm Optimization},
  booktitle = {Encyclopedia of Machine Learning},
  author = {Kennedy, James},
  year = {2011},
  pages = {760--766},
  publisher = {Springer}
}

@article{kerg_nonnormal_2019,
  title = {Non-Normal {{Recurrent Neural Network}} ({{nnRNN}}): Learning Long Time Dependencies While Improving Expressivity with Transient Dynamics},
  shorttitle = {Non-Normal {{Recurrent Neural Network}} ({{nnRNN}})},
  author = {Kerg, Giancarlo and Goyette, Kyle and Touzel, Maximilian Puelma and Gidel, Gauthier and Vorontsov, Eugene and Bengio, Yoshua and Lajoie, Guillaume},
  year = {2019},
  month = may,
  journal = {arXiv:1905.12080 [cs, stat]},
  eprint = {1905.12080},
  primaryclass = {cs, stat},
  urldate = {2019-06-06},
  abstract = {A recent strategy to circumvent the exploding and vanishing gradient problem in RNNs, and to allow the stable propagation of signals over long time scales, is to constrain recurrent connectivity matrices to be orthogonal or unitary. This ensures eigenvalues with unit norm and thus stable dynamics and training. However this comes at the cost of reduced expressivity due to the limited variety of orthogonal transformations. We propose a novel connectivity structure based on the Schur decomposition and a splitting of the Schur form into normal and non-normal parts. This allows to parametrize matrices with unit-norm eigenspectra without orthogonality constraints on eigenbases. The resulting architecture ensures access to a larger space of spectrally constrained matrices, of which orthogonal matrices are a subset. This crucial difference retains the stability advantages and training speed of orthogonal RNNs while enhancing expressivity, especially on tasks that require computations over ongoing input sequences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AH7M9KPK/kerg_non-normal_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/BTD4F6IH/1905.html}
}

@article{keskar_largebatch_2016,
  title = {On {{Large-Batch Training}} for {{Deep Learning}}: {{Generalization Gap}} and {{Sharp Minima}}},
  shorttitle = {On {{Large-Batch Training}} for {{Deep Learning}}},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  year = {2016},
  month = sep,
  journal = {arXiv:1609.04836 [cs, math]},
  eprint = {1609.04836},
  primaryclass = {cs, math},
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Learning,Mathematics - Optimization and Control},
  file = {/Users/antoniohortaribeiro/Zotero/storage/56H48KHC/keskar_on_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/CBX27QIS/keskar_on_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/4QP2CK4M/1609.html;/Users/antoniohortaribeiro/Zotero/storage/72D9CU2T/1609.html;/Users/antoniohortaribeiro/Zotero/storage/SLW24YH7/1609.html;/Users/antoniohortaribeiro/Zotero/storage/XPEY9MKH/1609.html}
}

@phdthesis{keskar_secondorder_2017,
  title = {Second-{{Order Methods}} for {{Stochastic}} and {{Nonsmooth Optimization}}},
  author = {Keskar, Nitish Shirish},
  year = {2017},
  school = {Northwestern University},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FUKH5DAT/keskar_second-ord_2017.pdf}
}

@book{khalil_nonlinear_2002,
  title = {Nonlinear Systems},
  author = {Khalil, Hassan K},
  year = {2002},
  edition = {Third},
  publisher = {Upper Saddle River},
  file = {/Users/antoniohortaribeiro/Zotero/storage/I9SZD7ZU/Hassan K. Khalil-Nonlinear Systems (3rd Edition)  -Prentice Hall (2001).pdf}
}

@article{khan_forecasting_2015,
  title = {Forecasting the {{Number}} of {{Muslim Pilgrims Using NARX Neural Networks}} with a {{Comparison Study}} with {{Other Modern Methods}}},
  author = {Khan, Esam A and Elgamal, Mahmoud A and Shaarawy, Sameer M},
  year = {2015},
  journal = {British Journal of Mathematics \& Computer Science},
  volume = {6},
  number = {5},
  pages = {394},
  doi = {10.9734/BJMCS/2015/14563}
}

@article{khandelwal_sharp_2018,
  title = {Sharp {{Nearby}}, {{Fuzzy Far Away}}: {{How Neural Language Models Use Context}}},
  shorttitle = {Sharp {{Nearby}}, {{Fuzzy Far Away}}},
  author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
  year = {2018},
  month = may,
  journal = {arXiv:1805.04623 [cs]},
  eprint = {1805.04623},
  primaryclass = {cs},
  urldate = {2019-06-14},
  abstract = {We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/antoniohortaribeiro/Zotero/storage/79F8XRFU/khandelwal_sharp_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/WEUB49IV/1805.html}
}

@article{kiefer_sequential_1953,
  title = {Sequential Minimax Search for a Maximum},
  author = {Kiefer, Jack},
  year = {1953},
  journal = {Proceedings of the American Mathematical Society},
  volume = {4},
  number = {3},
  pages = {502--506},
  doi = {10.1090/S0002-9939-1953-0055639-3}
}

@article{kilani_cognitive_2016,
  title = {Cognitive Waveform and Receiver Selection Mechanism for Multistatic Radar},
  author = {Kilani, Moez Ben and Nijsure, Yogesh and Gagnon, Ghyslain and Kaddoum, Georges and Gagnon, Fran{\c c}ois},
  year = {2016},
  journal = {IET Radar, Sonar \& Navigation},
  volume = {10},
  number = {2},
  pages = {417--425},
  keywords = {ðŸ”No DOI found}
}

@book{kilts_advanced_2007,
  title = {Advanced {{FPGA}} Design: Architecture, Implementation, and Optimization},
  shorttitle = {Advanced {{FPGA}} Design},
  author = {Kilts, Steve},
  year = {2007},
  publisher = {Wiley : IEEE},
  address = {Hoboken, N.J},
  isbn = {978-0-470-05437-6},
  lccn = {TK7895.G36 K55 2007},
  keywords = {Design and construction,Field programmable gate arrays},
  annotation = {OCLC: ocm72799161},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CXIK53VB/kilts_advanced_2007.pdf}
}

@article{kim_interpretability_,
  title = {Interpretability {{Beyond Feature Attribution}}:  {{Quantitative Testing}} with {{Concept Activation Vectors}} ({{TCAV}})},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  pages = {10},
  abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of zebra is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
  langid = {english},
  keywords = {ðŸ”No DOI found,deep learning reading group},
  annotation = {00006},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DJ5FWHMU/Kim et al. - Interpretability Beyond Feature Attribution  Quan.pdf}
}

@inproceedings{kim_interpretability_2018,
  title = {Interpretability {{Beyond Feature Attribution}}: {{Quantitative Testing}} with {{Concept Activation Vectors}} ({{TCAV}})},
  shorttitle = {Interpretability {{Beyond Feature Attribution}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  year = {2018},
  month = jul,
  pages = {2668--2677},
  urldate = {2018-11-15},
  abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level ...},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CBM7X5G5/kim_interpreta_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/NMT9356T/kim18d.html}
}

@article{kim_languageuniversal_2017,
  title = {Towards {{Language-Universal End-to-End Speech Recognition}}},
  author = {Kim, Suyoun and Seltzer, Michael L.},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.02207 [cs]},
  eprint = {1711.02207},
  primaryclass = {cs},
  abstract = {Building speech recognizers in multiple languages typically involves replicating a monolingual training recipe for each language, or utilizing a multi-task learning approach where models for different languages have separate output labels but share some internal parameters. In this work, we exploit recent progress in end-to-end speech recognition to create a single multilingual speech recognition system capable of recognizing any of the languages seen in training. To do so, we propose the use of a universal character set that is shared among all languages. We also create a language-specific gating mechanism within the network that can modulate the network's internal representations in a language-specific way. We evaluate our proposed approach on the Microsoft Cortana task across three languages and show that our system outperforms both the individual monolingual systems and systems built with a multi-task learning approach. We also show that this model can be used to initialize a monolingual speech recognizer, and can be used to create a bilingual model for use in code-switching scenarios.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computation and Language},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RCMZDMTP/kim_towards_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/HNNS44PU/1711.html}
}

@inproceedings{kingma_adam_2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  booktitle = {Proceedings of the 3rd {{International Conference}} for {{Learning Representations}} ({{ICLR}})},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  month = dec,
  eprint = {1412.6980},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/C2Z3WXUS/kingma_adam_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/PB8NG6WM/1412.html}
}

@article{kingma_autoencoding_2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  journal = {ICLR},
  eprint = {1312.6114},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5N7JRL5I/kingma_auto-encod_2013.pdf;/Users/antoniohortaribeiro/Zotero/storage/WB356YEP/kingma_auto-encod_2013.pdf;/Users/antoniohortaribeiro/Zotero/storage/73TY8TU9/1312.html;/Users/antoniohortaribeiro/Zotero/storage/LZWN7XD4/1312.html}
}

@book{kirk_optimal_2012,
  title = {Optimal Control Theory: An Introduction},
  shorttitle = {Optimal Control Theory},
  author = {Kirk, Donald E.},
  year = {2012},
  publisher = {Courier Corporation},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CRAP7MWU/kirk_optimal_2012.pdf}
}

@article{kirkpatrick_optimization_1983,
  title = {Optimization by Simulated Annealing},
  author = {Kirkpatrick, Scott and Gelatt, C Daniel and Vecchi, Mario P and others},
  year = {1983},
  journal = {science},
  volume = {220},
  number = {4598},
  pages = {671--680},
  doi = {10.1126/science.220.4598.671}
}

@book{kleinberg_algorithm_2006,
  title = {Algorithm Design},
  author = {Kleinberg, Jon and Tardos, {\'E}va},
  year = {2006},
  publisher = {Pearson/Addison-Wesley},
  address = {Boston},
  isbn = {978-0-321-29535-4},
  lccn = {QA76.9.A43 K54 2006},
  keywords = {Computer algorithms,Data structures (Computer science)},
  file = {/Users/antoniohortaribeiro/Zotero/storage/P4WPS5TQ/kleinberg_algorithm_2006.pdf}
}

@article{kligfield_recommendations_2007,
  title = {Recommendations for the {{Standardization}} and {{Interpretation}} of the {{Electrocardiogram}}},
  author = {Kligfield, Paul and Gettes, Leonard S. and Bailey, James J. and Childers, Rory and Deal, Barbara J. and Hancock, E. William and {van Herpen}, Gerard and Kors, Jan A. and Macfarlane, Peter and Mirvis, David M. and Pahlm, Olle and Rautaharju, Pentti and Wagner, Galen S.},
  year = {2007},
  month = mar,
  journal = {Journal of the American College of Cardiology},
  volume = {49},
  number = {10},
  pages = {1109},
  doi = {10/bwp48j},
  abstract = {This statement examines the relation of the resting ECG to its technology. Its purpose is to foster understanding of how the modern ECG is derived and displayed and to establish standards that will improve the accuracy and usefulness of the ECG in practice. Derivation of representative waveforms and measurements based on global intervals are described. Special emphasis is placed on digital signal acquisition and computer-based signal processing, which provide automated measurements that lead to computer-generated diagnostic statements. Lead placement, recording methods, and waveform presentation are reviewed. Throughout the statement, recommendations for ECG standards are placed in context of the clinical implications of evolving ECG technology.}
}

@article{kobak_optimal_2020,
  title = {Optimal Ridge Penalty for Real-World High-Dimensional Data Can Be Zero or Negative Due to the Implicit Ridge Regularization},
  author = {Kobak, Dmitry and Lomond, Jonathan and Sanchez, Benoit},
  year = {2020},
  month = apr,
  journal = {arXiv:1805.10939 [math, stat]},
  eprint = {1805.10939},
  primaryclass = {math, stat},
  urldate = {2020-11-12},
  abstract = {A conventional wisdom in statistical learning is that large models require strong regularization to prevent overfitting. Here we show that this rule can be violated by linear regression in the underdetermined \$n{\textbackslash}ll p\$ situation under realistic conditions. Using simulations and real-life high-dimensional data sets, we demonstrate that an explicit positive ridge penalty can fail to provide any improvement over the minimum-norm least squares estimator. Moreover, the optimal value of ridge penalty in this situation can be negative. This happens when the high-variance directions in the predictor space can predict the response variable, which is often the case in the real-world high-dimensional data. In this regime, low-variance directions provide an implicit ridge regularization and can make any further positive ridge penalty detrimental. We prove that augmenting any linear model with random covariates and using minimum-norm estimator is asymptotically equivalent to adding the ridge penalty. We use a spiked covariance model as an analytically tractable example and prove that the optimal ridge penalty in this case is negative when \$n{\textbackslash}ll p\$.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/38UVEM2J/Kobak et al. - 2020 - Optimal ridge penalty for real-world high-dimensio.pdf;/Users/antoniohortaribeiro/Zotero/storage/P48ETSUJ/1805.html}
}

@inproceedings{koehler_uniform_2021,
  title = {Uniform {{Convergence}} of {{Interpolators}}: {{Gaussian Width}}, {{Norm Bounds}} and {{Benign Overfitting}}},
  shorttitle = {Uniform {{Convergence}} of {{Interpolators}}},
  booktitle = {{{NeurIPS}}},
  author = {Koehler, Frederic and Zhou, Lijia and Sutherland, Danica J. and Srebro, Nathan},
  year = {2021},
  month = oct,
  urldate = {2022-08-04},
  abstract = {Uniform convergence of interpolating predictors can explain consistency for high-dimensional linear regression.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/249VV8X2/Koehler et al_2021_Uniform Convergence of Interpolators.pdf;/Users/antoniohortaribeiro/Zotero/storage/T6ULMFRV/forum.html}
}

@article{kohl_probabilistic_2018,
  title = {A {{Probabilistic U-Net}} for {{Segmentation}} of {{Ambiguous Images}}},
  author = {Kohl, Simon A. A. and {Romera-Paredes}, Bernardino and Meyer, Clemens and De Fauw, Jeffrey and Ledsam, Joseph R. and {Maier-Hein}, Klaus H. and Eslami, S. M. Ali and Rezende, Danilo Jimenez and Ronneberger, Olaf},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.05034 [cs, stat]},
  eprint = {1806.05034},
  primaryclass = {cs, stat},
  urldate = {2018-11-26},
  abstract = {Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RVCII5WG/kohl_a_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/28Y4ZP92/1806.html}
}

@book{koller_probabilistic_2009,
  title = {Probabilistic Graphical Models: Principles and Techniques},
  shorttitle = {Probabilistic Graphical Models},
  author = {Koller, Daphne and Friedman, Nir},
  year = {2009},
  series = {Adaptive Computation and Machine Learning},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  isbn = {978-0-262-01319-2},
  lccn = {QA279.5 .K65 2009},
  keywords = {Bayesian statistical decision theory,Graphic methods,Graphical modeling (Statistics)},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WTIHTWH5/koller_probabilis_2009.pdf}
}

@phdthesis{kors_expert_1992,
  title = {Expert Knowledge for Computerized {{ECG}} Interpretation},
  author = {Kors, Jan},
  year = {1992},
  address = {S.l.},
  langid = {english},
  school = {s.n.]},
  annotation = {OCLC: 65910805},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3J9FF5VD/kors_expert_1992.pdf}
}

@article{koziel_robust_2010,
  title = {Robust Trust-Region Space-Mapping Algorithms for Microwave Design Optimization},
  author = {Koziel, Slawomir and Bandler, John W and Cheng, Qingsha S},
  year = {2010},
  journal = {IEEE Transactions on Microwave Theory and Techniques},
  volume = {58},
  number = {8},
  pages = {2166--2174},
  keywords = {â“Multiple DOI}
}

@article{koziel_robust_2010a,
  title = {Robust {{Trust-Region Space-Mapping Algorithms}} for {{Microwave Design Optimization}}},
  author = {Koziel, Slawomir and Bandler, John W and Cheng, Qingsha S},
  year = {2010},
  journal = {IEEE Transactions on Microwave Theory and Techniques},
  volume = {58},
  number = {8},
  pages = {2166--2174},
  doi = {10/dk8kvf},
  annotation = {00063}
}

@article{kraft_software_1988,
  title = {A Software Package for Sequential Quadratic Programming},
  author = {Kraft, Dieter},
  year = {1988},
  journal = {Forschungsbericht- Deutsche Forschungs- und Versuchsanstalt fur Luft- und Raumfahrt},
  issn = {0171-1342},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IUSNBMQU/kraft_a_1988.pdf}
}

@article{kristensen_parameter_2004,
  title = {Parameter Estimation in Stochastic Grey-Box Models},
  author = {Kristensen, Niels Rode and Madsen, Henrik and J{\o}rgensen, Sten Bay},
  year = {2004},
  journal = {Automatica},
  volume = {40},
  number = {2},
  pages = {225--237},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2003.10.001}
}

@inproceedings{krizhevsky_imagenet_2012,
  title = {Imagenet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  pages = {1097--1105},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2FN4SE32/krizhevsky_imagenet_2012.pdf}
}

@article{kukreja_least_2006,
  title = {A Least Absolute Shrinkage and Selection Operator ({{LASSO}}) for Nonlinear System Identification},
  author = {Kukreja, Sunil L. and L{\"o}fberg, Johan and Brenner, Martin J.},
  year = {2006},
  journal = {IFAC proceedings volumes},
  volume = {39},
  number = {1},
  pages = {814--819},
  doi = {10.3182/20060329-3-AU-2901.00128},
  urldate = {2017-09-13},
  keywords = {Aeroelasticity,nonlinear systems,Structure Detection,system identification},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6HJ55397/kukreja_a least_2006.pdf;/Users/antoniohortaribeiro/Zotero/storage/ITIN43TN/kukreja_a least_2006.pdf;/Users/antoniohortaribeiro/Zotero/storage/I2WZNTPM/S1474667015353647.html;/Users/antoniohortaribeiro/Zotero/storage/QREQIY83/S1474667015353647.html}
}

@article{kumar_comparative_2019,
  title = {Comparative Study of Neural Networks for Dynamic Nonlinear Systems Identification},
  author = {Kumar, Rajesh and Srivastava, Smriti and Gupta, J. R. P. and Mohindru, Amit},
  year = {2019},
  month = jan,
  journal = {Soft Computing},
  volume = {23},
  number = {1},
  pages = {101--114},
  issn = {1433-7479},
  doi = {10/gfwvbb},
  abstract = {In this paper, a comparative study is performed to test the approximation ability of different neural network structures. It involves three neural networks multilayer feedforward neural network (MLFFNN), diagonal recurrent neural network (DRNN), and nonlinear autoregressive with exogenous inputs (NARX) neural network. Their robustness is also tested and compared when the system is subjected to parameter variations and disturbance signals. Further, dynamic back-propagation algorithm is used to update the parameters associated with these neural networks. Four dynamical systems of different complexities including motor-driven robotic link are considered on which the comparative study is performed. The simulation results show the superior performance of DRNN identification model over NARX and MLFFNN identification models.}
}

@inproceedings{kurakin_adversarial_2018,
  title = {Adversarial Attacks and Defences Competition},
  booktitle = {The {{NIPS}} '17 Competition: {{Building Intelligent Systems}}},
  author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy and Dong, Yinpeng and Liao, Fangzhou and Liang, Ming and Pang, Tianyu and Zhu, Jun and Hu, Xiaolin and Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Ren, Zhou and Yuille, Alan and Huang, Sangxia and Zhao, Yao and Zhao, Yuzhe and Han, Zhonglin and Long, Junjiajia and Berdibekov, Yerkebulan and Akiba, Takuya and Tokui, Seiya and Abe, Motoki},
  editor = {Escalera, Sergio and Weimer, Markus},
  year = {2018},
  pages = {195--231},
  abstract = {To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several of the top-placing teams.},
  isbn = {978-3-319-94042-7},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5RBKZ9P4/Kurakin et al. - 2018 - Adversarial Attacks and Defences Competition.pdf}
}

@article{kuznetsov_elements_2004,
  title = {Elements of Applied Bifurcation Theory},
  author = {Kuznetsov, Yu A},
  year = {2004}
}

@techreport{labvolt_mobile_2015,
  title = {Mobile {{Instrumentation}} and {{Process Control Training Systems}}},
  author = {{LabVolt}},
  year = {2015},
  institution = {Festo}
}

@techreport{labvolt_mobile_2015a,
  title = {Mobile {{Instrumentation}} and {{Process Control Training Systems}}},
  author = {{LabVolt}},
  year = {2015},
  institution = {Festo},
  annotation = {00000}
}

@article{lai_extended_1986,
  title = {Extended Least Squares and Their Applications to Adaptive Control and Prediction in Linear Systems},
  author = {Lai, Tze and Wei, Ching-Zong},
  year = {1986},
  month = oct,
  journal = {IEEE Transactions on Automatic Control},
  volume = {31},
  number = {10},
  pages = {898--906},
  issn = {0018-9286},
  doi = {10.1109/TAC.1986.1104138},
  abstract = {Herein strong consistency of recursive extended least squares is established under considerably weaker assumptions than previously assumed in the literature. The argument used to establish consistency also leads to certain basic properties of adaptive predictors based on these recursive estimators. Making use of these properties of the adaptive predictors, simple modifications of the {\AA}str{\"o}m-Wittenmark self-tuning regulator are proposed and shown to be asymptotically optimal.},
  keywords = {{Adaptive control, linear systems},{Adaptive estimation, linear systems},Adaptive control,Context modeling,Delay,Least squares approximation,Least squares methods,Least-squares methods,Linear systems,Polynomials,Prediction methods,Recursive estimation,Stochastic systems,Tin},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3AH6MWVC/lai_extended_1986.pdf;/Users/antoniohortaribeiro/Zotero/storage/V2E5E9AF/1104138.html}
}

@article{lalee_implementation_1998,
  title = {On the Implementation of an Algorithm for Large-Scale Equality Constrained Optimization},
  author = {Lalee, Marucha and Nocedal, Jorge and Plantenga, Todd},
  year = {1998},
  journal = {SIAM Journal on Optimization},
  volume = {8},
  number = {3},
  pages = {682--706},
  doi = {10.1137/S1052623493262993},
  urldate = {2017-08-20},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QCLRG4KQ/lalee_on the_1998.pdf}
}

@article{lalee_implementation_1998a,
  title = {On the {{Implementation}} of an {{Algorithm}} for {{Large-Scale Equality Constrained Optimization}}},
  author = {Lalee, Marucha and Nocedal, Jorge and Plantenga, Todd},
  year = {1998},
  journal = {SIAM Journal on Optimization},
  volume = {8},
  number = {3},
  pages = {682--706},
  doi = {10/dhjrhk},
  urldate = {2017-08-20}
}

@book{lambert_numerical_1991,
  title = {Numerical Methods for Ordinary Differential Systems: The Initial Value Problem},
  author = {Lambert, John Denholm},
  year = {1991},
  publisher = {John Wiley \& Sons, Inc.},
  isbn = {0-471-92990-5},
  file = {/Users/antoniohortaribeiro/Zotero/storage/V84GP3V4/lambert_numerical_1991.pdf}
}

@article{lange_lottery_2020,
  title = {The Lottery Ticket Hypothesis: {{A}} Survey},
  author = {Lange, Robert Tjarko},
  year = {2020},
  journal = {https://roberttlange.github.io/year-archive/posts/2020/06/lottery-ticket-hypothesis/},
  file = {/Users/antoniohortaribeiro/Zotero/storage/M34KEDG7/lange2020_lottery_ticket_hypothesis.pdf}
}

@article{langone_incremental_2014,
  title = {Incremental Kernel Spectral Clustering for Online Learning of Non-Stationary Data},
  author = {Langone, Rocco and Mauricio Agudelo, Oscar and De Moor, Bart and Suykens, Johan A. K.},
  year = {2014},
  month = sep,
  journal = {Neurocomputing},
  volume = {139},
  pages = {246--260},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2014.02.036},
  abstract = {In this work a new model for online clustering named Incremental kernel spectral clustering (IKSC) is presented. It is based on kernel spectral clustering (KSC), a model designed in the Least Squares Support Vector Machines (LS-SVMs) framework, with primal-dual setting. The IKSC model is developed to quickly adapt itself to a changing environment, in order to learn evolving clusters with high accuracy. In contrast with other existing incremental spectral clustering approaches, the eigen-updating is performed in a model-based manner, by exploiting one of the Karush--Kuhn--Tucker (KKT) optimality conditions of the KSC problem. We test the capacities of IKSC with some experiments conducted on computer-generated data and a real-world data-set of PM10 concentrations registered during a pollution episode occurred in Northern Europe in January 2010. We observe that our model is able to precisely recognize the dynamics of shifting patterns in a non-stationary context.},
  keywords = {Incremental kernel spectral clustering,LS-SVMs,Non-stationary data,Online clustering,Out-of-sample eigenvectors,PM concentrations},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9TG3U7TJ/langone_incrementa_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/HG7JS3QV/S0925231214004433.html;/Users/antoniohortaribeiro/Zotero/storage/RD5R9TYJ/S0925231214004433.html}
}

@article{laranja_chagas_1956,
  title = {Chagas' {{Disease}}: {{A Clinical}}, {{Epidemiologic}}, and {{Pathologic Study}}},
  shorttitle = {Chagas' {{Disease}}},
  author = {Laranja, F. S. and Dias, E. and Nobrega, G. and Miranda, A.},
  year = {1956},
  month = dec,
  journal = {Circulation},
  volume = {14},
  number = {6},
  pages = {1035--1060},
  issn = {0009-7322, 1524-4539},
  doi = {10.1161/01.CIR.14.6.1035},
  urldate = {2021-11-25},
  abstract = {A study of the most important clinical and pathologic aspects of Chagas' disease has been presented, on the basis of the analysis of 180 cases of acute infection (11 with autopsy), 657 cases of chronic asymptomatic infection, and 683 cases of chronic Chagas' heart disease (21 autopsied cases with               Schizotrypanum cruzi               in myocardium).},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/B6P8VZU4/Laranja et al. - 1956 - Chagas' Disease A Clinical, Epidemiologic, and Pa.pdf}
}

@article{laurent_deep_,
  title = {Deep {{Linear Networks}} with {{Arbitrary Loss}}: {{All Local Minima Are Global}}},
  author = {Laurent, Thomas},
  pages = {6},
  abstract = {We consider deep linear networks with arbitrary convex differentiable loss. We provide a short and elementary proof of the fact that all local minima are global minima if the hidden layers are either 1) at least as wide as the input layer, or 2) at least as wide as the output layer. This result is the strongest possible in the following sense: If the loss is convex and Lipschitz but not differentiable then deep linear networks can have sub-optimal local minima.},
  langid = {english}
}

@inproceedings{laurent_deep_2018,
  title = {Deep {{Linear Networks}} with {{Arbitrary Loss}}: {{All Local Minima Are Global}}},
  shorttitle = {Deep {{Linear Networks}} with {{Arbitrary Loss}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Laurent, Thomas and Brecht, James},
  year = {2018},
  month = jul,
  pages = {2902--2907},
  issn = {2640-3498},
  urldate = {2020-09-07},
  abstract = {We consider deep linear networks with arbitrary convex differentiable loss. We provide a short and elementary proof of the fact that all local minima are global minima if the hidden layers are eith...},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3UWG3XCI/Laurent - Deep Linear Networks with Arbitrary Loss All Loca.pdf;/Users/antoniohortaribeiro/Zotero/storage/JCXEX2BY/Laurent and Brecht - 2018 - Deep Linear Networks with Arbitrary Loss All Loca.pdf;/Users/antoniohortaribeiro/Zotero/storage/SPRWNJLN/Laurent - Deep Linear Networks with Arbitrary Loss All Loca.pdf;/Users/antoniohortaribeiro/Zotero/storage/TEACMLJT/laurent18a.html}
}

@article{laurent_recurrent_2016,
  title = {A Recurrent Neural Network without Chaos},
  author = {Laurent, Thomas and {von Brecht}, James},
  year = {2016},
  month = dec,
  journal = {arXiv:1612.06212 [cs]},
  eprint = {1612.06212},
  primaryclass = {cs},
  urldate = {2019-03-08},
  abstract = {We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/N7WAC9AM/laurent_a_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/FDW7HLG5/1612.html}
}

@book{lax_functional_2002,
  title = {Functional Analysis},
  author = {Lax, Peter D.},
  year = {2002},
  publisher = {Wiley},
  address = {New York},
  isbn = {978-0-471-55604-6},
  langid = {english},
  lccn = {QA320 .L345 2002},
  keywords = {Functional analysis},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZSKJAREI/Lax - 2002 - Functional analysis.pdf}
}

@article{lecun_deep_2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {1476-4687},
  doi = {10/bmqp},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CY95ME2Y/lecun_deep_2015.pdf}
}

@article{lecun_deep_2015a,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  da = {2015/05/01},
  date-added = {2020-09-23 19:36:56 -0300},
  date-modified = {2020-09-23 19:36:56 -0300},
  isbn = {1476-4687},
  ty = {JOUR}
}

@incollection{lecun_efficient_1998,
  title = {Efficient {{BackProp}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  year = {1998},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {9--50},
  publisher = {Springer, Berlin, Heidelberg},
  doi = {10.1007/3-540-49430-8_2},
  urldate = {2017-09-11},
  abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
  isbn = {978-3-540-65311-0 978-3-540-49430-0},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WJZ9KMIB/lecun_efficient_1998.pdf;/Users/antoniohortaribeiro/Zotero/storage/6T8U8HTP/3-540-49430-8_2.html;/Users/antoniohortaribeiro/Zotero/storage/XG4AF6IR/3-540-49430-8_2.html}
}

@article{lecun_gradientbased_1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {0018-9219},
  doi = {10.1109/5.726791},
  keywords = {2D shape variability,back-propagation,backpropagation,Character recognition,cheque reading,complex decision surface synthesis,convolution,convolutional neural network character recognizers,document recognition,document recognition systems,Feature extraction,field extraction,gradient based learning technique,gradient-based learning,graph transformer networks,GTN,handwritten character recognition,handwritten digit recognition task,Hidden Markov models,high-dimensional patterns,language modeling,Machine learning,Multi-layer neural network,multilayer neural networks,multilayer perceptrons,multimodule systems,Neural networks,optical character recognition,Optical character recognition software,Optical computing,Pattern recognition,performance measure minimization,Principal component analysis,segmentation recognition},
  annotation = {14818},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IWMZ34P4/y. lecun_gradient-b_1998.pdf}
}

@inproceedings{lecun_handwritten_1990,
  title = {Handwritten Zip Code Recognition with Multilayer Networks},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Le Cun, Yann and Matan, Ofer and Boser, Bernhard and Denker, John S and Henderson, Don and Howard, Richard E and Hubbard, Wayne and Jacket, {\relax LD} and Baird, Henry S},
  year = {1990},
  volume = {2},
  pages = {35--40},
  organization = {IEEE},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PJX97U92/Handwritten zip code recognition with multilayer networks - Pattern Recognition, 1990. Proceedings., 10th International Conference on.pdf}
}

@inproceedings{lee_deep_2018,
  title = {Deep {{Neural Networks}} as {{Gaussian Processes}}},
  booktitle = {{{ICLR}}},
  author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and {Sohl-Dickstein}, Jascha},
  year = {2018},
  eprint = {1711.00165},
  primaryclass = {cs, stat},
  abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/THTW8BDL/Lee et al. - 2018 - Deep Neural Networks as Gaussian Processes.pdf}
}

@inproceedings{lee_generalizing_2016,
  title = {Generalizing {{Pooling Functions}} in {{Convolutional Neural Networks}}: {{Mixed}}, {{Gated}}, and {{Tree}}},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Lee, Chen-Yu and Gallagher, Patrick W and Tu, Zhuowen},
  year = {2016},
  pages = {9},
  abstract = {We seek to improve deep neural networks by generalizing the pooling operations that play a central role in current architectures. We pursue a careful exploration of approaches to allow pooling to learn and to adapt to complex and variable patterns. The two primary directions lie in (1) learning a pooling function via (two strategies of) combining of max and average pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling filters that are themselves learned. In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These benefits come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2FNBG7JT/Lee et al. - Generalizing Pooling Functions in Convolutional Ne.pdf}
}

@article{lee_introduction_2001,
  title = {Introduction to {{Smooth Manifolds}}},
  author = {Lee, John M},
  year = {2001},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7CG68R7K/Lee - INTRODUCTION TO SMOOTH MANIFOLDS.pdf}
}

@article{lee_wide_2020,
  title = {Wide Neural Networks of Any Depth Evolve as Linear Models under Gradient Descent},
  author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S. and Bahri, Yasaman and Novak, Roman and {Sohl-Dickstein}, Jascha and Pennington, Jeffrey},
  year = {2020},
  month = dec,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2020},
  number = {12},
  pages = {124002},
  publisher = {IOP Publishing},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/abc62b},
  urldate = {2021-05-04},
  abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks (NNs) have made a theory of learning dynamics elusive. In this work, we show that for wide NNs the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian NNs and Gaussian processes (GPs), gradient-based training of wide NNs with a squared loss produces test set predictions drawn from a GP with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YFK96V3P/Lee et al. - 2020 - Wide neural networks of any depth evolve as linear.pdf}
}

@inproceedings{lejeune_implicit_2020,
  title = {The Implicit Regularization of Ordinary Least Squares Ensembles},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {LeJeune, Daniel and Javadi, Hamid and Baraniuk, Richard},
  editor = {Chiappa, Silvia and Calandra, Roberto},
  year = {2020},
  series = {{{PMLR}}},
  volume = {108},
  pages = {3525--3535},
  abstract = {Ensemble methods that average over a collection of independent predictors that are each limited to a subsampling of both the examples and features of the training data command a significant presence in machine learning, such as the ever-popular random forest, yet the nature of the subsampling effect, particularly of the features, is not well understood. We study the case of an ensemble of linear predictors, where each individual predictor is fit using ordinary least squares on a random submatrix of the data matrix. We show that, under standard Gaussianity assumptions, when the number of features selected for each predictor is optimally tuned, the asymptotic risk of a large ensemble is equal to the asymptotic ridge regression risk, which is known to be optimal among linear predictors in this setting. In addition to eliciting this implicit regularization that results from subsampling, we also connect this ensemble to the dropout technique used in training deep (neural) networks, another strategy that has been shown to have a ridge-like regularizing effect.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Z4FGGBLD/LeJeune et al. - The Implicit Regularization of Ordinary Least Squa.pdf}
}

@article{lenders_trlib_2016,
  title = {Trlib: {{A}} Vector-Free Implementation of the {{GLTR}} Method for Iterative Solution of the Trust Region Problem},
  author = {Lenders, Felix and Kirches, Christian and Potschka, Andreas},
  year = {2016},
  journal = {arXiv preprint arXiv:1611.04718},
  eprint = {1611.04718},
  archiveprefix = {arXiv},
  keywords = {{35Q90, 65K05, 90C20, 90C30, 97N90},ðŸ”No DOI found,Mathematics - Optimization and Control},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DF9BE7FE/lenders_trlib_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/UKJPNWPB/1611.html}
}

@article{lensink_fully_2019,
  title = {Fully {{Hyperbolic Convolutional Neural Networks}}},
  author = {Lensink, Keegan and Haber, Eldad and Peters, Bas},
  year = {2019},
  month = sep,
  journal = {arXiv:1905.10484 [cs]},
  eprint = {1905.10484},
  primaryclass = {cs},
  urldate = {2020-07-07},
  abstract = {Convolutional Neural Networks (CNN) have recently seen tremendous success in various computer vision tasks. However, their application to problems with high dimensional input and output, such as high-resolution image and video segmentation or 3D medical imaging, has been limited by various factors. Primarily, in the training stage, it is necessary to store network activations for back propagation. In these settings, the memory requirements associated with storing activations can exceed what is feasible with current hardware, especially for problems in 3D. Previously proposed reversible architectures allow one to recalculate activations in the backwards pass instead of storing them. For computer visions tasks, only block reversible networks have been possible because pooling operations are not reversible. Block-reversibility still requires storing a number of activations that grows with the number of blocks. Motivated by the propagation of signals over physical networks, that are governed by the hyperbolic Telegraph equation, in this work we introduce a fully conservative hyperbolic network for problems with high dimensional input and output. We introduce a coarsening operation that allows completely reversible CNNs by using the Discrete Wavelet Transform and its inverse to both coarsen and interpolate the network state and change the number of channels. This means that during training we do not need to store any of the activations from the forward pass, and can train arbitrarily deep networks. We show that fully reversible networks are able to achieve results comparable to the state of the art in image depth estimation and full 3D video segmentation, with a much lower memory footprint that is a constant independent of the network depth.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/US2DGU8V/Lensink et al. - 2019 - Fully Hyperbolic Convolutional Neural Networks.pdf;/Users/antoniohortaribeiro/Zotero/storage/S7A6HRQY/1905.html}
}

@book{leonhard_control_2001,
  title = {Control of Electrical Drives},
  author = {Leonhard, Werner},
  year = {2001},
  publisher = {Springer Science \& Business Media},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2BHNWUZA/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/2CWRCFPG/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/6FQ2ZFCM/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/A2AGCQ5W/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/GMMS82QW/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/GSCKDQPT/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/HH7TX7VA/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/HUHIBEFQ/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/HWHT9C66/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/KAEIN7RQ/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/N8P7HV55/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/P9FXZP8G/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/QNK2USXZ/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/SD6G43AC/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/SS8SVGQH/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/SXUZQPH8/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/UPCUZXPK/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/XEB9R4XK/leonhard_control_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/XQPEB8EX/leonhard_control_2001.pdf}
}

@article{leonov_lyapunov_1992,
  title = {Lyapunov's Direct Method in the Estimation of the {{Hausdorff}} Dimension of Attractors},
  author = {Leonov, G. A. and Boichenko, V. A.},
  year = {1992},
  month = jan,
  journal = {Acta Applicandae Mathematicae},
  volume = {26},
  number = {1},
  pages = {1--60},
  issn = {0167-8019, 1572-9036},
  doi = {10.1007/BF00046607},
  urldate = {2019-12-27},
  abstract = {This paper surveys results of the authors and others concerningestimatesfor the Hausdorffdimensionof strange attractors,particularlyin the case of (generalized)Lorenz systems and ROsslersystems. A key idea is the interpretationof Hausdorffmeasure as an analogue of a Lyapunovfunction.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8IZHFJCM/Leonov and Boichenko - 1992 - Lyapunov's direct method in the estimation of the .pdf}
}

@article{leontaritis_experimental_1987,
  title = {Experimental Design and Identifiability for Non-Linear Systems},
  author = {Leontaritis, I. J. and Billings, S. A.},
  year = {1987},
  journal = {International Journal of Systems Science},
  volume = {18},
  number = {1},
  pages = {189--202},
  doi = {10.1080/00207728708963958}
}

@article{levenberg_method_1944,
  title = {A Method for the Solution of Certain Non--Linear Problems in Least Squares},
  author = {Levenberg, Kenneth},
  year = {1944},
  journal = {Journal of the Society for Industrial and Applied Mathematics},
  keywords = {ðŸ”No DOI found}
}

@incollection{levy_online_2018,
  title = {Online {{Adaptive Methods}}, {{Universality}} and {{Acceleration}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Levy, Yehuda Kfir and Yurtsever, Alp and Cevher, Volkan},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {6501--6510},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-13},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WW9HTLSY/levy_online_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/A8KM5YCP/7885-online-adaptive-methods-universality-and-acceleration.html}
}

@article{lewis_polygenic_2020,
  title = {Polygenic Risk Scores: From Research Tools to Clinical Instruments},
  author = {Lewis, Cathryn M and Vassos, Evangelos},
  year = {2020},
  journal = {Genome medicine},
  volume = {12},
  number = {1},
  pages = {1--11},
  publisher = {BioMed Central},
  issn = {1756-994X}
}

@book{lewis_robot_2004,
  title = {Robot Manipulator Control: Theory and Practice},
  shorttitle = {Robot Manipulator Control},
  author = {Lewis, Frank L. and Abdallah, C. T. and Dawson, D. M. and Lewis, Frank L.},
  year = {2004},
  series = {Control Engineering Series},
  edition = {2nd ed., rev. and expanded},
  publisher = {Marcel Dekker},
  address = {New York},
  isbn = {978-0-8247-4072-6},
  lccn = {TJ211.35 .L48 2004},
  keywords = {Automatic control,Control systems,Manipulators (Mechanism),Robots},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FUE93A68/lewis_robot_2004.pdf}
}

@inproceedings{lezcano-casado_cheap_2019,
  title = {Cheap {{Orthogonal Constraints}} in {{Neural Networks}}: {{A Simple Parametrization}} of the {{Orthogonal}} and {{Unitary Group}}},
  shorttitle = {Cheap {{Orthogonal Constraints}} in {{Neural Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {{Lezcano-Casado}, Mario and {Mart{\'i}nez-Rubio}, David},
  year = {2019},
  eprint = {1901.08428},
  pages = {3794--3803},
  urldate = {2019-09-19},
  abstract = {We introduce a novel approach to perform first-order optimization with orthogonal and unitary constraints. This approach is based on a parametrization stemming from Lie group theory through the exponential map. The parametrization transforms the constrained optimization problem into an unconstrained one over a Euclidean space, for which common first-order optimization methods can be used. The theoretical results presented are general enough to cover the special orthogonal group, the unitary group and, in general, any connected compact Lie group. We discuss how this and other parametrizations can be computed efficiently through an implementation trick, making numerically complex parametrizations usable at a negligible runtime cost in neural networks. In particular, we apply our results to RNNs with orthogonal recurrent weights, yielding a new architecture called expRNN. We demonstrate how our method constitutes a more robust approach to optimization with orthogonal constraints, showing faster, accurate, and more stable convergence in several tasks designed to test RNNs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {â›” No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UIRPDF4C/Lezcano-Casado and MartÃ­nez-Rubio - 2019 - Cheap Orthogonal Constraints in Neural Networks A.pdf}
}

@article{lezcano-casado_trivializations_2019,
  title = {Trivializations for {{Gradient-Based Optimization}} on {{Manifolds}}},
  author = {{Lezcano-Casado}, Mario},
  year = {2019},
  journal = {Advances in Neural Information Processing Systems}
}

@incollection{li_learning_2018,
  title = {Learning {{Overparameterized Neural Networks}} via {{Stochastic Gradient Descent}} on {{Structured Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Li, Yuanzhi and Liang, Yingyu},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {8167--8176},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-04},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2IKBXWD6/220cd(04-10-05)-04-10-20-12563-Learning_Overpa.pdf;/Users/antoniohortaribeiro/Zotero/storage/8ZNYBMTB/li_learning_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/CMBWS8Y8/8038-learning-overparameterized-neural-networks-via-stochastic-gradient-descent-on-structured-d.html}
}

@article{li_learning_2019,
  title = {Learning {{Overparameterized Neural Networks}} via {{Stochastic Gradient Descent}} on {{Structured Data}}},
  author = {Li, Yuanzhi and Liang, Yingyu},
  year = {2019},
  month = aug,
  journal = {arXiv:1808.01204 [cs, stat]},
  eprint = {1808.01204},
  primaryclass = {cs, stat},
  urldate = {2020-08-10},
  abstract = {Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KP5IK8D7/Li and Liang - 2019 - Learning Overparameterized Neural Networks via Sto.pdf;/Users/antoniohortaribeiro/Zotero/storage/Q4EYAAKG/1808.html}
}

@article{li_limit_2011,
  title = {Limit {{Distributions}} of {{Eigenvalues}} for {{Random Block Toeplitz}} and {{Hankel Matrices}}},
  author = {Li, Yi-Ting and Liu, Dang-Zheng and Wang, Zheng-Dong},
  year = {2011},
  month = dec,
  journal = {Journal of Theoretical Probability},
  volume = {24},
  number = {4},
  pages = {1063--1086},
  issn = {0894-9840, 1572-9230},
  doi = {10.1007/s10959-010-0326-3},
  urldate = {2020-11-14},
  abstract = {Block Toeplitz and Hankel matrices arise in many aspects of applications. In this paper, we will research the distributions of eigenvalues for some models and get the semicircle law. Firstly we will give trace formulas of block Toeplitz and Hankel matrix. Then we will prove that the almost sure limit {$\gamma$}T(m) ({$\gamma$}H(m)) of eigenvalue distributions of random block Toeplitz (Hankel) matrices exist and give the moments of the limit distributions where m is the order of the blocks. Then we will prove the existence of almost sure limit of eigenvalue distributions of random block Toeplitz and Hankel band matrices and give the moments of the limit distributions. Finally we will prove that {$\gamma$}T(m) ({$\gamma$}H(m)) converges weakly to the semicircle law as m {$\rightarrow$} {$\infty$}.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5G2PZD4X/Li et al. - 2011 - Limit Distributions of Eigenvalues for Random Bloc.pdf}
}

@article{li_neurofuzzy_2017,
  title = {Neuro-Fuzzy Based Identification Method for {{Hammerstein}} Output Error Model with Colored Noise},
  author = {Li, Feng and Jia, Li and Peng, Daogang and Han, Chao},
  year = {2017},
  month = jun,
  journal = {Neurocomputing},
  volume = {244},
  pages = {90--101},
  issn = {09252312},
  doi = {10.1016/j.neucom.2017.03.026},
  urldate = {2017-08-23},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SZ85IGJR/li_neuro-fuzz_2017.pdf}
}

@article{li_semisupervised_2018,
  title = {Semi-Supervised {{Rare Disease Detection Using Generative Adversarial Network}}},
  author = {Li, Wenyuan and Wang, Yunlong and Cai, Yong and Arnold, Corey and Zhao, Emily and Yuan, Yilian},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.00547 [cs, stat]},
  eprint = {1812.00547},
  primaryclass = {cs, stat},
  urldate = {2018-12-13},
  abstract = {Rare diseases affect a relatively small number of people, which limits investment in research for treatments and cures. Developing an efficient method for rare disease detection is a crucial first step towards subsequent clinical research. In this paper, we present a semi-supervised learning framework for rare disease detection using generative adversarial networks. Our method takes advantage of the large amount of unlabeled data for disease detection and achieves the best results in terms of precision-recall score compared to baseline techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/L24F3DIY/li_semi-super_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/2CW8CYGB/1812.html}
}

@article{li_visualizing_2017,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.09913 [cs, stat]},
  eprint = {1712.09913},
  primaryclass = {cs, stat},
  urldate = {2019-04-12},
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CHNM7SXN/li_visualizin_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/PX9CW9DT/1712.html}
}

@incollection{li_visualizing_2018,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {6389--6399},
  publisher = {Curran Associates, Inc.},
  urldate = {2019-04-09},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FWLUC9L4/li_visualizin_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/EVEMA3NJ/7875-visualizing-the-loss-landscape-of-neural-nets.html}
}

@inproceedings{liang_multiple_2020,
  title = {On the Multiple Descent of Minimum-Norm Interpolants and Restricted Lower Isometry of Kernels},
  booktitle = {Proceedings of Thirty Third Conference on Learning Theory},
  author = {Liang, Tengyuan and Rakhlin, Alexander and Zhai, Xiyu},
  editor = {Abernethy, Jacob and Agarwal, Shivani},
  year = {2020-07-09/2020-07-12},
  series = {Proceedings of Machine Learning Research},
  volume = {125},
  pages = {2683--2711},
  publisher = {PMLR},
  abstract = {We study the risk of minimum-norm interpolants of data in Reproducing Kernel Hilbert Spaces. Our upper bounds on the risk are of a multiple-descent shape for the various scalings of d = n\textsuperscript{{$\alpha$}}, {$\alpha\in$}(0,1), for the input dimension d and sample size n. Empirical evidence supports our finding that minimum-norm interpolants in RKHS can exhibit this unusual non-monotonicity in sample size; furthermore, locations of the peaks in our experiments match our theoretical predictions. Since gradient flow on appropriately initialized wide neural networks converges to a minimum-norm interpolant with respect to a certain kernel, our analysis also yields novel estimation and generalization guarantees for these over-parametrized models. At the heart of our analysis is a study of spectral properties of the random kernel matrix restricted to a filtration of eigen-spaces of the population covariance operator, and may be of independent interest.},
  pdf = {http://proceedings.mlr.press/v125/liang20a/liang20a.pdf},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5AFS9PW2/Liang et al. - 2020 - On the multiple descent of minimum-norm interpolan.pdf}
}

@article{lima_deep_2021,
  title = {Deep Neural Network Estimated Electrocardiographic-Age as a Mortality Predictor},
  author = {Lima, Emilly M. and Ribeiro, Ant{\^o}nio H. and Paix{\~a}o, Gabriela M M and Ribeiro, Manoel Horta and Filho, Marcelo M. Pinto and Gomes, Paulo R. and Oliveira, Derick M. and Sabino, Ester C. and Duncan, Bruce B. and Giatti, Luana and Barreto, Sandhi M. and Meira, Wagner and Sch{\"o}n, Thomas B. and Ribeiro, Antonio Luiz P.},
  year = {2021},
  journal = {Nature Communications},
  volume = {12},
  doi = {10.1038/s41467-021-25351-7},
  abstract = {The electrocardiogram (ECG) is the most commonly used exam for the screening and evaluation of cardiovascular diseases. Here we propose that the age predicted by artificial intelligence (AI) from the raw ECG tracing (ECG-age) can be a measure of cardiovascular health and provide prognostic information. A deep convolutional neural network was trained to predict a patient9s age from the 12-lead ECG using data from patients that underwent an ECG from 2010 to 2017 - the CODE study cohort (n=1,558,415 patients). On the 15\% hold-out CODE test split, patients with ECG-age more than 8 years greater than chronological age had a higher mortality rate (hazard ratio (HR) 1.79, p\&lt;0.001) in a mean follow-up of 3.67 years, whereas those with ECG-age more than 8 years less than chronological age had a lower mortality rate (HR 0.78, p\&lt;0.001). Similar results were obtained in the external cohorts ELSA-Brasil (n=14,236) and SaMi-Trop (n=1,631). The ability to predict mortality from the ECG predicted age remains even when we adjust the model for cardiovascular risk factors. Moreover, even for apparent normal ECGs, having a predicted ECG-age 8 or more years greater than chronological age remained a statistically significant predictor of risk (HR 1.53, p\&lt;0.001 in CODE 15\% test split). These results show that AI-enabled analysis of the ECG can add prognostic information to the interpretation of the 12-lead ECGs.},
  annotation = {medRxiv doi: 10.1101/2021.02.19.21251232},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BXQAMNJG/Lima et al_2021_Deep neural network estimated electrocardiographic-age as a mortality predictor.pdf;/Users/antoniohortaribeiro/Zotero/storage/RVQZR9WL/2021.02.19.html}
}

@article{lin_aienabled_2024,
  title = {{{AI-enabled}} Electrocardiography Alert Intervention and All-Cause Mortality: A Pragmatic Randomized Clinical Trial},
  shorttitle = {{{AI-enabled}} Electrocardiography Alert Intervention and All-Cause Mortality},
  author = {Lin, Chin-Sheng and Liu, Wei-Ting and Tsai, Dung-Jang and Lou, Yu-Sheng and Chang, Chiao-Hsiang and Lee, Chiao-Chin and Fang, Wen-Hui and Wang, Chih-Chia and Chen, Yen-Yuan and Lin, Wei-Shiang and Cheng, Cheng-Chung and Lee, Chia-Cheng and Wang, Chih-Hung and Tsai, Chien-Sung and Lin, Shih-Hua and Lin, Chin},
  year = {2024},
  month = may,
  journal = {Nature Medicine},
  volume = {30},
  number = {5},
  pages = {1461--1470},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-024-02961-4},
  urldate = {2024-06-08},
  abstract = {The early identification of vulnerable patients has the potential to improve outcomes but poses a substantial challenge in clinical practice. This study evaluated the ability of an artificial intelligence (AI)-enabled electrocardiogram (ECG) to identify hospitalized patients with a high risk of mortality in a multisite randomized controlled trial involving 39 physicians and 15,965 patients. The AI-ECG alert intervention included an AI report and warning messages delivered to the physicians, flagging patients predicted to be at high risk of mortality. The trial met its primary outcome, finding that implementation of the AI-ECG alert was associated with a significant reduction in all-cause mortality within 90 days: 3.6\% patients in the intervention group died within 90 days, compared to 4.3\% in the control group (4.3\%) (hazard ratio (HR)\,=\,0.83, 95\% confidence interval (CI)\,=\,0.70--0.99). A prespecified analysis showed that reduction in all-cause mortality associated with the AI-ECG alert was observed primarily in patients with high-risk ECGs (HR\,=\,0.69, 95\% CI\,=\,0.53--0.90). In analyses of secondary outcomes, patients in the intervention group with high-risk ECGs received increased levels of intensive care compared to the control group; for the high-risk ECG group of patients, implementation of the AI-ECG alert was associated with a significant reduction in the risk of cardiac death (0.2\% in the intervention arm versus 2.4\% in the control arm, HR\,=\,0.07, 95\% CI\,=\,0.01--0.56). While the precise means by which implementation of the AI-ECG alert led to decreased mortality are to be fully elucidated, these results indicate that such implementation assists in the detection of high-risk patients, prompting timely clinical care and reducing mortality. ClinicalTrials.gov registration: NCT05118035.},
  copyright = {2024 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Prognosis,Prognostic markers},
  file = {/Users/antoniohortaribeiro/Zotero/storage/C6KLFB4H/Lin et al. - 2024 - AI-enabled electrocardiography alert intervention .pdf}
}

@article{lin_how_1998,
  title = {How Embedded Memory in Recurrent Neural Network Architectures Helps Learning Long-Term Temporal Dependencies},
  author = {Lin, Tsungnan and Horne, Bill G. and Giles, C. Lee},
  year = {1998},
  month = jul,
  journal = {Neural Networks},
  volume = {11},
  number = {5},
  pages = {861--868},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(98)00018-5},
  abstract = {Learning long-term temporal dependencies with recurrent neural networks can be a difficult problem. It has recently been shown that a class of recurrent neural networks called NARX networks perform much better than conventional recurrent neural networks for learning certain simple long-term dependency problems. The intuitive explanation for this behavior is that the output memories of a NARX network can be manifested as jump-ahead connections in the time-unfolded network. These jump-ahead connections can propagate gradient information more efficiently, thus reducing the sensitivity of the network to long-term dependencies. This work gives empirical justification to our hypothesis that similar improvements in learning long-term dependencies can be achieved with other classes of recurrent neural network axchitectures simply by increasing the order of the embedded memory. In particular we explore the impact of learning simple long-term dependency problems on three classes of recurrent neural network architectures: globally recurrent networks, locally recurrent networks, and NARX (output feedback) networks. Comparing the performance of these architectures with different orders of embedded memory on two simple long-term dependencies problems shows that all of these classes of network architectures demonstrate significant improvement on learning long-term dependencies when the orders of embedded memory are increased. These results can be important to a user comfortable with a specific recurrent neural network architecture because simply increasing the embedding memory order of that architecture will make it more robust to the problem of long-term dependency learning.},
  keywords = {Automata,Gradient-descent,Latching,long-term dependencies,Memory,Neural networks,Recurrent,Time-delay,Training},
  file = {/Users/antoniohortaribeiro/Zotero/storage/A9DMNC6Z/lin_how_1998.pdf;/Users/antoniohortaribeiro/Zotero/storage/C7U4X7FQ/lin_how_1998.pdf;/Users/antoniohortaribeiro/Zotero/storage/5DCIUW46/S0893608098000185.html;/Users/antoniohortaribeiro/Zotero/storage/MACZMSSI/S0893608098000185.html}
}

@article{lin_learning_1996,
  title = {Learning Long-Term Dependencies in {{NARX}} Recurrent Neural Networks},
  author = {Lin, Tsungnan and Horne, B. G. and Tino, P. and Giles, C. L.},
  year = {1996},
  month = nov,
  journal = {IEEE Transactions on Neural Networks},
  volume = {7},
  number = {6},
  pages = {1329--1338},
  issn = {1045-9227},
  doi = {10.1109/72.548162},
  abstract = {It has previously been shown that gradient-descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. We show that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities. We have previously reported that gradient descent learning can be more effective in NARX networks than in recurrent neural network architectures that have ``hidden states'' on problems including grammatical inference and nonlinear system identification. Typically, the network converges much faster and generalizes better than other networks. The results in this paper are consistent with this phenomenon. We present some experimental results which show that NARX networks can often retain information for two to three times as long as conventional recurrent neural networks. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on long-term dependency problems. We also describe in detail some of the assumptions regarding what it means to latch information robustly and suggest possible ways to loosen these assumptions},
  keywords = {autoregressive processes,Computer science,Ear,generalisation (artificial intelligence),grammatical inference,information latching,information retention,Intelligent networks,learning (artificial intelligence),long-term dependencies,NARX recurrent neural networks,National electric code,Neural networks,nonlinear autoregressive models with exogenous recurrent neural networks,nonlinear dynamical systems,Nonlinear system identification,Power system modeling,recurrent neural nets,Recurrent neural networks,representational capabilities,Robustness,system identification},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RS29CGK4/lin_learning_1996.pdf;/Users/antoniohortaribeiro/Zotero/storage/572RZUGK/548162.html}
}

@article{lin_resnet_2018,
  title = {{{ResNet}} with One-Neuron Hidden Layers Is a {{Universal Approximator}}},
  author = {Lin, Hongzhou and Jegelka, Stefanie},
  year = {2018},
  month = jun,
  urldate = {2018-12-06},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q4JDAT9U/lin_resnet_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/BSWEFQTL/1806.html}
}

@article{lin_structured_2017,
  title = {A {{Structured Self-attentive Sentence Embedding}}},
  author = {Lin, Zhouhan and Feng, Minwei and dos Santos, Cicero Nogueira and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.03130 [cs]},
  eprint = {1703.03130},
  primaryclass = {cs},
  urldate = {2019-05-29},
  abstract = {This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/A57CDDTS/lin_a_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/ZVCEUVY3/1703.html}
}

@book{lindholm_machine_2022,
  title = {Machine {{Learning}}: {{A First Course}} for {{Engineers}} and {{Scientists}}},
  author = {Lindholm, Andreas and Wahlstr{\"o}m, Niklas and Lindsten, Fredrik and Sch{\"o}n, Thomas B.},
  year = {2022},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  abstract = {This book introduces machine learning for readers with some background in basic linear algebra, statistics, probability, and programming. In a coherent statistical framework it covers a selection of supervised machine learning methods, from the most fundamental (k-NN, decision trees, linear and logistic regression) to more advanced methods (deep neural networks, support vector machines, Gaussian processes, random forests and boosting), plus commonly-used unsupervised methods (generative modeling, k-means, PCA, autoencoders and generative adversarial networks). Careful explanations and pseudo-code are presented for all methods. The authors maintain a focus on the fundamentals by drawing connections between methods and discussing general concepts such as loss functions, maximum likelihood, the bias-variance decomposition, ensemble averaging, kernels and the Bayesian approach along with generally useful tools such as regularization, cross validation, evaluation metrics and optimization methods. The final chapters offer practical advice for solving real-world supervised machine learning problems and on ethical aspects of modern machine learning.},
  isbn = {978-1-108-84360-7}
}

@article{lindow_heart_2023,
  title = {Heart Age Gap by Explainable Advanced Electrocardiography Is Associated with Cardiovascular Risk Factors and Survival},
  author = {Lindow, Thomas and Maanja, Maren and Schelbert, Erik B and Ribeiro, Antonio H and Ribeiro, Antonio Luiz P and Schlegel, Todd T and Ugander, Martin},
  year = {2023},
  journal = {European Heart Journal - Digital Health},
  doi = {10.1093/ehjdh/ztad045},
  copyright = {All rights reserved}
}

@article{lindsten_particle_2014,
  title = {Particle {{Gibbs}} with {{Ancestor Sampling}}},
  author = {Lindsten, Fredrik and Jordan, Michael I and Schon, Thomas B},
  year = {2014},
  pages = {40},
  abstract = {Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combining the two main tools used for Monte Carlo statistical inference: sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). We present a new PMCMC algorithm that we refer to as particle Gibbs with ancestor sampling (PGAS). PGAS provides the data analyst with an off-the-shelf class of Markov kernels that can be used to simulate, for instance, the typically high-dimensional and highly autocorrelated state trajectory in a state-space model. The ancestor sampling procedure enables fast mixing of the PGAS kernel even when using seemingly few particles in the underlying SMC sampler. This is important as it can significantly reduce the computational burden that is typically associated with using SMC. PGAS is conceptually similar to the existing PG with backward simulation (PGBS) procedure. Instead of using separate forward and backward sweeps as in PGBS, however, we achieve the same effect in a single forward sweep. This makes PGAS well suited for addressing inference problems not only in state-space models, but also in models with more complex dependencies, such as non-Markovian, Bayesian nonparametric, and general probabilistic graphical models.},
  langid = {english},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/X5S35ILE/Lindsten et al. - Particle Gibbs with Ancestor Sampling.pdf}
}

@article{ling_spectrum_2019,
  title = {Spectrum {{Concentration}} in {{Deep Residual Learning}}: {{A Free Probability Approach}}},
  shorttitle = {Spectrum {{Concentration}} in {{Deep Residual Learning}}},
  author = {Ling, Zenan and Qiu, Robert C.},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {105212--105223},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2931991},
  abstract = {We revisit the weight initialization of deep residual networks (ResNets) by introducing a novel analytical tool in free probability to the community of deep learning. This tool deals with the limiting spectral distribution of non-Hermitian random matrices, rather than their conventional Hermitian counterparts in the literature. This new tool enables us to evaluate the singular value spectrum of the input-output Jacobian of a fully connected deep ResNet in both linear and nonlinear cases. With the powerful tool of free probability, we conduct an asymptotic analysis of the (limiting) spectrum on the single-layer case, and then extend this analysis to the multi-layer case of an arbitrary number of layers. The asymptotic analysis illustrates the necessity and university of rescaling the classical random initialization by the number of residual units L, so that the squared singular value of the associated Jacobian remains of order O(1), when compared with the large width and depth of the network. We empirically demonstrate that the proposed initialization scheme learns at a speed of orders of magnitudes faster than the classical ones, and thus attests a strong practical relevance of this investigation.},
  keywords = {Eigenvalues and eigenfunctions,Jacobian matrices,Jacobian matrix,Limiting,Neural networks,non-Hermitian free probability theory,random matrix theory,Random variables,Residual network,spectral density,Tools,Training,weight initialization},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5QM9B8NA/Ling and Qiu - 2019 - Spectrum Concentration in Deep Residual Learning .pdf}
}

@article{lipton_troubling_2018,
  title = {Troubling {{Trends}} in {{Machine Learning Scholarship}}},
  author = {Lipton, Zachary C. and Steinhardt, Jacob},
  year = {2018},
  journal = {arXiv preprint arXiv:1807.03341},
  eprint = {1807.03341},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UQVT5A45/lipton_troubling_2018.pdf}
}

@incollection{liu_adaptive_2018,
  title = {Adaptive {{Negative Curvature Descent}} with {{Applications}} in {{Non-convex Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Liu, Mingrui and Li, Zhe and Wang, Xiaoyu and Yi, Jinfeng and Yang, Tianbao},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {4858--4867},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-13},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EAXRR4VX/liu_adaptive_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/9L7RDZBL/7734-adaptive-negative-curvature-descent-with-applications-in-non-convex-optimization.html}
}

@inproceedings{liu_blockwise_2009,
  title = {Blockwise Coordinate Descent Procedures for the Multi-Task Lasso, with Applications to Neural Semantic Basis Discovery},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Liu, Han and Palatucci, Mark and Zhang, Jian},
  year = {2009},
  pages = {649--656},
  publisher = {ACM},
  urldate = {2017-09-18},
  file = {/Users/antoniohortaribeiro/Zotero/storage/99CRWDWB/liu_blockwise_2009.pdf}
}

@misc{liu_double_2022,
  title = {On the {{Double Descent}} of {{Random Features Models Trained}} with {{SGD}}},
  author = {Liu, Fanghui and Suykens, Johan A. K. and Cevher, Volkan},
  year = {2022},
  month = oct,
  number = {arXiv:2110.06910},
  eprint = {2110.06910},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.06910},
  urldate = {2022-11-25},
  abstract = {We study generalization properties of random features (RF) regression in high dimensions optimized by stochastic gradient descent (SGD) in under-/over-parameterized regime. In this work, we derive precise non-asymptotic error bounds of RF regression under both constant and polynomial-decay step-size SGD setting, and observe the double descent phenomenon both theoretically and empirically. Our analysis shows how to cope with multiple randomness sources of initialization, label noise, and data sampling (as well as stochastic gradients) with no closed-form solution, and also goes beyond the commonly-used Gaussian/spherical data assumption. Our theoretical results demonstrate that, with SGD training, RF regression still generalizes well for interpolation learning, and is able to characterize the double descent behavior by the unimodality of variance and monotonic decrease of bias. Besides, we also prove that the constant step-size SGD setting incurs no loss in convergence rate when compared to the exact minimum-norm interpolator, as a theoretical justification of using SGD in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/L6G2C3GI/Liu et al. - 2022 - On the Double Descent of Random Features Models Tr.pdf;/Users/antoniohortaribeiro/Zotero/storage/DRRWIWIZ/2110.html}
}

@incollection{liu_frequencydomain_2018,
  title = {Frequency-{{Domain Dynamic Pruning}} for {{Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Liu, Zhenhua and Xu, Jizheng and Peng, Xiulian and Xiong, Ruiqin},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {1051--1061},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-10},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SKIM8VM6/liu_frequency-_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/W3HRANMH/7382-frequency-domain-dynamic-pruning-for-convolutional-neural-networks.html}
}

@article{liu_fuzzy_2016,
  title = {Fuzzy Approximation-Based Adaptive Backstepping Optimal Control for a Class of Nonlinear Discrete-Time Systems with Dead-Zone},
  author = {Liu, Yan-Jun and Gao, Ying and Tong, Shaocheng and Li, Yongming},
  year = {2016},
  journal = {IEEE Transactions on Fuzzy Systems},
  volume = {24},
  number = {1},
  pages = {16--28},
  issn = {1063-6706},
  doi = {10.1109/TFUZZ.2015.2418000}
}

@inproceedings{liu_kernel_2021,
  title = {Kernel Regression in High Dimensions: {{Refined}} Analysis beyond Double Descent},
  shorttitle = {Kernel Regression in High Dimensions},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Liu, Fanghui and Liao, Zhenyu and Suykens, Johan},
  year = {2021},
  month = mar,
  pages = {649--657},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2021-06-29},
  abstract = {In this paper, we provide a precise characterization of generalization properties of high dimensional kernel ridge regression across the under- and over-parameterized regimes, depending on whether...},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8P7WYGIR/Liu et al. - 2021 - Kernel regression in high dimensions Refined anal.pdf;/Users/antoniohortaribeiro/Zotero/storage/66I9USDI/liu21b.html}
}

@article{liu_limited_1989,
  title = {On the Limited Memory {{BFGS}} Method for Large Scale Optimization},
  author = {Liu, Dong C and Nocedal, Jorge},
  year = {1989},
  journal = {Mathematical programming},
  volume = {45},
  number = {1-3},
  pages = {503--528},
  urldate = {2020-02-07},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AXTB3GX6/liu89limited.pdf}
}

@article{liu_multilevel_2019,
  title = {Multi-{{Level Wavelet Convolutional Neural Networks}}},
  author = {Liu, Pengju and Zhang, Hongzhi and Lian, Wei and Zuo, Wangmeng},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {74973--74985},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2921451},
  abstract = {In computer vision, convolutional networks (CNNs) often adopt pooling to enlarge receptive field which has the advantage of low computational complexity. However, pooling can cause information loss and thus is detrimental to further operations such as features extraction and analysis. Recently, dilated filter has been proposed to tradeoff between receptive field size and efficiency. But the accompanying gridding effect can cause a sparse sampling of input images with checkerboard patterns. To address this problem, in this paper, we propose a novel multi-level wavelet CNN (MWCNN) model to achieve a better tradeoff between receptive field size and computational efficiency. The core idea is to embed wavelet transform into CNN architecture to reduce the resolution of feature maps while at the same time, increasing receptive field. Specifically, MWCNN for image restoration is based on U-Net architecture, and inverse wavelet transform (IWT) is deployed to reconstruct the high resolution (HR) feature maps. The proposed MWCNN can also be viewed as an improvement of dilated filter and a generalization of average pooling and can be applied to not only image restoration tasks, but also any CNNs requiring a pooling operation. The experimental results demonstrate the effectiveness of the proposed MWCNN for tasks, such as image denoising, single image super-resolution, JPEG image artifacts removal and object classification.},
  keywords = {average pooling generalization,checkerboard patterns,CNN architecture,computational efficiency,Computer architecture,computer vision,Convolutional networks,convolutional neural nets,dilated filter,Discrete wavelet transforms,efficiency,feature extraction,Feature extraction,feature maps,features extraction,filtering theory,gridding effect,image denoising,image resolution,image restoration,Image restoration,image restoration tasks,image sampling,information loss,input images,inverse transforms,inverse wavelet transform,low computational complexity,multi-level wavelet,multilevel wavelet convolutional neural networks,MWCNN,novel multilevel wavelet CNN model,object classification,PEG image artifacts removal,pooling operation,receptive field size,single image super-resolution,sparse sampling,Task analysis,U-Net architecture,wavelet transforms},
  file = {/Users/antoniohortaribeiro/Zotero/storage/26WXA6I8/Liu et al. - 2019 - Multi-Level Wavelet Convolutional Neural Networks.pdf;/Users/antoniohortaribeiro/Zotero/storage/W8FWU4BC/8732332.html}
}

@article{liu_multiview_2013,
  title = {Multiview {{Partitioning}} via {{Tensor Methods}}},
  author = {Liu, X. and Ji, S. and Gl{\"a}nzel, W. and Moor, B. De},
  year = {2013},
  month = may,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {25},
  number = {5},
  pages = {1056--1069},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2012.95},
  abstract = {Clustering by integrating multiview representations has become a crucial issue for knowledge discovery in heterogeneous environments. However, most prior approaches assume that the multiple representations share the same dimension, limiting their applicability to homogeneous environments. In this paper, we present a novel tensor-based framework for integrating heterogeneous multiview data in the context of spectral clustering. Our framework includes two novel formulations; that is multiview clustering based on the integration of the Frobenius-norm objective function (MC-FR-OI) and that based on matrix integration in the Frobenius-norm objective function (MC-FR-MI). We show that the solutions for both formulations can be computed by tensor decompositions. We evaluated our methods on synthetic data and two real-world data sets in comparison with baseline methods. Experimental results demonstrate that the proposed formulations are effective in integrating multiview data in heterogeneous environments.},
  keywords = {Clustering algorithms,Data mining,Frobenius-norm objective function,heterogeneous multiview data,higher order orthogonal iteration,Kernel,knowledge discovery,matrix algebra,Matrix decomposition,matrix integration,MC-FR-MI,MC-FR-OI,multilinear singular value decomposition,multiview clustering,multiview partitioning,multiview representations,novel tensor-based framework,Optimization,pattern clustering,spectral clustering,Tensile stress,tensor decomposition,tensor decompositions,tensor methods,tensors,Tin,Vectors},
  file = {/Users/antoniohortaribeiro/Zotero/storage/43H56UAW/liu_multiview_2013.pdf;/Users/antoniohortaribeiro/Zotero/storage/S46GUDWK/liu_multiview_2013.pdf;/Users/antoniohortaribeiro/Zotero/storage/L5JI8HV6/6193101.html;/Users/antoniohortaribeiro/Zotero/storage/S2JMRSDV/6193101.html;/Users/antoniohortaribeiro/Zotero/storage/UW3TMNRC/6193101.html;/Users/antoniohortaribeiro/Zotero/storage/XQDRKFF9/6193101.html}
}

@article{liu_open_2018,
  title = {An {{Open Access Database}} for {{Evaluating}} the {{Algorithms}} of {{Electrocardiogram Rhythm}} and {{Morphology Abnormality Detection}}},
  author = {Liu, Feifei and Liu, Chengyu and Zhao, Lina and Zhang, Xiangyu and Wu, Xiaoling and Xu, Xiaoyan and Liu, Yulin and Ma, Caiyun and Wei, Shoushui and He, Zhiqiang and Li, Jianqing and Yin Kwee, Eddie Ng},
  year = {2018},
  month = sep,
  journal = {Journal of Medical Imaging and Health Informatics},
  volume = {8},
  number = {7},
  pages = {1368--1373},
  doi = {10.1166/jmihi.2018.2442},
  abstract = {Over the past few decades, methods for classification and detection of rhythm or morphology abnormalities in ECG signals have been widely studied. However, it lacks the comprehensive performance evaluation on an open database. This paper presents a detailed introduction for the database used for the 1st China Physiological Signal Challenge 2018 (CPSC 2018), which will be run as a special section during the ICBEB 2018. CPSC 2018 aims to encourage the development of algorithms to identify the rhythm/morphology abnormalities from 12-lead ECGs. The data used in CPSC 2018 include one normal ECG type and eight abnormal types. This paper details the data source, recording information, patients' clinical baseline parameters as age, gender and so on. Meanwhile, it also presents the commonly used detection/classification methods for the abovementioned abnormal ECG types. We hope this paper could be a guide reference for the CPSC 2018, to facilitate the researchers familiar with the data and the related research advances.},
  keywords = {CPSC,DATABASE,ELECTROCARDIOGRAM (ECG),RHYTHM AND MORPHOLOGY ABNORMAL}
}

@article{liu_reliable_2018,
  title = {Reliable {{Semi-Supervised Learning}} When {{Labels}} Are {{Missing}} at {{Random}}},
  author = {Liu, Xiuming and Zachariah, Dave and W{\aa}gberg, Johan and Sch{\"o}n, Thomas},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.10947 [cs, stat]},
  eprint = {1811.10947},
  primaryclass = {cs, stat},
  urldate = {2019-03-21},
  abstract = {Semi-supervised learning methods are motivated by the availability of large datasets with unlabeled features in addition to labeled data. Unlabeled data is, however, not guaranteed to improve classification performance and has in fact been reported to impair the performance in certain cases. A fundamental source of error arises from restrictive assumptions about the unlabeled features, which result in unreliable classifiers. In this paper, we develop a semi-supervised learning approach that relaxes such assumptions and is capable of providing classifiers that reliably measure the label uncertainty. The approach is applicable using any generative model with a supervised learning algorithm. We illustrate the approach using both handwritten digit and cloth classification data where the labels are missing at random.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZRQBAW3C/liu_reliable_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/3TB4XYPF/1811.html}
}

@article{livan_introduction_2018,
  title = {Introduction to {{Random Matrices}} - {{Theory}} and {{Practice}}},
  author = {Livan, Giacomo and Novaes, Marcel and Vivo, Pierpaolo},
  year = {2018},
  journal = {arXiv:1712.07903 [cond-mat, physics:math-ph]},
  volume = {26},
  eprint = {1712.07903},
  primaryclass = {cond-mat, physics:math-ph},
  doi = {10.1007/978-3-319-70885-0},
  urldate = {2020-11-12},
  abstract = {This is a book for absolute beginners. If you have heard about random matrix theory, commonly denoted RMT, but you do not know what that is, then welcome!, this is the place for you. Our aim is to provide a truly accessible introductory account of RMT for physicists and mathematicians at the beginning of their research career. We tried to write the sort of text we would have loved to read when we were beginning Ph.D. students ourselves. Our book is structured with light and short chapters, and the style is informal. The calculations we found most instructive are spelt out in full. Particular attention is paid to the numerical verification of most analytical results. Our book covers standard material - classical ensembles, orthogonal polynomial techniques, spectral densities and spacings - but also more advanced and modern topics - replica approach and free probability - that are not normally included in elementary accounts on RMT. This book is dedicated to the fond memory of Oriol Bohigas.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematical Physics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZPUBSC3D/Livan et al. - 2018 - Introduction to Random Matrices - Theory and Pract.pdf;/Users/antoniohortaribeiro/Zotero/storage/K873RFNC/1712.html}
}

@article{livne_aeroelasticity_2003,
  title = {Aeroelasticity of {{Nonconventional Airplane Configurations-Past}} and {{Future}}},
  author = {Livne, E. and Weisshaar, Terrence A.},
  year = {2003},
  journal = {Journal of Aircraft},
  volume = {40},
  number = {6},
  pages = {1047--1065},
  doi = {10.2514/2.7217},
  urldate = {2020-01-28},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DT52EQL9/2.html}
}

@inproceedings{liwan_regularization_2013,
  title = {Regularization of {{Neural Networks}} Using {{DropConnect}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {{Li Wan} and {Matthew Zeiler} and {Sixin Zhang} and {Yann Le Cun} and {Rob Fergus}},
  editor = {{Sanjoy Dasgupta} and {David McAllester}},
  year = {2013},
  month = feb,
  pages = {1058--1066},
  publisher = {PMLR},
  abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.}
}

@article{ljung_asymptotic_1980,
  title = {Asymptotic Normality of Prediction Error Estimators for Approximate System Models},
  author = {Ljung, Lennart and Caines, Peter E.},
  year = {1980},
  month = jan,
  journal = {Stochastics},
  volume = {3},
  number = {1-4},
  pages = {29--46},
  issn = {0090-9491},
  doi = {10.1080/17442507908833135},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5RRC9RBI/lennart_asymptotic_1980.pdf}
}

@incollection{ljung_consistency_1976,
  title = {On {{The Consistency}} of {{Prediction Error Identification Methods}}},
  booktitle = {Mathematics in {{Science}} and {{Engineering}}},
  author = {Ljung, Lennart},
  editor = {Mehra, Raman K. and Lainiotis, Dimitri G.},
  year = {1976},
  month = jan,
  series = {System {{Identification Advances}} and {{Case Studies}}},
  volume = {126},
  pages = {121--164},
  publisher = {Elsevier},
  doi = {10.1016/S0076-5392(08)60871-1},
  abstract = {The problem of identification is to determine a model that describes input--output data obtained from a certain system. In this chapter, strong consistency for general prediction error methods, including the maximum-likelihood (ML) method is considered. The results are valid for general process models: linear and nonlinear. An error identification method is discussed in the chapter along with a general model for stochastic dynamic systems. Different identifiability concepts are also introduced, where a procedure to prove consistency is outlined. Consistency is shown for a general system structure, as well as for linear systems. The application of the results to linear time-invariant systems is also discussed in the chapter.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AVRTFFEK/ljung_on the_1976.pdf;/Users/antoniohortaribeiro/Zotero/storage/X7Z3SMA5/S0076539208608711.html}
}

@article{ljung_convergence_1978,
  title = {Convergence Analysis of Parametric Identification Methods},
  author = {Ljung, L.},
  year = {1978},
  month = oct,
  journal = {IEEE Transactions on Automatic Control},
  volume = {23},
  number = {5},
  pages = {770--783},
  issn = {0018-9286},
  doi = {10.1109/TAC.1978.1101840},
  abstract = {A certain class of methods to select suitable models of dynamical stochastic systems from measured input-output data is considered. The methods are based on a comparison between the measured outputs and the outputs of a candidate model. Depending on the set of models that is used, such methods are known under a variety of names, like output-error methods, equation-error methods, maximum-likelihood methods, etc. General results are proved concerning the models that are selected asymptotically as the number of observed data tends to infinity. For these results it is not assumed that the true system necessarily can be exactly represented within the chosen set of models. In the particular case when the model set contains the system, general consistency results are obtained and commented upon. Rather than to seek an exact description of the system, it is usually more realistic to be content with a suitable approximation of the true system with reasonable complexity properties. Here, the consequences of such a viewpoint are discussed.},
  keywords = {Convergence,Entropy,Equations,H infinity control,Helium,maximum likelihood estimation,Parameter identification,Predictive models,Random processes,stochastic systems,Testing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RF46C8TC/ljung_convergenc_1978.pdf;/Users/antoniohortaribeiro/Zotero/storage/ZDKXBU7Q/ljung1978.pdf;/Users/antoniohortaribeiro/Zotero/storage/F9UTAX8B/1101840.html}
}

@inproceedings{ljung_deep_2020,
  title = {Deep {{Learning}} and {{System Identification}}},
  booktitle = {Proceedings of the {{IFAC Congress}}, {{Berlin}}},
  author = {Ljung, Lennart and Andersson, Carl and Tiels, Koen and Schon, Thomas B},
  year = {2020},
  pages = {8},
  abstract = {Deep learning is a topic of considerable interest today. Since it deals with estimating -- or learning -- models, there are connections to the area of System Identification developed in the Automatic Control community. Such connections are explored and exploited in this contribution. It is stressed that common deep nets such as feedforward and cascadeforward nets are nonlinear ARX (NARX) models, and can thus be easily incorporated in System Identification code and practice. The case of LSTM nets is an example of NonLinear State-Space (NLSS) models. It performs worse than the cascadeforwardnet for a standard benchmark example.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/S9FK5F4U/Ljung et al. - Deep Learning and System Identiï¬cation.pdf}
}

@article{ljung_perspectives_2010,
  title = {Perspectives on System Identification},
  author = {Ljung, Lennart},
  year = {2010},
  journal = {Annual Reviews in Control},
  volume = {34},
  number = {1},
  pages = {1--12},
  doi = {10.1016/j.arcontrol.2009.12.001},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4XTSQ3R9/ljung_perspectiv_2010.pdf}
}

@article{ljung_perspectives_2010a,
  title = {Perspectives on System Identification},
  author = {Ljung, Lennart},
  year = {2010},
  month = apr,
  journal = {Annual Reviews in Control},
  volume = {34},
  number = {1},
  pages = {1--12},
  issn = {13675788},
  doi = {10.1016/j.arcontrol.2009.12.001},
  urldate = {2020-07-07},
  abstract = {System identification is the art and science of building mathematical models of dynamic systems from observed input--output data. It can be seen as the interface between the real world of applications and the mathematical world of control theory and model abstractions. As such, it is an ubiquitous necessity for successful applications. System identification is a very large topic, with different techniques that depend on the character of the models to be estimated: linear, nonlinear, hybrid, nonparametric, etc. At the same time, the area can be characterized by a small number of leading principles, e.g. to look for sustainable descriptions by proper decisions in the triangle of model complexity, information contents in the data, and effective validation. The area has many facets and there are many approaches and methods. A tutorial or a survey in a few pages is not quite possible. Instead, this presentation aims at giving an overview of the ``science'' side, i.e. basic principles and results and at pointing to open problem areas in the practical, ``art'', side of how to approach and solve a real problem.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/N9H2IB79/Ljung - 2010 - Perspectives on system identification.pdf}
}

@article{ljung_prediction_2002,
  title = {Prediction Error Estimation Methods},
  author = {Ljung, Lennart},
  year = {2002},
  journal = {Circuits, Systems and Signal Processing},
  volume = {21},
  number = {1},
  pages = {11--21},
  doi = {10.1007/BF01211648}
}

@book{ljung_system_1998,
  title = {System Identification},
  author = {Ljung, Lennart},
  year = {1998},
  publisher = {Springer},
  file = {/Users/antoniohortaribeiro/Zotero/storage/M5B84855/ljung_system_1998.pdf}
}

@inproceedings{long_fully_2015,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  pages = {3431--3440},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DDAN9ZXQ/Long et al. - Fully Convolutional Networks for Semantic Segmenta.pdf}
}

@inproceedings{loop_computing_1999,
  title = {Computing Rectifying Homographies for Stereo Vision},
  booktitle = {Computer {{Vision}} and {{Pattern Recognition}}, 1999. {{IEEE Computer Society Conference}} On.},
  author = {Loop, Charles and Zhang, Zhengyou},
  year = {1999},
  volume = {1},
  publisher = {IEEE}
}

@article{lorenz_deterministic_1963,
  title = {Deterministic Nonperiodic Flow},
  author = {Lorenz, Edward N},
  year = {1963},
  journal = {Journal of atmospheric sciences},
  volume = {20},
  number = {2},
  pages = {130--141},
  issn = {1520-0469}
}

@article{loubaton_almost_2016,
  title = {On the Almost Sure Location of the Singular Values of Certain {{Gaussian}} Block-{{Hankel}} Large Random Matrices},
  author = {Loubaton, Philippe},
  year = {2016},
  month = dec,
  journal = {Journal of Theoretical Probability},
  volume = {29},
  number = {4},
  eprint = {1405.2006},
  pages = {1339--1443},
  issn = {0894-9840, 1572-9230},
  doi = {10.1007/s10959-015-0614-z},
  urldate = {2020-11-23},
  abstract = {This paper studies the almost sure location of the eigenvalues of matrices \$\{{\textbackslash}bf W\}\_N \{{\textbackslash}bf W\}\_N{\textasciicircum}\{*\}\$ where \$\{{\textbackslash}bf W\}\_N = (\{{\textbackslash}bf W\}\_N{\textasciicircum}\{(1)T\}, ..., \{{\textbackslash}bf W\}\_N{\textasciicircum}\{(M)T\}){\textasciicircum}\{T\}\$ is a \$ML {\textbackslash}times N\$ block-line matrix whose block-lines \$(\{{\textbackslash}bf W\}\_N{\textasciicircum}\{(m)\})\_\{m=1, ..., M\}\$ are independent identically distributed \$L {\textbackslash}times N\$ Hankel matrices built from i.i.d. standard complex Gaussian sequences. It is shown that if \$M {\textbackslash}rightarrow +{\textbackslash}infty\$ and \${\textbackslash}frac\{ML\}\{N\} {\textbackslash}rightarrow c\_*\$ (\$c\_* {\textbackslash}in (0, {\textbackslash}infty)\$), then the empirical eigenvalue distribution of \$\{{\textbackslash}bf W\}\_N \{{\textbackslash}bf W\}\_N{\textasciicircum}\{*\}\$ converges almost surely towards the Marcenko-Pastur distribution. More importantly, it is established that if \$L = {\textbackslash}mathcal\{O\}(N{\textasciicircum}\{{\textbackslash}alpha\})\$ with \${\textbackslash}alpha {$<$} 2/3\$, then, almost surely, for \$N\$ large enough, the eigenvalues of \$\{{\textbackslash}bf W\}\_N \{{\textbackslash}bf W\}\_N{\textasciicircum}\{*\}\$ are located in the neighbourhood of the Marcenko-Pastur distribution.},
  archiveprefix = {arXiv},
  keywords = {60B20,Mathematics - Probability},
  file = {/Users/antoniohortaribeiro/Zotero/storage/LZKSC3VN/Loubaton - 2016 - On the almost sure location of the singular values.pdf;/Users/antoniohortaribeiro/Zotero/storage/5KG4U2T9/1405.html}
}

@inproceedings{loukas_how_2017,
  title = {How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Loukas, Andreas},
  editor = {Precup, Doina and Teh, Yee Whye},
  year = {2017-08-06/2017-08-11},
  series = {Proceedings of Machine Learning Research},
  volume = {70},
  pages = {2228--2237},
  publisher = {PMLR},
  abstract = {How many samples are sufficient to guarantee that the eigenvectors of the sample covariance matrix are close to those of the actual covariance matrix? For a wide family of distributions, including distributions with finite second moment and sub-gaussian distributions supported in a centered Euclidean ball, we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance and the number of samples. Our findings imply {\textexclamdown}em{\textquestiondown}non-asymptotic{\textexclamdown}/em{\textquestiondown} concentration bounds for eigenvectors and eigenvalues and carry strong consequences for the non-asymptotic analysis of PCA and its applications. For instance, they provide conditions for separating components estimated from O(1) samples and show that even few samples can be sufficient to perform dimensionality reduction, especially for low-rank covariances.},
  pdf = {http://proceedings.mlr.press/v70/loukas17a/loukas17a.pdf},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BQRY3GES/Loukas - 2017 - How close are the eigenvectors of the sample and a.pdf}
}

@book{love_linux_2010,
  title = {Linux {{Kernel Development}}},
  author = {Love, R.},
  year = {2010},
  series = {Developer's {{Library}}},
  publisher = {Pearson Education},
  isbn = {978-0-7686-9679-0},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DZX5JJXE/love_linux_2010.pdf}
}

@article{lu_attractor_2018,
  title = {Attractor Reconstruction by Machine Learning},
  author = {Lu, Zhixin and Hunt, Brian R. and Ott, Edward},
  year = {2018},
  month = jun,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {28},
  number = {6},
  pages = {061104},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5039508},
  urldate = {2021-02-19},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HUEMIV7F/Lu et al. - 2018 - Attractor reconstruction by machine learning.pdf}
}

@article{lu_decoding_2024,
  title = {Decoding 2.3 {{Million ECGs}}: {{Interpretable Deep Learning}} for {{Advancing Cardiovascular Diagnosis}} and {{Mortality Risk Stratification}}},
  shorttitle = {Decoding 2.3 {{Million ECGs}}},
  author = {Lu, Lei and Zhu, Tingting and Ribeiro, Antonio H and Clifton, Lei and Zhao, Erying and Zhou, Jiandong and Ribeiro, Antonio Luiz P and Zhang, Yuan-Ting and Clifton, David A},
  year = {2024},
  month = feb,
  journal = {European Heart Journal - Digital Health},
  pages = {ztae014},
  issn = {2634-3916},
  doi = {10.1093/ehjdh/ztae014},
  urldate = {2024-02-26},
  abstract = {Electrocardiogram (ECG) is widely considered the primary test for evaluating cardiovascular diseases. However, the use of artificial intelligence to advance these medical practices and learn new clinical insights from ECGs remains largely unexplored. Utilising a dataset of 2,322,513 ECGs collected from 1,558,772 patients with 7 years of follow-up, we developed a deep learning model with state-of-the-art granularity for the interpretable diagnosis of cardiac abnormalities, gender identification, and hyper- tension screening solely from ECGs, which are then used to stratify the risk of mortality. The model achieved the area under the receiver operating characteristic curve (AUC) scores of 0.998 (95\% confidence interval (CI), 0.995-0.999), 0.964 (0.963-0.965), and 0.839 (0.837-0.841) for the three diagnostic tasks separately. Using ECG-predicted results, we find high risks of mortality for subjects with sinus tachycardia (adjusted hazard ratio (HR) of 2.24, 1.96-2.57), and atrial fibrillation (adjusted HR of 2.22, 1.99-2.48). We further use salient morphologies produced by the deep learning model to identify key ECG leads that achieved similar performance for the three diagnoses, and we find that the V1 ECG lead is important for hypertension screening and mortality risk stratification of hypertensive cohorts, with an AUC of 0.816 (0.814-0.818) and a univariate HR of 1.70 (1.61-1.79) for the two tasks separately. Using ECGs alone, our developed model showed cardiologist-level accuracy in interpretable cardiac diagnosis, and the advancement in mortality risk stratification; In addition, the potential to facilitate clinical knowledge discovery for gender and hypertension detection which are not readily available.},
  copyright = {All rights reserved},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GKZNL3A5/Lu et al. - 2024 - Decoding 2.3 Million ECGs Interpretable Deep Lear.pdf}
}

@article{lu_knowledge_2022,
  title = {Knowledge {{Discovery}} with {{Electrocardiography Using Interpretable Deep Neural Networks}}},
  author = {Lu, Lei and Zhu, Tingting and Ribeiro, Ant{\^o}nio H. and Clifton, Lei and Zhao, Erying and Ribeiro, Antonio Luiz P. and Zhang, Yuan-Ting and Clifton, David A.},
  year = {2022},
  month = nov,
  journal = {Under review at Nature Comunications (preprint: medRxiv)},
  doi = {10.1101/2022.11.01.22281722},
  urldate = {2022-11-17},
  abstract = {Despite the potentials of artificial intelligence (AI) in healthcare, very little work focuses on the extraction of clinical information or knowledge discovery from clinical measurements. Here we propose a novel deep learning model to extract characteristics in electrocardiogram (ECG) and explore its usage in knowledge discovery. Utilising a 12-lead ECG dataset (nECGs = 2,322,513) collected from unique subjects (nSubjects = 1,558,772) in primary care, we performed three independent medical tasks with the proposed model: (i) cardiac abnormality diagnosis, (ii) gender identification, and (iii) hypertension screening. We achieved an area under the curve (AUC) score of 0.998 (95\% confidence interval (CI), 0.995-0.999), 0.964 (95\% CI, 0.963-0.965), and 0.839 (95\% CI, 0.837-0.841) for each task, respectively; We provide interpretation of salient morphologies and further identified key ECG leads that achieve similar performance for the three tasks: (i) AVR and V1 leads (AUC=0.990 (95\% CI, 0.982-0.995); (ii) V5 lead (AUC=0.900 (95\% CI, 0.899-0.902)); and (iii) V1 lead (AUC=0.816 (95\% CI, 0.814-0.818)). Using ECGs, our model not only has demonstrated cardiologist-level accuracy in heart diagnosis with interpretability, but also shows its potentials in facilitating clinical knowledge discovery for gender and hypertension detection which are not readily available.},
  copyright = {{\copyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XEGE9H3S/Lu et al. - 2022 - Knowledge Discovery with Electrocardiography Using.pdf;/Users/antoniohortaribeiro/Zotero/storage/PL5X4PR2/2022.11.01.html}
}

@article{lu_multiscale_2017,
  title = {Multiscale {{Support Vector Learning With Projection Operator Wavelet Kernel}} for {{Nonlinear Dynamical System Identification}}},
  author = {Lu, Z. and Sun, J. and Butts, K.},
  year = {2017},
  month = jan,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {28},
  number = {1},
  pages = {231--243},
  issn = {2162-237X},
  doi = {10.1109/TNNLS.2015.2513902},
  abstract = {A giant leap has been made in the past couple of decades with the introduction of kernel-based learning as a mainstay for designing effective nonlinear computational learning algorithms. In view of the geometric interpretation of conditional expectation and the ubiquity of multiscale characteristics in highly complex nonlinear dynamic systems [1]-[3], this paper presents a new orthogonal projection operator wavelet kernel, aiming at developing an efficient computational learning approach for nonlinear dynamical system identification. In the framework of multiresolution analysis, the proposed projection operator wavelet kernel can fulfill the multiscale, multidimensional learning to estimate complex dependencies. The special advantage of the projection operator wavelet kernel developed in this paper lies in the fact that it has a closed-form expression, which greatly facilitates its application in kernel learning. To the best of our knowledge, it is the first closed-form orthogonal projection wavelet kernel reported in the literature. It provides a link between grid-based wavelets and mesh-free kernel-based methods. Simulation studies for identifying the parallel models of two benchmark nonlinear dynamical systems confirm its superiority in model accuracy and sparsity.},
  keywords = {closed-form expression,closed-form orthogonal projection wavelet kernel,complex dependency estimation,Composite kernel,Computational modeling,conditional expectation geometric interpretation,grid-based wavelets,identification,Kernel,kernel-based learning,learning (artificial intelligence),Learning systems,linear programming support vector regression (LP-SVR),mesh-free kernel-based methods,multidimensional learning,multiresolution analysis,multiscale characteristics ubiquity,multiscale modeling,multiscale support vector learning,nonlinear computational learning algorithms,nonlinear dynamical system identification,nonlinear dynamical systems,nonlinear systems identification,orthogonal projection operator,orthogonal projection operator wavelet kernel,raised-cosine wavelet,support vector machines,wavelet transforms},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KB8SH8PS/lu_multiscale_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/JNUX39ZW/7398084.html;/Users/antoniohortaribeiro/Zotero/storage/MLQXWJGC/7398084.html}
}

@article{lu_reservoir_2017,
  title = {Reservoir Observers: {{Model-free}} Inference of Unmeasured Variables in Chaotic Systems},
  shorttitle = {Reservoir Observers},
  author = {Lu, Zhixin and Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Brockett, Roger and Ott, Edward},
  year = {2017},
  month = apr,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {27},
  number = {4},
  pages = {041102},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4979665},
  urldate = {2021-04-01},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QS8I4HEJ/Lu et al. - 2017 - Reservoir observers Model-free inference of unmea.pdf}
}

@book{luenberger_linear_2008,
  title = {Linear and Nonlinear Programming},
  author = {Luenberger, David G. and Ye, Yinyu},
  year = {2008},
  series = {International Series in Operations Research and Management Science},
  edition = {3rd ed},
  publisher = {Springer},
  address = {New York, NY},
  isbn = {978-0-387-74502-2},
  lccn = {T57.7 .L8 2008},
  keywords = {Linear programming,Nonlinear programming},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PQE5KR5P/luenberger_linear_2008.pdf}
}

@incollection{lukosevicius_practical_2012,
  title = {A {{Practical Guide}} to {{Applying Echo State Networks}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {Luko{\v s}evi{\v c}ius, Mantas},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  volume = {7700},
  pages = {659--686},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-35289-8_36},
  urldate = {2021-03-30},
  abstract = {Reservoir computing has emerged in the last decade as an alternative to gradient descent methods for training recurrent neural networks. Echo State Network (ESN) is one of the key reservoir computing ``flavors''. While being practical, conceptually simple, and easy to implement, ESNs require some experience and insight to achieve the hailed good performance in many tasks. Here we present practical techniques and recommendations for successfully applying ESNs, as well as some more advanced application-specific modifications.},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TPC3FRYY/LukoÅ¡eviÄius - 2012 - A Practical Guide to Applying Echo State Networks.pdf}
}

@article{luo_companion_2014,
  title = {Companion {{Matrices}} and {{Their Relations}} to {{Toeplitz}} and {{Hankel Matrices}}},
  author = {Luo, Yousong and Hill, Robin},
  year = {2014},
  month = nov,
  journal = {arXiv:1411.4592 [math]},
  eprint = {1411.4592},
  primaryclass = {math},
  urldate = {2020-12-29},
  abstract = {In this paper we describe some properties of companion matrices and demonstrate some special patterns that arise when a Toeplitz or a Hankel matrix is multiplied by a related companion matrix. We present a new condition, generalizing known results, for a Toeplitz or a Hankel matrix to be the transforming matrix for a similarity between a pair of companion matrices. A special case of our main result shows that a Toeplitz or a Hankel matrix can be extended using associated companion matrices, preserving the Toeplitz or Hankel structure respectively.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - General Mathematics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/E7DJQ97G/Luo and Hill - 2014 - Companion Matrices and Their Relations to Toeplitz.pdf;/Users/antoniohortaribeiro/Zotero/storage/Y2Z5CJY8/1411.html}
}

@article{luo_review_2010,
  title = {A Review of Electrocardiogram Filtering},
  author = {Luo, Shen and Johnston, Paul},
  year = {2010},
  month = nov,
  journal = {Journal of Electrocardiology},
  volume = {43},
  number = {6},
  pages = {486--496},
  issn = {0022-0736},
  doi = {10/fp6hfc},
  abstract = {Analog filtering and digital signal processing algorithms in the preprocessing modules of an electrocardiographic device play a pivotal role in providing high-quality electrocardiogram (ECG) signals for analysis, interpretation, and presentation (display, printout, and storage). In this article, issues relating to inaccuracy of ECG preprocessing filters are investigated in the context of facilitating efficient ECG interpretation and diagnosis. The discussion covers 4 specific ECG preprocessing applications: anti-aliasing and upper-frequency cutoff, baseline wander suppression and lower-frequency cutoff, line frequency rejection, and muscle artifact reduction. Issues discussed include linear phase, aliasing, distortion, ringing, and attenuation of desired ECG signals. Due to the overlapping power spectrum of signal and noise in acquired ECG data, frequency selective filters must seek a delicate balance between noise removal and deformation of the desired signal. Most importantly, the filtering output should not adversely impact subsequent diagnosis and interpretation. Based on these discussions, several suggestions are made to improve and update existing ECG data preprocessing standards and guidelines.},
  keywords = {Aliasing,ECG filter,ECG standards,Frequency domain,Linear phase,Magnitude distortion,Ringing artifact}
}

@misc{luo_understanding_2022,
  title = {Understanding {{Diffusion Models}}: {{A Unified Perspective}}},
  shorttitle = {Understanding {{Diffusion Models}}},
  author = {Luo, Calvin},
  year = {2022},
  month = aug,
  number = {arXiv:2208.11970},
  eprint = {2208.11970},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.11970},
  urldate = {2023-07-27},
  abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VT73AZ5U/Luo_2022_Understanding Diffusion Models.pdf;/Users/antoniohortaribeiro/Zotero/storage/DDUWJZYY/2208.html}
}

@inproceedings{luong_effective_2015,
  title = {Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
  year = {2015},
  month = sep,
  pages = {1412--1421},
  publisher = {Association for Computational Linguistics},
  address = {Lisbon, Portugal},
  doi = {10/gdpd6w},
  urldate = {2019-05-29},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HWXEEZJI/luong_effective_2015.pdf}
}

@article{lyon_computational_2018,
  title = {Computational Techniques for {{ECG}} Analysis and Interpretation in Light of Their Contribution to Medical Advances},
  author = {Lyon, Aurore and Minchol{\'e}, Ana and Mart{\'i}nez, Juan Pablo and Laguna, Pablo and Rodriguez, Blanca},
  year = {2018},
  month = jan,
  journal = {Journal of the Royal Society Interface},
  volume = {15},
  number = {138},
  issn = {1742-5689},
  doi = {10.1098/rsif.2017.0821},
  urldate = {2018-10-22},
  abstract = {Widely developed for clinical screening, electrocardiogram (ECG) recordings capture the cardiac electrical activity from the body surface. ECG analysis can therefore be a crucial first step to help diagnose, understand and predict cardiovascular disorders responsible for 30\% of deaths worldwide. Computational techniques, and more specifically machine learning techniques and computational modelling are powerful tools for classification, clustering and simulation, and they have recently been applied to address the analysis of medical data, especially ECG data. This review describes the computational methods in use for ECG analysis, with a focus on machine learning and 3D computer simulations, as well as their accuracy, clinical implications and contributions to medical advances. The first section focuses on heartbeat classification and the techniques developed to extract and classify abnormal from regular beats. The second section focuses on patient diagnosis from whole recordings, applied to different diseases. The third section presents real-time diagnosis and applications to wearable devices. The fourth section highlights the recent field of personalized ECG computer simulations and their interpretation. Finally, the discussion section outlines the challenges of ECG analysis and provides a critical assessment of the methods presented. The computational methods reported in this review are a strong asset for medical discoveries and their translation to the clinical world may lead to promising advances.},
  pmcid = {PMC5805987},
  pmid = {29321268},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PPKYSDA5/lyon_computatio_2018.pdf}
}

@article{ma_complete_2015,
  title = {A {{Complete Recipe}} for {{Stochastic Gradient MCMC}}},
  author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily B.},
  year = {2015},
  month = jun,
  journal = {arXiv:1506.04696 [math, stat]},
  eprint = {1506.04696},
  primaryclass = {math, stat},
  urldate = {2019-01-30},
  abstract = {Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing MCMC samplers--including stochastic gradient versions--based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WB5NPHUL/ma_a_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/ZCIXIQJY/1506.html}
}

@inproceedings{maas_rectifier_2013,
  title = {Rectifier Nonlinearities Improve Neural Network Acoustic Models},
  booktitle = {In {{ICML Workshop}} on {{Deep Learning}} for {{Audio}}, {{Speech}} and {{Language Processing}}},
  author = {Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew Y.},
  year = {2013},
  keywords = {â›” No DOI found}
}

@article{maass_realtime_2002,
  title = {Real-{{Time Computing Without Stable States}}: {{A New Framework}} for {{Neural Computation Based}} on {{Perturbations}}},
  shorttitle = {Real-{{Time Computing Without Stable States}}},
  author = {Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  year = {2002},
  month = nov,
  journal = {Neural Computation},
  volume = {14},
  number = {11},
  pages = {2531--2560},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976602760407955},
  urldate = {2019-10-28},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BGYXKDAS/Maass et al. - 2002 - Real-Time Computing Without Stable States A New F.pdf}
}

@book{macedo_eletromagnetismo_1988,
  title = {Eletromagnetismo},
  author = {Macedo, Annita},
  year = {1988},
  publisher = {GUANABARA},
  isbn = {978-85-277-0100-6},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5KP3SPUK/macedo_eletromagn_1988.pdf;/Users/antoniohortaribeiro/Zotero/storage/75NSIKWD/macedo_eletromagn_1988.pdf;/Users/antoniohortaribeiro/Zotero/storage/A3NR247U/macedo_eletromagn_1988.pdf;/Users/antoniohortaribeiro/Zotero/storage/CEDC6GSK/macedo_eletromagn_1988.pdf;/Users/antoniohortaribeiro/Zotero/storage/WIKMQXV4/macedo_eletromagn_1988.pdf}
}

@article{macfarlane_automated_1996,
  title = {Automated Serial {{ECG}} Comparison Based on the {{Minnesota}} Code},
  author = {Macfarlane, Peter W and Latif, Shahid},
  year = {1996},
  journal = {Journal of Electrocardiology},
  volume = {29},
  pages = {29--34},
  issn = {0022-0736},
  doi = {10/cdh7qw}
}

@article{macfarlane_methodology_1990,
  title = {Methodology of {{ECG}} Interpretation in the {{Glasgow}} Program},
  author = {Macfarlane, {\relax PW} and Devine, B and Latif, S and McLaughlin, S and Shoat, {\relax DB} and Watts, {\relax MP}},
  year = {1990},
  journal = {Methods of information in medicine},
  volume = {29},
  number = {04},
  pages = {354--361},
  issn = {0026-1270},
  doi = {10/gftz83}
}

@inproceedings{macfarlane_university_2005,
  title = {The University of Glasgow ({{Uni-G}}) {{ECG}} Analysis Program},
  booktitle = {Computers in {{Cardiology}}},
  author = {Macfarlane, P. W. and Devine, B. and Clark, E.},
  year = {2005},
  pages = {451--454},
  doi = {10.1109/CIC.2005.1588134},
  isbn = {0276-6574},
  keywords = {Cardiology,Computer aided manufacturing,Databases,descriptor Uni-G ECG analysis program,diagnostic software,electrocardiography,Electrocardiography,Instruments,medical signal processing,neonates,nomenclature,Pacemakers,Pediatrics,racial variation,Signal analysis,signal processing,Signal processing,terminology,Terminology}
}

@book{mackay_information_2003,
  title = {Information Theory, Inference and Learning Algorithms},
  author = {MacKay, David JC and Mac Kay, David JC},
  year = {2003},
  publisher = {Cambridge university press},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DRKBBA29/mackay_informatio_2003.pdf}
}

@article{maddox_simple_2019,
  title = {A {{Simple Baseline}} for {{Bayesian Uncertainty}} in {{Deep Learning}}},
  author = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  month = feb,
  urldate = {2019-02-14},
  abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of computer vision tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, and temperature scaling.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/LINSUEU3/maddox_a simple_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/ND5234C9/1902.html}
}

@article{madry_deep_2018,
  title = {Towards {{Deep Learning Models Resistant}} to {{Adversarial Attacks}}},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  year = {2018},
  journal = {International Conference for Learning Representations (ICLR)},
  abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NL2ISC8W/Madry et al. - 2019 - Towards Deep Learning Models Resistant to Adversar.pdf}
}

@inproceedings{maduranga_complex_2019,
  title = {Complex Unitary Recurrent Neural Networks Using Scaled Cayley Transform},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Maduranga, Kehelwala DG and Helfrich, Kyle E and Ye, Qiang},
  year = {2019},
  volume = {33},
  pages = {4528--4535},
  isbn = {2374-3468},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UTGMYZR2/maduranga_complex_2019.pdf}
}

@incollection{maheswaranathan_reverse_2019,
  title = {Reverse Engineering Recurrent Networks for Sentiment Classification Reveals Line Attractor Dynamics},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew and Ganguli, Surya and Sussillo, David},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {15696--15705},
  publisher = {Curran Associates, Inc.}
}

@incollection{maheswaranathan_reverse_2019a,
  title = {Reverse Engineering Recurrent Networks for Sentiment Classification Reveals Line Attractor Dynamics},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew and Ganguli, Surya and Sussillo, David},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {15696--15705},
  publisher = {Curran Associates, Inc.}
}

@article{maheswaranathan_reverse_2019b,
  title = {Reverse Engineering Recurrent Networks for Sentiment Classification Reveals Line Attractor Dynamics},
  author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew D. and Ganguli, Surya and Sussillo, David},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.10720 [cs, stat]},
  eprint = {1906.10720},
  primaryclass = {cs, stat},
  urldate = {2019-06-28},
  abstract = {Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8BY5CB4K/maheswaranathan_reverse_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/NTBG62LD/1906.html}
}

@article{mahloujifar_curse_2019,
  title = {The {{Curse}} of {{Concentration}} in {{Robust Learning}}: {{Evasion}} and {{Poisoning Attacks}} from {{Concentration}} of {{Measure}}},
  shorttitle = {The {{Curse}} of {{Concentration}} in {{Robust Learning}}},
  author = {Mahloujifar, Saeed and Diochnos, Dimitrios I. and Mahmoody, Mohammad},
  year = {2019},
  journal = {AAAI},
  eprint = {1809.03063},
  urldate = {2021-08-10},
  abstract = {Many modern machine learning classifiers are shown to be vulnerable to adversarial perturbations of the instances. Despite a massive amount of work focusing on making classifiers robust, the task seems quite challenging. In this work, through a theoretical study, we investigate the adversarial risk and robustness of classifiers and draw a connection to the well-known phenomenon of concentration of measure in metric measure spaces. We show that if the metric probability space of the test instance is concentrated, any classifier with some initial constant error is inherently vulnerable to adversarial perturbations. One class of concentrated metric probability spaces are the so-called Levy families that include many natural distributions. In this special case, our attacks only need to perturb the test instance by at most \$O({\textbackslash}sqrt n)\$ to make it misclassified, where \$n\$ is the data dimension. Using our general result about Levy instance spaces, we first recover as special case some of the previously proved results about the existence of adversarial examples. However, many more Levy families are known (e.g., product distribution under the Hamming distance) for which we immediately obtain new attacks that find adversarial examples of distance \$O({\textbackslash}sqrt n)\$. Finally, we show that concentration of measure for product spaces implies the existence of forms of "poisoning" attacks in which the adversary tampers with the training data with the goal of degrading the classifier. In particular, we show that for any learning algorithm that uses \$m\$ training examples, there is an adversary who can increase the probability of any "bad property" (e.g., failing on a particular test instance) that initially happens with non-negligible probability to \${\textbackslash}approx 1\$ by substituting only \${\textbackslash}tilde\{O\}({\textbackslash}sqrt m)\$ of the examples with other (still correctly labeled) examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7YM4ST3M/Mahloujifar et al. - 2018 - The Curse of Concentration in Robust Learning Eva.pdf;/Users/antoniohortaribeiro/Zotero/storage/RF7BIIA7/1809.html}
}

@inproceedings{mahloujifar_empirically_2019,
  title = {Empirically {{Measuring Concentration}}: {{Fundamental Limits}} on {{Intrinsic Robustness}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Mahloujifar, Saeed and Zhang, Xiao and Mahmoody, Mohammad and Evans, David},
  year = {2019},
  abstract = {Many recent works have shown that adversarial examples that fool classifiers can be found by minimally perturbing a normal input. Recent theoretical results, starting with Gilmer et al. (2018b), show that if the inputs are drawn from a concentrated metric probability space, then adversarial examples with small perturbation are inevitable. A concentrated space has the property that any subset with {\textohm}(1) (e.g., 1/100) measure, according to the imposed distribution, has small distance to almost all (e.g., 99/100) of the points in the space. It is not clear, however, whether these theoretical results apply to actual distributions such as images. This paper presents a method for empirically measuring and bounding the concentration of a concrete dataset which is proven to converge to the actual concentration. We use it to empirically estimate the intrinsic robustness to {$\infty$} and 2 perturbations of several image classification benchmarks. Code for our experiments is available at https://github.com/xiaozhanguva/Measure-Concentration.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DWUQJ4Y7/Mahloujifar et al. - Empirically Measuring Concentration Fundamental L.pdf}
}

@article{mak_polygenic_2017,
  title = {Polygenic Scores via Penalized Regression on Summary Statistics},
  author = {Mak, Timothy Shin Heng and Porsch, Robert Milan and Choi, Shing Wan and Zhou, Xueya and Sham, Pak Chung},
  year = {2017},
  month = sep,
  journal = {Genetic Epidemiology},
  volume = {41},
  number = {6},
  pages = {469--480},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {0741-0395},
  doi = {10.1002/gepi.22050},
  urldate = {2024-01-24},
  abstract = {ABSTRACT Polygenic scores (PGS) summarize the genetic contribution of a person's genotype to a disease or phenotype. They can be used to group participants into different risk categories for diseases, and are also used as covariates in epidemiological analyses. A number of possible ways of calculating PGS have been proposed, and recently there is much interest in methods that incorporate information available in published summary statistics. As there is no inherent information on linkage disequilibrium (LD) in summary statistics, a pertinent question is how we can use LD information available elsewhere to supplement such analyses. To answer this question, we propose a method for constructing PGS using summary statistics and a reference panel in a penalized regression framework, which we call lassosum. We also propose a general method for choosing the value of the tuning parameter in the absence of validation data. In our simulations, we showed that pseudovalidation often resulted in prediction accuracy that is comparable to using a dataset with validation phenotype and was clearly superior to the conservative option of setting the tuning parameter of lassosum to its lowest value. We also showed that lassosum achieved better prediction accuracy than simple clumping and P-value thresholding in almost all scenarios. It was also substantially faster and more accurate than the recently proposed LDpred.},
  keywords = {elastic net,LASSO,linkage disequilibrium,polygenic score,summary statistics}
}

@article{mallon_projective_2005,
  title = {Projective Rectification from the Fundamental Matrix},
  author = {Mallon, John and Whelan, Paul F},
  year = {2005},
  journal = {Image and Vision Computing},
  volume = {23},
  number = {7},
  pages = {643--650},
  doi = {10.1016/j.imavis.2005.03.002}
}

@article{man_vectorcardiographic_2015,
  title = {Vectorcardiographic Diagnostic \& Prognostic Information Derived from the 12-lead Electrocardiogram: {{Historical}} Review and Clinical Perspective},
  shorttitle = {Vectorcardiographic Diagnostic \& Prognostic Information Derived from the 12-lead Electrocardiogram},
  author = {Man, Sumche and Maan, Arie C. and Schalij, Martin J. and Swenne, Cees A.},
  year = {2015},
  month = jul,
  journal = {Journal of Electrocardiology},
  volume = {48},
  number = {4},
  pages = {463--475},
  issn = {00220736},
  doi = {10.1016/j.jelectrocard.2015.05.002},
  urldate = {2018-07-17},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MWEKXQNR/man_vectorcard_2015.pdf}
}

@inproceedings{manchester_input_2010,
  title = {Input Design for System Identification via Convex Relaxation},
  booktitle = {49th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Manchester, I. R.},
  year = {2010},
  month = dec,
  pages = {2041--2046},
  doi = {10.1109/CDC.2010.5717097},
  abstract = {We consider the problem of designing an excitation input for a system idenfication experiment. The optimization problem considered is to maximize a reduced Fisher information matrix in any of the classical D-, E-, or A-optimal senses. In contrast to the majority of published work on this topic, we consider the problem in the time domain and subject to constraints on the amplitude of the input signal. This optimization problem is nonconvex. The main result of the paper is a convex relaxation that gives an upper bound accurate to within 2/{$\pi$} of the true maximum. A randomized algorithm is presented for finding a feasible solution which, in a certain sense is expected to be at least 2/{$\pi$} as informative as the globally optimal input signal. In the case of a single constraint on input power, the proposed approach recovers the true global optimum exactly. Extensions to situations with both power and amplitude constraints on both inputs and outputs are given. A simple simulation example illustrates the technique.},
  keywords = {Binary sequences,Computational modeling,convex relaxation,excitation input,Fisher information matrix,Frequency domain analysis,globally optimal input signal,identification,input design,matrix algebra,optimisation,Optimization,optimization problem,randomised algorithms,randomized algorithm,system idenfication,system identification,time domain,Time domain analysis,Tin,upper bound},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7QDRGVRI/manchester_input_2010.pdf;/Users/antoniohortaribeiro/Zotero/storage/ZIAR3AP8/5717097.html}
}

@book{mancini_op_2003,
  title = {Op Amps for Everyone: Design Reference},
  shorttitle = {Op Amps for Everyone},
  author = {Mancini, Ron},
  year = {2003},
  publisher = {Newnes},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PEWUHMM4/mancini_op amps_2003.pdf}
}

@book{mandic_recurrent_2001,
  title = {Recurrent Neural Networks for Prediction: Learning Algorithms, Architectures, and Stability},
  shorttitle = {Recurrent Neural Networks for Prediction},
  author = {Mandic, Danilo P. and Chambers, Jonathon A.},
  year = {2001},
  series = {Wiley Series in Adaptive and Learning Systems for Signal Processing, Communications, and Control},
  publisher = {John Wiley},
  address = {Chichester ; New York},
  isbn = {978-0-471-49517-8},
  lccn = {Q325.5 .M36 2001},
  keywords = {machine learning,Neural networks (Computer science)},
  file = {/Users/antoniohortaribeiro/Zotero/storage/N9UB3XBW/mandic_recurrent_2001.pdf}
}

@misc{manski_patientcentered_2022,
  title = {Patient-{{Centered Appraisal}} of {{Race-Free Clinical Risk Assessment}}},
  author = {Manski, Charles F.},
  year = {2022},
  month = feb,
  number = {arXiv:2112.01639},
  eprint = {2112.01639},
  primaryclass = {econ},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.01639},
  urldate = {2023-09-04},
  abstract = {Until recently, there has been a consensus that clinicians should condition patient risk assessments on all observed patient covariates with predictive power. The broad idea is that knowing more about patients enables more accurate predictions of their health risks and, hence, better clinical decisions. This consensus has recently unraveled with respect to a specific covariate, namely race. There have been increasing calls for race-free risk assessment, arguing that using race to predict patient outcomes contributes to racial disparities and inequities in health care. Writers calling for race-free risk assessment have not studied how it would affect the quality of clinical decisions. Considering the matter from the patient-centered perspective of medical economics yields a disturbing conclusion: Race-free risk assessment would harm patients of all races.},
  archiveprefix = {arXiv},
  keywords = {\_tablet\_modified,Economics - Econometrics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YE2WJNBG/Manski_2022_Patient-Centered Appraisal of Race-Free Clinical Risk Assessment.pdf;/Users/antoniohortaribeiro/Zotero/storage/RYYTGG4Y/2112.html}
}

@article{mant_accuracy_2007,
  title = {Accuracy of Diagnosing Atrial Fibrillation on Electrocardiogram by Primary Care Practitioners and Interpretative Diagnostic Software: Analysis of Data from Screening for Atrial Fibrillation in the Elderly ({{SAFE}}) Trial},
  shorttitle = {Accuracy of Diagnosing Atrial Fibrillation on Electrocardiogram by Primary Care Practitioners and Interpretative Diagnostic Software},
  author = {Mant, Jonathan and Fitzmaurice, David A. and Hobbs, F. D. Richard and Jowett, Sue and Murray, Ellen T. and Holder, Roger and Davies, Michael and Lip, Gregory Y. H.},
  year = {2007},
  month = aug,
  journal = {BMJ (Clinical research ed.)},
  volume = {335},
  number = {7616},
  pages = {380},
  issn = {1756-1833},
  doi = {10.1136/bmj.39227.551713.AE},
  abstract = {OBJECTIVE: To assess the accuracy of general practitioners, practice nurses, and interpretative software in the use of different types of electrocardiogram to diagnose atrial fibrillation. DESIGN: Prospective comparison with reference standard of assessment of electrocardiograms by two independent specialists. SETTING: 49 general practices in central England. PARTICIPANTS: 2595 patients aged 65 or over screened for atrial fibrillation as part of the screening for atrial fibrillation in the elderly (SAFE) study; 49 general practitioners and 49 practice nurses. INTERVENTIONS: All electrocardiograms were read with the Biolog interpretative software, and a random sample of 12 lead, limb lead, and single lead thoracic placement electrocardiograms were assessed by general practitioners and practice nurses independently of each other and of the Biolog assessment. MAIN OUTCOME MEASURES: Sensitivity, specificity, and positive and negative predictive values. RESULTS: General practitioners detected 79 out of 99 cases of atrial fibrillation on a 12 lead electrocardiogram (sensitivity 80\%, 95\% confidence interval 71\% to 87\%) and misinterpreted 114 out of 1355 cases of sinus rhythm as atrial fibrillation (specificity 92\%, 90\% to 93\%). Practice nurses detected a similar proportion of cases of atrial fibrillation (sensitivity 77\%, 67\% to 85\%), but had a lower specificity (85\%, 83\% to 87\%). The interpretative software was significantly more accurate, with a specificity of 99\%, but missed 36 of 215 cases of atrial fibrillation (sensitivity 83\%). Combining general practitioners' interpretation with the interpretative software led to a sensitivity of 92\% and a specificity of 91\%. Use of limb lead or single lead thoracic placement electrocardiograms resulted in some loss of specificity. CONCLUSIONS: Many primary care professionals cannot accurately detect atrial fibrillation on an electrocardiogram, and interpretative software is not sufficiently accurate to circumvent this problem, even when combined with interpretation by a general practitioner. Diagnosis of atrial fibrillation in the community needs to factor in the reading of electrocardiograms by appropriately trained people.},
  langid = {english},
  pmcid = {PMC1952490},
  pmid = {17604299},
  keywords = {{Diagnosis, Computer-Assisted},Aged,Atrial Fibrillation,Clinical Competence,Electrocardiography,England,Family Practice,Humans,Nurse Practitioners,Predictive Value of Tests,Prospective Studies},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DWA7DVPX/mant_accuracy_2007.pdf}
}

@inproceedings{marcotte_abide_2023,
  title = {Abide by the Law and Follow the Flow: Conservation Laws for Gradient Flows},
  shorttitle = {Abide by the Law and Follow the Flow},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Marcotte, Sibylle and Gribonval, R{\'e}mi and Peyr{\'e}, Gabriel},
  year = {2023},
  month = nov,
  urldate = {2023-12-12},
  abstract = {Understanding the geometric properties of gradient descent dynamics is a key ingredient in deciphering the recent success of very large machine learning models. A striking observation is that trained over-parameterized models retain some properties of the optimization initialization. This "implicit bias" is believed to be responsible for some favorable properties of the trained models and could explain their good generalization properties. The purpose of this article is threefold. First, we rigorously expose the definition and basic properties of "conservation laws", that define quantities conserved during gradient flows of a given model (e.g. of a ReLU network with a given architecture) with any training data and any loss. Then we explain how to find the maximal number of independent conservation laws by performing finite-dimensional algebraic manipulations on the Lie algebra generated by the Jacobian of the model. Finally, we provide algorithms to: a) compute a family of polynomial laws; b) compute the maximal number of (not necessarily polynomial) independent conservation laws. We provide showcase examples that we fully work out theoretically. Besides, applying the two algorithms confirms for a number of ReLU network architectures that all known laws are recovered by the algorithm, and that there are no other independent laws. Such computational tools pave the way to understanding desirable properties of optimization initialization in large machine learning models.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6V3VGCRI/Marcotte et al_2023_Abide by the law and follow the flow.pdf}
}

@article{marcus_building_1993,
  title = {Building a Large Annotated Corpus of {{English}}: {{The Penn Treebank}}},
  author = {Marcus, Mitchell P. and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  year = {1993},
  journal = {Computational Linguistics},
  volume = {19},
  number = {2},
  pages = {313--330}
}

@article{marin-neto_pathogenesis_2007,
  title = {Pathogenesis of {{Chronic Chagas Heart Disease}}},
  author = {{Marin-Neto}, Jose Antonio and {Cunha-Neto}, Ed{\'e}cio and Maciel, Benedito C. and Sim{\~o}es, Marcus V.},
  year = {2007},
  month = mar,
  journal = {Circulation},
  volume = {115},
  number = {9},
  pages = {1109--1123},
  publisher = {American Heart Association},
  doi = {10.1161/CIRCULATIONAHA.106.624296},
  urldate = {2021-11-25},
  abstract = {Background--- Chagas disease remains a significant public health issue and a major cause of morbidity and mortality in Latin America. Despite nearly 1 century of research, the pathogenesis of chronic Chagas cardiomyopathy is incompletely understood, the most intriguing challenge of which is the complex host-parasite interaction. Methods and Results--- A systematic review of the literature found in MEDLINE, EMBASE, BIREME, LILACS, and SCIELO was performed to search for relevant references on pathogenesis and pathophysiology of Chagas disease. Evidence from studies in animal models and in anima nobile points to 4 main pathogenetic mechanisms to explain the development of chronic Chagas heart disease: autonomic nervous system derangements, microvascular disturbances, parasite-dependent myocardial aggression, and immune-mediated myocardial injury. Despite its prominent peculiarities, the role of autonomic derangements and microcirculatory disturbances is probably ancillary among causes of chronic myocardial damage. The pathogenesis of chronic Chagas heart disease is dependent on a low-grade but incessant systemic infection with documented immune-adverse reaction. Parasite persistence and immunological mechanisms are inextricably related in the myocardial aggression in the chronic phase of Chagas heart disease. Conclusions--- Most clinical studies have been performed in very small number of patients. Future research should explore the clinical potential implications and therapeutic opportunities of these 2 fundamental underlying pathogenetic mechanisms.},
  keywords = {autonomic nervous system,cardiomyopathy,Chagas disease,immune system,inflammation,microcirculation,Valsalva maneuver},
  file = {/Users/antoniohortaribeiro/Zotero/storage/LU9DGZ79/Marin-Neto et al_2007_Pathogenesis of Chronic Chagas Heart Disease.pdf}
}

@book{markovsky_exact_2006,
  title = {Exact and Approximate Modeling of Linear Systems: {{A}} Behavioral Approach},
  author = {Markovsky, Ivan and Willems, Jan C and Van Huffel, Sabine and De Moor, Bart},
  year = {2006},
  publisher = {SIAM}
}

@article{maros_repository_1999,
  title = {A Repository of Convex Quadratic Programming Problems},
  author = {Maros, Istvan and M{\'e}sz{\'a}ros, Csaba},
  year = {1999},
  journal = {Optimization Methods and Software},
  volume = {11},
  number = {1-4},
  pages = {671--681},
  doi = {10.1080/10556789908805768},
  urldate = {2017-08-20},
  file = {/Users/antoniohortaribeiro/Zotero/storage/LNX5BB5R/maros_a_1999.pdf}
}

@article{maros_repository_1999a,
  title = {A {{Repository}} of {{Convex Quadratic Programming Problems}}},
  author = {Maros, Istvan and M{\'e}sz{\'a}ros, Csaba},
  year = {1999},
  journal = {Optimization Methods and Software},
  volume = {11},
  number = {1-4},
  pages = {671--681},
  doi = {10/cvxwrj},
  urldate = {2017-08-20}
}

@article{marquardt_algorithm_1963,
  title = {An Algorithm for Least-Squares Estimation of Nonlinear Parameters},
  author = {Marquardt, Donald W},
  year = {1963},
  journal = {Journal of the Society for Industrial and Applied Mathematics},
  volume = {11},
  number = {2},
  pages = {431--441},
  doi = {10.1137/0111030}
}

@inproceedings{martens_learning_2011,
  title = {Learning {{Recurrent Neural Networks}} with {{Hessian-free Optimization}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Martens, James and Sutskever, Ilya},
  year = {2011},
  series = {{{ICML}}'11},
  pages = {1033--1040},
  publisher = {Omnipress},
  address = {USA},
  urldate = {2019-04-23},
  abstract = {In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-the-art method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of Schraudolph (2002) which is used within the HF approach of Martens.},
  isbn = {978-1-4503-0619-5},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TN6RJL8Z/Martens and Sutskever - Learning Recurrent Neural Networks with Hessian-Fr.pdf}
}

@article{martinez-millana_artificial_2022,
  title = {Artificial Intelligence and Its Impact on the Domains of Universal Health Coverage, Health Emergencies and Health Promotion: {{An}} Overview of Systematic Reviews},
  shorttitle = {Artificial Intelligence and Its Impact on the Domains of Universal Health Coverage, Health Emergencies and Health Promotion},
  author = {{Martinez-Millana}, Antonio and {Saez-Saez}, Aida and {Tornero-Costa}, Roberto and {Azzopardi-Muscat}, Natasha and Traver, Vicente and {Novillo-Ortiz}, David},
  year = {2022},
  month = oct,
  journal = {International Journal of Medical Informatics},
  volume = {166},
  pages = {104855},
  issn = {1386-5056},
  doi = {10.1016/j.ijmedinf.2022.104855},
  urldate = {2024-08-19},
  abstract = {Background Artificial intelligence is fueling a new revolution in medicine and in the healthcare sector. Despite the growing evidence on the benefits of artificial intelligence there are several aspects that limit the measure of its impact in people's health. It is necessary to assess the current status on the application of AI towards the improvement of people's health in the domains defined by WHO's Thirteenth General Programme of Work (GPW13) and the European Programme of Work (EPW), to inform about trends, gaps, opportunities, and challenges. Objective To perform a systematic overview of systematic reviews on the application of artificial intelligence in the people's health domains as defined in the GPW13 and provide a comprehensive and updated map on the application specialties of artificial intelligence in terms of methodologies, algorithms, data sources, outcomes, predictors, performance, and methodological quality. Methods A systematic search in MEDLINE, EMBASE, Cochrane and IEEEXplore was conducted between January 2015 and June 2021 to collect systematic reviews using a combination of keywords related to the domains of universal health coverage, health emergencies protection, and better health and wellbeing as defined by the WHO's PGW13 and EPW. Eligibility criteria was based on methodological quality and the inclusion of practical implementation of artificial intelligence. Records were classified and labeled using ICD-11 categories into the domains of the GPW13. Descriptors related to the area of implementation, type of modeling, data entities, outcomes and implementation on care delivery were extracted using a structured form and methodological aspects of the included reviews studies was assessed using the AMSTAR checklist. Results The search strategy resulted in the screening of 815 systematic reviews from which 203 were assessed for eligibility and 129 were included in the review. The most predominant domain for artificial intelligence applications was Universal Health Coverage (N~=~98) followed by Health Emergencies (N~=~16) and Better Health and Wellbeing (N~=~15). Neoplasms area on Universal Health Coverage was the disease area featuring most of the applications (21.7~\%, N~=~28). The reviews featured analytics primarily over both public and private data sources (67.44~\%, N~=~87). The most used type of data was medical imaging (31.8~\%, N~=~41) and predictors based on regions of interest and clinical data. The most prominent subdomain of Artificial Intelligence was Machine Learning (43.4~\%, N~=~56), in which Support Vector Machine method was predominant (20.9~\%, N~=~27). Regarding the purpose, the application of Artificial Intelligence I is focused on the prediction of the diseases (36.4~\%, N~=~47). With respect to the validation, more than a half of the reviews (54.3~\%, N~=~70) did not report a validation procedure and, whenever available, the main performance indicator was the accuracy (28.7~\%, N~=~37). According to the methodological quality assessment, a third of the reviews (34.9~\%, N~=~45) implemented methods for analysis the risk of bias and the overall AMSTAR score below was 5 (4.01~{\textpm}~1.93) on all the included systematic reviews. Conclusion Artificial intelligence is being used for disease modelling, diagnose, classification and prediction in the three domains of GPW13. However, the evidence is often limited to laboratory and the level of adoption is largely unbalanced between ICD-11 categoriesand diseases. Data availability is a determinant factor on the developmental stage of artificial intelligence applications. Most of the reviewed studies show a poor methodological quality and are at high risk of bias, which limits the reproducibility of the results and the reliability of translating these applications to real clinical scenarios. The analyzed papers show results only in laboratory and testing scenarios and not in clinical trials nor case studies, limiting the supporting evidence to transfer artificial intelligence to actual care delivery.},
  keywords = {European region,Health and well-being,Health emergencies,Machine learning,Universal health coverage},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9WJ7LUYP/Martinez-Millana et al. - 2022 - Artificial intelligence and its impact on the doma.pdf;/Users/antoniohortaribeiro/Zotero/storage/EUP82RHF/S1386505622001691.html}
}

@inproceedings{masti_learning_2018,
  title = {Learning {{Nonlinear State-Space Models Using Deep Autoencoders}}},
  booktitle = {2018 {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Masti, D. and Bemporad, A.},
  year = {2018},
  pages = {3862--3867},
  doi = {10/gfwtwq},
  isbn = {2576-2370},
  keywords = {Computational modeling,deep autoencoders,dimensionality reduction,direct acyclic computational graph,directed graphs,learning (artificial intelligence),machine-learning techniques,model order reduction,neural networks,Neural networks,neural state observer,neurocontrollers,nonlinear control systems,nonlinear model predictive control,nonlinear state-space models,nonlinear system,Nonlinear systems,observers,Observers,predictive control,reduced order systems,state-space methods,State-space methods,state-update maps,Training,Tuning}
}

@article{matthews_gaussian_2018,
  title = {Gaussian {{Process Behaviour}} in {{Wide Deep Neural Networks}}},
  author = {Matthews, Alexander G. de G. and Rowland, Mark and Hron, Jiri and Turner, Richard E. and Ghahramani, Zoubin},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.11271 [cs, stat]},
  eprint = {1804.11271},
  primaryclass = {cs, stat},
  urldate = {2018-10-24},
  abstract = {Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between random, wide, fully connected, feedforward networks with more than one hidden layer and Gaussian processes with a recursive kernel definition. We show that, under broad conditions, as we make the architecture increasingly wide, the implied random function converges in distribution to a Gaussian process, formalising and extending existing results by Neal (1996) to deep networks. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then compare finite Bayesian deep networks from the literature to Gaussian processes in terms of the key predictive quantities of interest, finding that in some cases the agreement can be very close. We discuss the desirability of Gaussian process behaviour and review non-Gaussian alternative models from the literature.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/P85N88SS/matthews_gaussian_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/9UM9I6DM/1804.html}
}

@book{mattuck_introduction_1999,
  title = {Introduction to {{Analysis}}},
  author = {Mattuck, A.},
  year = {1999},
  publisher = {Prentice Hall},
  isbn = {978-0-13-081132-5},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EJT24VEG/mattuck_introducti_1999.pdf}
}

@article{may_simple_1976,
  title = {Simple Mathematical Models with Very Complicated Dynamics},
  author = {May, Robert M},
  year = {1976},
  journal = {Nature},
  volume = {261},
  number = {5560},
  pages = {459--467},
  doi = {10.1038/261459a0}
}

@book{mccool_structured_2012,
  title = {Structured Parallel Programing: Patterns for Efficient Computation},
  shorttitle = {Structured Parallel Programing},
  author = {McCool, Michael D. and Robison, Arch D. and Reinders, James},
  year = {2012},
  publisher = {Elsevier, Morgan Kaufmann},
  address = {Amsterdam},
  isbn = {978-0-12-415993-8},
  lccn = {QA76.76.P37 M34 2012},
  keywords = {Software patterns,Structured programming},
  file = {/Users/antoniohortaribeiro/Zotero/storage/P999H5A9/mccool_structured_2012.pdf}
}

@book{mccullagh_generalized_1989,
  title = {Generalized {{Linear Models}}, {{Second Edition}}},
  author = {McCullagh, P. and Nelder, J.A.},
  year = {1989},
  series = {Chapman \& {{Hall}}/{{CRC Monographs}} on {{Statistics}} \& {{Applied Probability}}},
  publisher = {Taylor \& Francis},
  isbn = {978-0-412-31760-6},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PCTSPN8V/mccullagh_generalize_1989.pdf}
}

@article{mcculloch_logical_1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S and Pitts, Walter},
  year = {1943},
  journal = {The bulletin of mathematical biophysics},
  volume = {5},
  number = {4},
  pages = {115--133},
  issn = {0007-4985}
}

@article{mcdonald_instabilities_1973,
  title = {Instabilities of {{Regression Estimates Relating Air Pollution}} to {{Mortality}}},
  author = {McDonald, Gary C. and Schwing, Richard C.},
  year = {1973},
  journal = {Technometrics},
  volume = {15},
  number = {3},
  eprint = {1266852},
  eprinttype = {jstor},
  pages = {463--481},
  publisher = {[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
  issn = {0040-1706},
  doi = {10.2307/1266852},
  urldate = {2024-05-16},
  abstract = {The instability of ordinary least squares estimates of linear regression coefficients is demonstrated for mortality rates regressed around various socioeconomic, weather and pollution variables. A ridge regression technique presented by Hoerl and Kennard (Technometrics 12 (1970) 69-82) is employed to arrive at "stable" regression coefficients which, in some instances, differ considerably from the ordinary least squares estimates. In addition, two methods of variable elimination are compared-one based on total squared error and the other on a ridge trace analysis.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YYCMCUL8/McDonald and Schwing - 1973 - Instabilities of Regression Estimates Relating Air.pdf}
}

@article{mckinney_international_2020,
  title = {International Evaluation of an {{AI}} System for Breast Cancer Screening},
  author = {McKinney, Scott Mayer and Sieniek, Marcin and Godbole, Varun and Godwin, Jonathan and Antropova, Natasha and Ashrafian, Hutan and Back, Trevor and Chesus, Mary and Corrado, Greg C. and Darzi, Ara and Etemadi, Mozziyar and {Garcia-Vicente}, Florencia and Gilbert, Fiona J. and {Halling-Brown}, Mark and Hassabis, Demis and Jansen, Sunny and Karthikesalingam, Alan and Kelly, Christopher J. and King, Dominic and Ledsam, Joseph R. and Melnick, David and Mostofi, Hormuz and Peng, Lily and Reicher, Joshua Jay and {Romera-Paredes}, Bernardino and Sidebottom, Richard and Suleyman, Mustafa and Tse, Daniel and Young, Kenneth C. and De Fauw, Jeffrey and Shetty, Shravya},
  year = {2020},
  month = jan,
  journal = {Nature},
  volume = {577},
  number = {7788},
  pages = {89--94},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1799-6},
  abstract = {Screening mammography aims to identify breast cancer at earlier stages of the disease, when treatment can be more successful1. Despite the existence of screening programmes worldwide, the interpretation of mammograms is affected by high rates of false positives and false negatives2. Here we present an artificial intelligence (AI) system that is capable of surpassing human experts in breast cancer prediction. To assess its performance in the clinical setting, we curated a large representative dataset from the UK and a large enriched dataset from the USA. We show an absolute reduction of 5.7\%~and 1.2\% (USA and UK) in false positives and 9.4\%~and 2.7\% in false negatives. We provide evidence of the ability of the system to generalize from the UK to the USA. In an independent study of six radiologists, the AI system outperformed all of the human readers: the area under the receiver operating characteristic curve (AUC-ROC) for the AI system was greater than the AUC-ROC for the average radiologist by an absolute margin of 11.5\%. We ran a simulation in which the AI system participated in the double-reading process that is used in the UK, and found that the AI system maintained non-inferior performance and reduced the workload of the second reader by 88\%. This robust assessment of the AI system paves the way for clinical trials to improve the accuracy and efficiency of breast cancer screening.}
}

@article{mcnemar_note_1947,
  title = {Note on the Sampling Error of the Difference between Correlated Proportions or Percentages},
  author = {McNemar, Quinn},
  year = {1947},
  month = jun,
  journal = {Psychometrika},
  volume = {12},
  number = {2},
  pages = {153--157},
  issn = {1860-0980},
  doi = {10/d9pvhs},
  urldate = {2019-01-28},
  abstract = {Two formulas are presented for judging the significance of the difference between correlated proportions. The chi square equivalent of one of the developed formulas is pointed out.},
  langid = {english},
  keywords = {Correlate Proportion,Develop Formula,Public Policy,Sampling Error,Statistical Theory},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Z73PXKNA/mcnemar_note on_1947.pdf}
}

@article{mcsharry_dynamical_2003,
  title = {A {{Dynamical Model}} for {{Generating Synthetic Electrocardiogram Signals}}},
  author = {McSharry, Patrick E and Clifford, Gari D and Tarassenko, Lionel and Smith, Leonard A},
  year = {2003},
  journal = {IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING},
  volume = {50},
  number = {3},
  pages = {6},
  abstract = {A dynamical model based on three coupled ordinary differential equations is introduced which is capable of generating realistic synthetic electrocardiogram (ECG) signals. The operator can specify the mean and standard deviation of the heart rate, the morphology of the PQRST cycle, and the power spectrum of the RR tachogram. In particular, both respiratory sinus arrhythmia at the high frequencies (HFs) and Mayer waves at the low frequencies (LFs) together with the LF/HF ratio are incorporated in the model. Much of the beat-to-beat variation in morphology and timing of the human ECG, including QT dispersion and R-peak amplitude modulation are shown to result. This model may be employed to assess biomedical signal processing techniques which are used to compute clinical statistics from the ECG.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5P8TYW88/McSharry et al. - 2003 - A Dynamical Model for Generating Synthetic Electro.pdf}
}

@article{medioni_segmentbased_1985,
  title = {Segment-Based Stereo Matching},
  author = {Medioni, Gerard and Nevatia, Ramakant},
  year = {1985},
  journal = {Computer Vision, Graphics, and Image Processing},
  volume = {31},
  number = {1},
  pages = {2--18},
  doi = {10.1016/S0734-189X(85)80073-6}
}

@article{medioni_segmentbased_1985a,
  title = {Segment-{{Based Stereo Matching}}},
  author = {Medioni, Gerard and Nevatia, Ramakant},
  year = {1985},
  journal = {Computer Vision, Graphics, and Image Processing},
  volume = {31},
  number = {1},
  pages = {2--18},
  doi = {10/dc7k3t},
  annotation = {00598}
}

@article{mehrkanoon_regularized_2017,
  title = {Regularized {{Semipaired Kernel CCA}} for {{Domain Adaptation}}},
  author = {Mehrkanoon, S. and Suykens, J. A. K.},
  year = {2017},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {PP},
  number = {99},
  pages = {1--15},
  issn = {2162-237X},
  doi = {10.1109/TNNLS.2017.2728719},
  abstract = {Domain adaptation learning is one of the fundamental research topics in pattern recognition and machine learning. This paper introduces a regularized semipaired kernel canonical correlation analysis formulation for learning a latent space for the domain adaptation problem. The optimization problem is formulated in the primal-dual least squares support vector machine setting where side information can be readily incorporated through regularization terms. The proposed model learns a joint representation of the data set across different domains by solving a generalized eigenvalue problem or linear system of equations in the dual. The approach is naturally equipped with out-of-sample extension property, which plays an important role for model selection. Furthermore, the Nystr{\"o}m approximation technique is used to make the computational issues due to the large size of the matrices involved in the eigendecomposition feasible. The learned latent space of the source domain is fed to a multiclass semisupervised kernel spectral clustering model that can learn from both labeled and unlabeled data points of the source domain in order to classify the data instances of the target domain. Experimental results are given to illustrate the effectiveness of the proposed approaches on synthetic and real-life data sets.},
  keywords = {Adaptation models,Correlation,Data models,Domain adaption,Eigenvalues and eigenfunctions,Kernel,kernel canonical correlation analysis (KCCA),Mathematical model,Nystr{\"o}m approximation,Optimization,semisupervised learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FF4X9GV5/mehrkanoon_regularize_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/NSS6XF4Q/mehrkanoon_regularize_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/XG9FS5MA/mehrkanoon_regularize_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/2UTSZ9CB/7999259.html;/Users/antoniohortaribeiro/Zotero/storage/36H7TTIC/7999259.html;/Users/antoniohortaribeiro/Zotero/storage/GJSQEKAN/7999259.html;/Users/antoniohortaribeiro/Zotero/storage/UP56TPXD/7999259.html;/Users/antoniohortaribeiro/Zotero/storage/UQIKBTTR/7999259.html;/Users/antoniohortaribeiro/Zotero/storage/WLYNSVTN/7999259.html}
}

@inproceedings{mei_building_2011,
  title = {On Building an Accurate Stereo Matching System on Graphics Hardware},
  booktitle = {Computer {{Vision Workshops}} ({{ICCV Workshops}}), 2011 {{IEEE International Conference}} On},
  author = {Mei, Xing and Sun, Xun and Zhou, Mingcai and Jiao, Shaohui and Wang, Haitao and Zhang, Xiaopeng},
  year = {2011},
  pages = {467--474},
  publisher = {IEEE}
}

@article{mei_generalization_2022,
  title = {The {{Generalization Error}} of {{Random Features Regression}}: {{Precise Asymptotics}} and the {{Double Descent Curve}}},
  shorttitle = {The Generalization Error of Random Features Regression},
  author = {Mei, Song and Montanari, Andrea},
  year = {2022},
  journal = {Communications on Pure and Applied Mathematics},
  volume = {75},
  number = {4},
  eprint = {1908.05355},
  pages = {667--766},
  doi = {10.1002/cpa.22008},
  urldate = {2020-07-22},
  abstract = {Deep learning methods operate in regimes that defy the traditional statistical mindset. The neural network architectures often contain more parameters than training samples, and are so rich that they can interpolate the observed labels, even if the latter are replaced by pure noise. Despite their huge complexity, the same architectures achieve small generalization error on real data. This phenomenon has been rationalized in terms of a so-called `double descent' curve. As the model complexity increases, the generalization error follows the usual U-shaped curve at the beginning, first decreasing and then peaking around the interpolation threshold (when the model achieves vanishing training error). However, it descends again as model complexity exceeds this threshold. The global minimum of the generalization error is found in this overparametrized regime, often when the number of parameters is much larger than the number of samples. Far from being a peculiar property of deep neural networks, elements of this behavior have been demonstrated in much simpler settings, including linear regression with random covariates. In this paper we consider the problem of learning an unknown function over the \$d\$-dimensional sphere \${\textbackslash}mathbb S{\textasciicircum}\{d-1\}\$, from \$n\$ i.i.d. samples \$({\textbackslash}boldsymbol x\_i, y\_i) {\textbackslash}in {\textbackslash}mathbb S{\textasciicircum}\{d-1\} {\textbackslash}times {\textbackslash}mathbb R\$, \$i {\textbackslash}le n\$. We perform ridge regression on \$N\$ random features of the form \${\textbackslash}sigma({\textbackslash}boldsymbol w\_a{\textasciicircum}\{{\textbackslash}mathsf T\}{\textbackslash}boldsymbol x)\$, \$a {\textbackslash}le N\$. This can be equivalently described as a two-layers neural network with random first-layer weights. We compute the precise asymptotics of the generalization error, in the limit \$N, n, d {\textbackslash}to {\textbackslash}infty\$ with \$N/d\$ and \$n/d\$ fixed. This provides the first analytically tractable model that captures all the features of the double descent phenomenon without assuming ad hoc misspecification structures.},
  archiveprefix = {arXiv},
  keywords = {62J99,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CB5ID42N/Mei_Montanari_2020_The generalization error of random features regression.pdf;/Users/antoniohortaribeiro/Zotero/storage/PZDSXUHF/1908.html}
}

@article{meier_group_2008,
  title = {The {{Group Lasso}} for {{Logistic Regression}}},
  author = {Meier, Lukas and {van de Geer}, Sara and B{\"u}hlmann, Peter},
  year = {2008},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {70},
  number = {1},
  eprint = {20203811},
  eprinttype = {jstor},
  pages = {53--71},
  issn = {1369-7412},
  doi = {10.1111/j.1467-9868.2007.00627.x},
  abstract = {The group lasso is an extension of the lasso to do variable selection on (predefined) groups of variables in linear regression models. The estimates have the attractive property of being invariant under groupwise orthogonal reparameterizations. We extend the group lasso to logistic regression models and present an efficient algorithm, that is especially suitable for high dimensional problems, which can also be applied to generalized linear models to solve the corresponding convex optimization problem. The group lasso estimator for logistic regression is shown to be statistically consistent even if the number of predictors is much larger than sample size but with sparse true underlying structure. We further use a two-stage procedure which aims for sparser models than the group lasso, leading to improved prediction performance for some cases. Moreover, owing to the two-stage nature, the estimates can be constructed to be hierarchical. The methods are used on simulated and real data sets about splice site detection in DNA sequences.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7M6HEGWA/meier_the group_2008.pdf}
}

@article{meirajr_contextualized_2020,
  title = {Contextualized {{Interpretable Machine Learning}} for {{Medical Diagnosis}}},
  author = {Meira Jr, Wagner and Ribeiro, Antonio L. P. and Oliveira, Derick M. and Ribeiro, Antonio H.},
  year = {2020},
  journal = {Communications of the ACM},
  doi = {10.1145/3416965},
  copyright = {All rights reserved},
  file = {/Users/antoniohortaribeiro/Zotero/storage/B8IN2Z5M/Meira Jr et al. - 2020 - Contextualized Interpretable Machine Learning for .pdf}
}

@article{menezes_longterm_2008,
  title = {Long-Term Time Series Prediction with the {{NARX}} Network: {{An}} Empirical Evaluation},
  shorttitle = {Long-Term Time Series Prediction with the {{NARX}} Network},
  author = {Menezes, Jos{\'e} Maria P. and Barreto, Guilherme A.},
  year = {2008},
  month = oct,
  journal = {Neurocomputing},
  series = {Advances in {{Neural Information Processing}} ({{ICONIP}} 2006) / {{Brazilian Symposium}} on {{Neural Networks}} ({{SBRN}} 2006)},
  volume = {71},
  number = {16},
  pages = {3335--3343},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2008.01.030},
  abstract = {The NARX network is a dynamical neural architecture commonly used for input--output modeling of nonlinear dynamical systems. When applied to time series prediction, the NARX network is designed as a feedforward time delay neural network (TDNN), i.e., without the feedback loop of delayed outputs, reducing substantially its predictive performance. In this paper, we show that the original architecture of the NARX network can be easily and efficiently applied to long-term (multi-step-ahead) prediction of univariate time series. We evaluate the proposed approach using two real-world data sets, namely the well-known chaotic laser time series and a variable bit rate (VBR) video traffic time series. All the results show that the proposed approach consistently outperforms standard neural network based predictors, such as the TDNN and Elman architectures.},
  keywords = {Chaotic time series,Long-term prediction,NARX neural network,Nonlinear traffic modeling,Recurrence plot},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6IFV7DVA/menezes_long-term_2008.pdf;/Users/antoniohortaribeiro/Zotero/storage/GHDX5ABS/menezes_long-term_2008.pdf;/Users/antoniohortaribeiro/Zotero/storage/5R2UJ9VA/S0925231208003081.html;/Users/antoniohortaribeiro/Zotero/storage/DXSBRPPZ/S0925231208003081.html}
}

@article{menick_generating_2019,
  title = {{{GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING}}},
  author = {Menick, Jacob and Kalchbrenner, Nal},
  year = {2019},
  pages = {15},
  abstract = {The unconditional generation of high fidelity images is a longstanding benchmark for testing the performance of image decoders. Autoregressive image models have been able to generate small images unconditionally, but the extension of these methods to large images where fidelity can be more readily assessed has remained an open problem. Among the major challenges are the capacity to encode the vast previous context and the sheer difficulty of learning a distribution that preserves both global semantic coherence and exactness of detail. To address the former challenge, we propose the Subscale Pixel Network (SPN), a conditional decoder architecture that generates an image as a sequence of sub-images of equal size. The SPN compactly captures image-wide spatial dependencies and requires a fraction of the memory and the computation required by other fully autoregressive models. To address the latter challenge, we propose to use Multidimensional Upscaling to grow an image in both size and depth via intermediate stages utilising distinct SPNs. We evaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of ImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in multiple settings, set up new benchmark results in previously unexplored settings and are able to generate very high fidelity large scale samples on the basis of both datasets.},
  langid = {english},
  keywords = {â›” No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6FLJMSSP/Menick and Kalchbrenner - 2019 - GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXE.pdf}
}

@article{merity_pointer_2016,
  title = {Pointer {{Sentinel Mixture Models}}},
  author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  year = {2016},
  month = sep,
  journal = {arXiv:1609.07843},
  eprint = {1609.07843},
  urldate = {2019-05-31},
  abstract = {Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/antoniohortaribeiro/Zotero/storage/D7H5XETR/merity_pointer_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/IW5BC3Y8/1609.html}
}

@article{meronen_periodic_2021,
  title = {Periodic {{Activation Functions Induce Stationarity}}},
  author = {Meronen, Lassi and Trapp, Martin and Solin, Arno},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.13572 [cs, stat]},
  eprint = {2110.13572},
  primaryclass = {cs, stat},
  urldate = {2021-11-22},
  abstract = {Neural network models are known to reinforce hidden data biases, making them unreliable and difficult to interpret. We seek to build models that `know what they do not know' by introducing inductive biases in the function space. We show that periodic activation functions in Bayesian neural networks establish a connection between the prior on the network weights and translation-invariant, stationary Gaussian process priors. Furthermore, we show that this link goes beyond sinusoidal (Fourier) activations by also covering triangular wave and periodic ReLU activation functions. In a series of experiments, we show that periodic activation functions obtain comparable performance for in-domain data and capture sensitivity to perturbed inputs in deep neural networks for out-of-domain detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9DN2F5BY/Meronen et al. - 2021 - Periodic Activation Functions Induce Stationarity.pdf;/Users/antoniohortaribeiro/Zotero/storage/APP4D4FU/2110.html}
}

@article{meronen_stationary_2020,
  title = {Stationary {{Activations}} for {{Uncertainty Calibration}} in {{Deep Learning}}},
  author = {Meronen, Lassi and Irwanto, Christabella and Solin, Arno},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.09494 [cs]},
  eprint = {2010.09494},
  primaryclass = {cs},
  urldate = {2021-11-24},
  abstract = {We introduce a new family of non-linear neural network activation functions that mimic the properties induced by the widely-used Mat{\textbackslash}'ern family of kernels in Gaussian process (GP) models. This class spans a range of locally stationary models of various degrees of mean-square differentiability. We show an explicit link to the corresponding GP models in the case that the network consists of one infinitely wide hidden layer. In the limit of infinite smoothness the Mat{\textbackslash}'ern family results in the RBF kernel, and in this case we recover RBF activations. Mat{\textbackslash}'ern activation functions result in similar appealing properties to their counterparts in GP models, and we demonstrate that the local stationarity property together with limited mean-square differentiability shows both good performance and uncertainty calibration in Bayesian deep learning tasks. In particular, local stationarity helps calibrate out-of-distribution (OOD) uncertainty. We demonstrate these properties on classification and regression benchmarks and a radar emitter classification task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JWFCFZHB/Meronen et al_2020_Stationary Activations for Uncertainty Calibration in Deep Learning.pdf;/Users/antoniohortaribeiro/Zotero/storage/AMYR7U4U/2010.html}
}

@article{meyers_comparison_2021,
  title = {Comparison of the {{ST-Elevation Myocardial Infarction}} ({{STEMI}}) vs. {{NSTEMI}} and {{Occlusion MI}} ({{OMI}}) vs. {{NOMI Paradigms}} of {{Acute MI}}},
  author = {Meyers, H. Pendell and Bracey, Alexander and Lee, Daniel and Lichtenheld, Andrew and Li, Wei J. and Singer, Daniel D. and Kane, Jesse A. and Dodd, Kenneth W. and Meyers, Kristen E. and Thode, Henry C. and Shroff, Gautam R. and Singer, Adam J. and Smith, Stephen W.},
  year = {2021},
  month = mar,
  journal = {The Journal of Emergency Medicine},
  volume = {60},
  number = {3},
  pages = {273--284},
  issn = {0736-4679},
  doi = {10.1016/j.jemermed.2020.10.026},
  abstract = {BACKGROUND: The current ST-elevation myocardial infarction (STEMI) vs. non-STEMI (NSTEMI) paradigm prevents some NSTEMI patients with acute coronary occlusion from receiving emergent reperfusion, in spite of their known increased mortality compared with NSTEMI without occlusion. We have proposed a new paradigm known as occlusion MI vs. nonocclusion MI (OMI vs. NOMI). OBJECTIVE: We aimed to compare the two paradigms within a single population. We hypothesized that STEMI(-) OMI would have characteristics similar to STEMI(+) OMI but longer time to catheterization. METHODS: We performed a retrospective review of a prospectively collected acute coronary syndrome population. OMI was defined as an acute culprit and either TIMI 0-2 flow or TIMI 3 flow plus peak troponin T~{$>~$}1.0~ng/mL. We collected electrocardiograms, demographic characteristics, laboratory results, angiographic data, and outcomes. RESULTS: Among 467 patients, there were 108 OMIs, with only 60\% (67 of 108) meeting STEMI criteria. Median peak troponin T for the STEMI(+) OMI, STEMI(-) OMI, and no occlusion groups were 3.78 (interquartile range [IQR] 2.18-7.63), 1.87 (IQR 1.12-5.48), and 0.00 (IQR 0.00-0.08). Median time from arrival to catheterization was 41~min (IQR 23-86~min) for STEMI(+) OMI compared with 437~min (IQR 85-1590~min) for STEMI(-) OMI (p~{$<~$}0.001). STEMI(+) OMI was more likely than STEMI(-) OMI to undergo catheterization within 90~min (76\% vs. 28\%; p~{$<~$}0.001). CONCLUSIONS: STEMI(-) OMI patients had significant delays to catheterization but adverse outcomes more similar to STEMI(+) OMI than those with no occlusion. These data support the OMI/NOMI paradigm and the importance of further research into emergent reperfusion for STEMI(-) OMI.},
  langid = {english},
  pmid = {33308915},
  keywords = {acute coronary syndrome,acute myocardial infarction,electrocardiogram,Electrocardiography,Humans,Myocardial Infarction,Non-ST Elevated Myocardial Infarction,occlusion myocardial infarction,Retrospective Studies,ST Elevation Myocardial Infarction,ST-segment elevation myocardial infarction}
}

@article{mezic_spectral_2005,
  title = {Spectral {{Properties}} of {{Dynamical Systems}}, {{Model Reduction}} and {{Decompositions}}},
  author = {Mezi{\'c}, Igor},
  year = {2005},
  month = aug,
  journal = {Nonlinear Dynamics},
  volume = {41},
  number = {1},
  pages = {309--325},
  issn = {1573-269X},
  doi = {10.1007/s11071-005-2824-x},
  urldate = {2020-12-15},
  abstract = {In this paper we discuss two issues related to model reduction of deterministic or stochastic processes. The first is the relationship of the spectral properties of the dynamics on the attractor of the original, high-dimensional dynamical system with the properties and possibilities for model reduction. We review some elements of the spectral theory of dynamical systems. We apply this theory to obtain a decomposition of the process that utilizes spectral properties of the linear Koopman operator associated with the asymptotic dynamics on the attractor. This allows us to extract the almost periodic part of the evolving process. The remainder of the process has continuous spectrum. The second topic we discuss is that of model validation, where the original, possibly high-dimensional dynamics and the dynamics of the reduced model -- that can be deterministic or stochastic -- are compared in some norm. Using the ``statistical Takens theorem'' proven in (Mezi{\'c}, I. and Banaszuk, A. Physica D, 2004) we argue that comparison of average energy contained in the finite-dimensional projection is one in the hierarchy of functionals of the field that need to be checked in order to assess the accuracy of the projection.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SGM4PNJ7/MeziÄ‡ - 2005 - Spectral Properties of Dynamical Systems, Model Re.pdf}
}

@article{mhammedi_efficient_2016,
  title = {Efficient {{Orthogonal Parametrisation}} of {{Recurrent Neural Networks Using Householder Reflections}}},
  author = {Mhammedi, Zakaria and Hellicar, Andrew and Rahman, Ashfaqur and Bailey, James},
  year = {2016},
  month = dec,
  journal = {arXiv:1612.00188 [cs]},
  eprint = {1612.00188},
  primaryclass = {cs},
  urldate = {2019-09-19},
  abstract = {The problem of learning long-term dependencies in sequences using Recurrent Neural Networks (RNNs) is still a major challenge. Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training which ensures that its norm is equal to one and prevents exploding gradients. These methods either have limited expressiveness or scale poorly with the size of the network when compared with the simple RNN case, especially when using stochastic gradient descent with a small mini-batch size. Our contributions are as follows; we first show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint. Then we present a new parametrisation of the transition matrix which allows efficient training of an RNN while ensuring that the matrix is always orthogonal. Our results show that the orthogonal constraint on the transition matrix applied through our parametrisation gives similar benefits to the unitary constraint, without the time complexity limitations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {â›” No DOI found,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PRF6MBQ8/Mhammedi et al. - 2016 - Efficient Orthogonal Parametrisation of Recurrent .pdf}
}

@inproceedings{mhammedi_efficient_2017,
  title = {Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning-Volume}} 70},
  author = {Mhammedi, Zakaria and Hellicar, Andrew and Rahman, Ashfaqur and Bailey, James},
  year = {2017},
  pages = {2401--2409},
  publisher = {JMLR. org},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IRCZCT7W/mhammedi_efficient_2017.pdf}
}

@inproceedings{mikolov_distributed_2013,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
  year = {2013},
  pages = {3111--3119},
  file = {/Users/antoniohortaribeiro/Zotero/storage/F56UZA7T/mikolov_distribute_2013.pdf}
}

@article{mikolov_efficient_2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = jan,
  journal = {arXiv:1301.3781 [cs]},
  eprint = {1301.3781},
  primaryclass = {cs},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computation and Language},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6KNEF7QB/mikolov_efficient_2013.pdf;/Users/antoniohortaribeiro/Zotero/storage/SF9DW48K/1301.html}
}

@inproceedings{mikolov_linguistic_2013,
  title = {Linguistic {{Regularities}} in {{Continuous Space Word Representations}}},
  booktitle = {Proceedings of the 2013 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  year = {2013},
  month = jun,
  pages = {746--751},
  publisher = {Association for Computational Linguistics},
  address = {Atlanta, Georgia},
  file = {/Users/antoniohortaribeiro/Zotero/storage/A3CRRVP9/mikolov_linguistic_2013.pdf;/Users/antoniohortaribeiro/Zotero/storage/BP5A5AIE/mikolov_linguistic_2013.pdf}
}

@article{milanese_model_2005,
  title = {Model Quality in Identification of Nonlinear Systems},
  author = {Milanese, Mario and Novara, Carlo},
  year = {2005},
  journal = {IEEE Transactions on Automatic Control},
  volume = {50},
  number = {10},
  pages = {1606--1611},
  doi = {10.1109/TAC.2005.856657}
}

@article{miller_stable_2018,
  title = {Stable {{Recurrent Models}}},
  author = {Miller, John and Hardt, Moritz},
  year = {2018},
  month = may,
  journal = {arXiv:1805.10369 [cs, stat]},
  eprint = {1805.10369},
  primaryclass = {cs, stat},
  urldate = {2019-07-27},
  abstract = {Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AK74H236/miller_stable_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/XP9IKTE5/1805.html}
}

@book{miller_subset_2002,
  title = {Subset Selection in Regression},
  author = {Miller, Alan},
  year = {2002},
  publisher = {CRC Press},
  isbn = {1-4200-3593-2}
}

@article{miller_when_2018,
  title = {When {{Recurrent Models Don}}'t {{Need To Be Recurrent}}},
  author = {Miller, John and Hardt, Moritz},
  year = {2018},
  month = may,
  journal = {arXiv:1805.10369 [cs, stat]},
  eprint = {1805.10369},
  primaryclass = {cs, stat},
  urldate = {2018-11-02},
  abstract = {We prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Our result applies to a broad range of non-linear recurrent neural networks under a natural stability condition, which we observe is also necessary. Complementing our theoretical findings, we verify the conclusions of our theory on both real and synthetic tasks. Furthermore, we demonstrate recurrent models satisfying the stability assumption of our theory can have excellent performance on real sequence learning tasks.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5JI8BSMV/miller_when_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/BGX7ZIL2/1805.html}
}

@article{min_curious_2021,
  title = {The {{Curious Case}} of {{Adversarially Robust Models}}: {{More Data Can Help}}, {{Double Descend}}, or {{Hurt Generalization}}},
  shorttitle = {The {{Curious Case}} of {{Adversarially Robust Models}}},
  author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
  year = {2021},
  journal = {Proceedings of the Conference on Uncertainty in Artificial Intelligence},
  volume = {161},
  pages = {129--139},
  abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classification problems. We first investigate the Gaussian mixture classification with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classification problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classification, support vector machines (SVMs), and linear regression.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VKWSXI2W/Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf;/Users/antoniohortaribeiro/Zotero/storage/8TG3UWMV/2002.html}
}

@article{minar_recent_2018,
  title = {Recent {{Advances}} in {{Deep Learning}}: {{An Overview}}},
  shorttitle = {Recent {{Advances}} in {{Deep Learning}}},
  author = {Minar, Matiur Rahman and Naher, Jibon},
  year = {2018},
  month = jul,
  doi = {10/gdvmmz},
  urldate = {2019-01-07},
  abstract = {Deep Learning is one of the newest trends in Machine Learning and Artificial Intelligence research. It is also one of the most popular scientific research trends now-a-days. Deep learning methods have brought revolutionary advances in computer vision and machine learning. Every now and then, new and new deep learning techniques are being born, outperforming state-of-the-art machine learning and even existing deep learning techniques. In recent years, the world has seen many major breakthroughs in this field. Since deep learning is evolving at a huge speed, its kind of hard to keep track of the regular advances especially for new researchers. In this paper, we are going to briefly discuss about recent advances in Deep Learning for past few years.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QCW8XKRW/minar_recent_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/MU4IVGW3/1807.html}
}

@article{minchole_artificial_,
  title = {Artificial Intelligence for the Electrocardiogram},
  author = {Minchol{\'e}, Ana and Rodriguez, Blanca},
  journal = {Nature Medicine},
  pages = {2},
  doi = {10/gfsvzh},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/38KCF22H/MincholÃ© and Rodriguez - Artificial intelligence for the electrocardiogram.pdf}
}

@article{mirza_conditional_2014,
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  year = {2014},
  month = nov,
  journal = {arXiv:1411.1784 [cs, stat]},
  eprint = {1411.1784},
  primaryclass = {cs, stat},
  urldate = {2018-12-04},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4DIM2AVM/mirza_conditiona_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/L7CIYAMU/1411.html}
}

@article{mitra_understanding_2019,
  title = {Understanding Overfitting Peaks in Generalization Error: {{Analytical}} Risk Curves for \$l\_2\$ and \$l\_1\$ Penalized Interpolation},
  shorttitle = {Understanding Overfitting Peaks in Generalization Error},
  author = {Mitra, Partha P.},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.03667 [physics, stat]},
  eprint = {1906.03667},
  primaryclass = {physics, stat},
  urldate = {2021-06-24},
  abstract = {Traditionally in regression one minimizes the number of fitting parameters or uses smoothing/regularization to trade training (TE) and generalization error (GE). Driving TE to zero by increasing fitting degrees of freedom (dof) is expected to increase GE. However modern big-data approaches, including deep nets, seem to over-parametrize and send TE to zero (data interpolation) without impacting GE. Overparametrization has the benefit that global minima of the empirical loss function proliferate and become easier to find. These phenomena have drawn theoretical attention. Regression and classification algorithms have been shown that interpolate data but also generalize optimally. An interesting related phenomenon has been noted: the existence of non-monotonic risk curves, with a peak in GE with increasing dof. It was suggested that this peak separates a classical regime from a modern regime where over-parametrization improves performance. Similar over-fitting peaks were reported previously (statistical physics approach to learning) and attributed to increased fitting model flexibility. We introduce a generative and fitting model pair ("Misparametrized Sparse Regression" or MiSpaR) and show that the overfitting peak can be dissociated from the point at which the fitting function gains enough dof's to match the data generative model and thus provides good generalization. This complicates the interpretation of overfitting peaks as separating a "classical" from a "modern" regime. Data interpolation itself cannot guarantee good generalization: we need to study the interpolation with different penalty terms. We present analytical formulae for GE curves for MiSpaR with \$l\_2\$ and \$l\_1\$ penalties, in the interpolating limit \${\textbackslash}lambda{\textbackslash}rightarrow 0\$.These risk curves exhibit important differences and help elucidate the underlying phenomena.},
  archiveprefix = {arXiv},
  keywords = {{Physics - Data Analysis, Statistics and Probability},Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NVJ6BM7P/Mitra - 2019 - Understanding overfitting peaks in generalization .pdf;/Users/antoniohortaribeiro/Zotero/storage/KLQ9FX2N/1906.html}
}

@inproceedings{mnih_recurrent_2014,
  title = {Recurrent Models of Visual Attention},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex},
  year = {2014},
  pages = {2204--2212},
  file = {/Users/antoniohortaribeiro/Zotero/storage/M75XR7UI/mnih_recurrent_2014.pdf}
}

@article{mohamed_acoustic_2012,
  title = {Acoustic Modeling Using Deep Belief Networks},
  author = {Mohamed, Abdel-rahman and Dahl, George E. and Hinton, Geoffrey},
  year = {2012},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {20},
  number = {1},
  pages = {14--22},
  doi = {10.1109/TASL.2011.2109382},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3GBCDVBK/mohamed_acoustic_2012.pdf}
}

@book{mohan_power_1995,
  title = {Power Electronics},
  author = {Mohan, Ned and Mohan, Tore M},
  year = {1995},
  volume = {3},
  publisher = {John wiley \& sons New York},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JZ3ATI7C/mohan_power_1995.pdf;/Users/antoniohortaribeiro/Zotero/storage/MNXDXNBI/mohan_power_1995.pdf}
}

@article{mokus_bayesian_1974,
  title = {On {{Bayesian Methods}} for {{Seeking}} the {{Extremum}}},
  author = {Mokus, J},
  year = {1974},
  journal = {Optimization Techniques},
  pages = {400--404},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3V6YAWEC/Mokus - ON BAYESIAN I~ETHODS FOR SEEKING THE EXTREMUM.pdf}
}

@article{morari_model_2002,
  title = {Model Predictive Control},
  author = {Morari, Manfred and Lee, Jay H. and Garcia, C. and Prett, D. M.},
  year = {2002},
  journal = {Preprint},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CW72GJKJ/morari_model_2002.pdf}
}

@article{morcos_one_2019,
  title = {One Ticket to Win Them All: Generalizing Lottery Ticket Initializations across Datasets and Optimizers},
  shorttitle = {One Ticket to Win Them All},
  author = {Morcos, Ari S. and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
  year = {2019},
  month = oct,
  journal = {arXiv:1906.02773 [cs, stat]},
  eprint = {1906.02773},
  primaryclass = {cs, stat},
  urldate = {2020-07-05},
  abstract = {The success of lottery ticket initializations (Frankle and Carbin, 2019) suggests that small, sparsified networks can be trained so long as the network is initialized appropriately. Unfortunately, finding these "winning ticket" initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training configuration (optimizer and dataset) and evaluating their performance on another configuration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations generated by sufficiently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2AMMMUYJ/Morcos et al. - 2019 - One ticket to win them all generalizing lottery t.pdf;/Users/antoniohortaribeiro/Zotero/storage/8HYYRLDH/1906.html}
}

@article{more_computing_1983,
  title = {Computing a Trust Region Step},
  author = {Mor{\'e}, Jorge J and Sorensen, Danny C},
  year = {1983},
  journal = {SIAM Journal on Scientific and Statistical Computing},
  volume = {4},
  number = {3},
  pages = {553--572},
  doi = {10.1137/0904038}
}

@incollection{more_levenbergmarquardt_1978,
  title = {The {{Levenberg-Marquardt}} Algorithm: Implementation and Theory},
  booktitle = {Numerical {{Analysis}}},
  author = {Mor{\'e}, Jorge J},
  year = {1978},
  pages = {105--116},
  publisher = {Springer}
}

@article{morris_what_2011,
  title = {What Is {{Hysteresis}}?},
  author = {Morris, K. A.},
  year = {2011},
  month = sep,
  journal = {Applied Mechanics Reviews},
  volume = {64},
  number = {5},
  issn = {0003-6900},
  doi = {10.1115/1.4007112},
  urldate = {2019-11-13},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3BY9CDIA/Morris - 2011 - What is Hysteresis.pdf;/Users/antoniohortaribeiro/Zotero/storage/GMACNAR7/What-is-Hysteresis.html}
}

@article{muehlematter_fdacleared_2023,
  title = {{{FDA-cleared}} Artificial Intelligence and Machine Learning-Based Medical Devices and Their 510(k) Predicate Networks},
  author = {Muehlematter, Urs J. and Bluethgen, Christian and Vokinger, Kerstin N.},
  year = {2023},
  month = sep,
  journal = {The Lancet Digital Health},
  volume = {5},
  number = {9},
  pages = {e618-e626},
  publisher = {Elsevier},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(23)00126-7},
  urldate = {2024-08-19},
  langid = {english},
  pmid = {37625896},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZMLSFKFG/Muehlematter et al. - 2023 - FDA-cleared artificial intelligence and machine le.pdf}
}

@book{munkres_analysis_1997,
  title = {Analysis {{On Manifolds}}},
  author = {Munkres, J.R.},
  year = {1997},
  series = {Advanced {{Books Classics}}},
  publisher = {Avalon Publishing},
  isbn = {978-0-8133-4548-2},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EA58A557/munkres_analysis_1997.pdf}
}

@book{munkres_topology_2000,
  title = {Topology},
  author = {Munkres, J.R.},
  year = {2000},
  series = {Featured {{Titles}} for {{Topology Series}}},
  publisher = {Prentice Hall, Incorporated},
  isbn = {978-0-13-181629-9},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JTPZ8A3T/munkres_topology_2000.pdf}
}

@inproceedings{murphy_lowcost_2007,
  title = {Low-Cost Stereo Vision on an {{FPGA}}},
  booktitle = {Field-{{Programmable Custom Computing Machines}}, 2007. {{FCCM}} 2007. 15th {{Annual IEEE Symposium}} On},
  author = {Murphy, Chris and Lindquist, Daniel and Rynning, Ann Marie and Cecil, Thomas and Leavitt, Sarah and Chang, Mark L},
  year = {2007},
  pages = {333--334},
  publisher = {IEEE}
}

@book{murphy_machine_2012,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2012},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  isbn = {978-0-262-01802-9},
  lccn = {Q325.5 .M87 2012},
  keywords = {machine learning,Probabilities},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9MA6ZJNZ/murphy_machine_2012.pdf}
}

@misc{muthukumar_classification_2021,
  title = {Classification vs Regression in Overparameterized Regimes: {{Does}} the Loss Function Matter?},
  shorttitle = {Classification vs Regression in Overparameterized Regimes},
  author = {Muthukumar, Vidya and Narang, Adhyyan and Subramanian, Vignesh and Belkin, Mikhail and Hsu, Daniel and Sahai, Anant},
  year = {2021},
  month = oct,
  number = {arXiv:2005.08054},
  eprint = {2005.08054},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.08054},
  urldate = {2022-11-23},
  abstract = {We compare classification and regression tasks in an overparameterized linear model with Gaussian features. On the one hand, we show that with sufficient overparameterization all training points are support vectors: solutions obtained by least-squares minimum-norm interpolation, typically used for regression, are identical to those produced by the hard-margin support vector machine (SVM) that minimizes the hinge loss, typically used for training classifiers. On the other hand, we show that there exist regimes where these interpolating solutions generalize well when evaluated by the 0-1 test loss function, but do not generalize if evaluated by the square loss function, i.e. they approach the null risk. Our results demonstrate the very different roles and properties of loss functions used at the training phase (optimization) and the testing phase (generalization).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BSCEAITM/Muthukumar et al. - 2021 - Classification vs regression in overparameterized .pdf;/Users/antoniohortaribeiro/Zotero/storage/TA9A5XK7/2005.html}
}

@article{muthukumar_harmless_2020,
  title = {Harmless {{Interpolation}} of {{Noisy Data}} in {{Regression}}},
  author = {Muthukumar, Vidya and Vodrahalli, Kailas and Subramanian, Vignesh and Sahai, Anant},
  year = {2020},
  month = may,
  journal = {IEEE Journal on Selected Areas in Information Theory},
  volume = {1},
  number = {1},
  pages = {67--83},
  issn = {2641-8770},
  doi = {10.1109/JSAIT.2020.2984716},
  abstract = {A continuing mystery in understanding the empirical success of deep neural networks is their ability to achieve zero training error and generalize well, even when the training data is noisy and there are more parameters than data points. We investigate this overparameterized regime in linear regression, where all solutions that minimize training error interpolate the data, including noise. We lower-bound the fundamental generalization (mean-squared) error of any interpolating solution in the presence of noise, and show that this bound decays to zero with the number of features. Thus, overparameterization can be beneficial in ensuring harmless interpolation of noise. We discuss two root causes for poor generalization that are complementary in nature - signal ``bleeding'' into a large number of alias features, and overfitting of noise by parsimonious feature selectors. For the sparse linear model with noise, we provide a hybrid interpolating scheme that mitigates both these issues and achieves order-optimal MSE over all possible interpolating solutions.},
  keywords = {function approximation,Information theory,interpolation,Interpolation,Kernel,Linear regression,Neural networks,Optimization,Statistical learning,supervised learning,Training data},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VUG5S5BP/Muthukumar et al. - 2020 - Harmless Interpolation of Noisy Data in Regression.pdf;/Users/antoniohortaribeiro/Zotero/storage/XL4SPBNA/9051968.html}
}

@book{mutto_timeofflight_2012,
  title = {Time-of-Flight Cameras and Microsoft Kinect ({{TM}})},
  author = {Mutto, Carlo Dal and Zanuttigh, Pietro and Cortelazzo, Guido M},
  year = {2012},
  publisher = {Springer Publishing Company, Incorporated}
}

@incollection{nagarajan_uniform_2019,
  title = {Uniform Convergence May Be Unable to Explain Generalization in Deep Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Nagarajan, Vaishnavh and Kolter, J. Zico},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {11611--11622},
  publisher = {Curran Associates, Inc.},
  urldate = {2019-12-30},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ELLNE7HK/Nagarajan and Kolter - 2019 - Uniform convergence may be unable to explain gener.pdf;/Users/antoniohortaribeiro/Zotero/storage/KPRVPLSV/Nagarajan and Kolter - 2019 - Uniform convergence may be unable to explain gener.pdf;/Users/antoniohortaribeiro/Zotero/storage/L5Z9JXKY/9336-uniform-convergence-may-be-unable-to-explain-generalization-in-deep-learning.html}
}

@inproceedings{nakkiran_deep_2020,
  title = {Deep {{Double Descent}}: {{Where Bigger Models}} and {{More Data Hurt}}},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  year = {2020},
  eprint = {1912.02292},
  abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZX3EWWQA/Nakkiran et al_2019_Deep Double Descent.pdf;/Users/antoniohortaribeiro/Zotero/storage/WIE6BLGP/1912.html}
}

@article{nar_step_2018,
  title = {Step {{Size Matters}} in {{Deep Learning}}},
  author = {Nar, Kamil and Sastry, S. Shankar},
  year = {2018},
  month = may,
  journal = {arXiv:1805.08890 [cs, math, stat]},
  eprint = {1805.08890},
  primaryclass = {cs, math, stat},
  urldate = {2018-12-10},
  abstract = {Training a neural network with the gradient descent algorithm gives rise to a discrete-time nonlinear dynamical system. Consequently, behaviors that are typically observed in these systems emerge during training, such as convergence to an orbit but not to a fixed point or dependence of convergence on the initialization. Step size of the algorithm plays a critical role in these behaviors: it determines the subset of the local optima that the algorithm can converge to, and it specifies the magnitude of the oscillations if the algorithm converges to an orbit. To elucidate the effects of the step size on training of neural networks, we study the gradient descent algorithm as a discrete-time dynamical system, and by analyzing the Lyapunov stability of different solutions, we show the relationship between the step size of the algorithm and the solutions that can be obtained with this algorithm. The results provide an explanation for several phenomena observed in practice, including the deterioration in the training error with increased depth, the hardness of estimating linear mappings with large singular values, and the distinct performance of deep residual networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/H2D78PII/nar_step size_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/S58YPWG4/1805.html}
}

@incollection{nar_step_2018a,
  title = {Step {{Size Matters}} in {{Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Nar, Kamil and Sastry, Shankar},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {3439--3447},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-04},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PSUT2GM3/nar_step size_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/GKNU85M3/7603-step-size-matters-in-deep-learning.html}
}

@article{nardi_autoregressive_2011,
  title = {Autoregressive Process Modeling via the {{Lasso}} Procedure},
  author = {Nardi, Y. and Rinaldo, A.},
  year = {2011},
  month = mar,
  journal = {Journal of Multivariate Analysis},
  volume = {102},
  number = {3},
  pages = {528--549},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2010.10.012},
  abstract = {The Lasso is a popular model selection and estimation procedure for linear models that enjoys nice theoretical properties. In this paper, we study the Lasso estimator for fitting autoregressive time series models. We adopt a double asymptotic framework where the maximal lag may increase with the sample size. We derive theoretical results establishing various types of consistency. In particular, we derive conditions under which the Lasso estimator for the autoregressive coefficients is model selection consistent, estimation consistent and prediction consistent. Simulation study results are reported.},
  keywords = {Autoregressive model,Estimation consistency,Lasso procedure,Model selection,Prediction consistency},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PIQIAWKB/nardi_autoregres_2011.pdf;/Users/antoniohortaribeiro/Zotero/storage/NCU2CNES/S0047259X10002186.html}
}

@article{narendra_identification_1990,
  title = {Identification and Control of Dynamical Systems Using Neural Networks},
  author = {Narendra, Kumpati S and Parthasarathy, Kannan},
  year = {1990},
  journal = {IEEE Transactions on Neural Networks},
  volume = {1},
  number = {1},
  pages = {4--27},
  doi = {10.1109/72.80202},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZZJME52M/narendra_identifica_1990.pdf}
}

@article{nascimento_implementing_2019,
  title = {Implementing Myocardial Infarction Systems of Care in Low/Middle-Income Countries},
  author = {Nascimento, Bruno R and Brant, Luisa C Caldeira and Marino, B{\'a}rbara C A and Passaglia, Luiz Guilherme and Ribeiro, Antonio Luiz P},
  year = {2019},
  month = jan,
  journal = {Heart},
  volume = {105},
  number = {1},
  pages = {20},
  doi = {10/gfchxp},
  abstract = {Ischaemic heart disease is the leading cause of death worldwide, with an increasing trend from 6.1\,million deaths in 1990 to 9.5\,million in 2016, markedly driven by rates observed in low/middle-income countries (LMIC). Improvements in myocardial infarction (MI) care are crucial for reducing premature mortality. We aimed to evaluate the main challenges for adequate MI care in LMIC, and possible strategies to overcome these existing barriers.Reperfusion is the cornerstone of MI treatment, but worldwide around 30\% of patients are not reperfused, with even lower rates in LMIC. The main challenges are related to delays associated with patient education, late diagnosis and inadequate referral strategies, health infrastructure and insufficient funding. The implementation of regional MI systems of care in LMIC, systematising timely reperfusion strategies, access to intensive care, risk stratification and use of adjunctive medications have shown some successful strategies. Telemedicine support for remote ECG, diagnosis and organisation of referrals has proven to be useful, improving access to reperfusion even in prehospital settings. Organisation of transport and referral hubs based on anticipated delays and development of MI excellence centres have also resulted in better equality of care. Also, education of healthcare staff and task shifting may potentially widen access to optimal therapy.In conclusion, efforts have been made for the implementation of MI systems of care in LMIC, aiming to address particularities of the health systems. However, the increasing impact of MI in these countries urges the development of further strategies to improve reperfusion and reduce system delays.}
}

@article{nash_newtontype_1984,
  title = {Newton-Type Minimization via the {{Lanczos}} Method},
  author = {Nash, Stephen G.},
  year = {1984},
  journal = {SIAM Journal on Numerical Analysis},
  volume = {21},
  number = {4},
  pages = {770--788},
  doi = {10.1137/0721052},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RP8WQSUX/nash_newton-typ_1984.pdf}
}

@article{nasrabadi_hopfield_1992,
  title = {Hopfield Network for Stereo Vision Correspondence},
  author = {Nasrabadi, Nasser M and Choo, Chang Y},
  year = {1992},
  journal = {Neural Networks, IEEE Transactions on},
  volume = {3},
  number = {1},
  pages = {5--13},
  doi = {10.1109/72.105413}
}

@article{naylorc_prospects_2018,
  title = {On the Prospects for a (Deep) Learning Health Care System},
  author = {{Naylor C}},
  year = {2018},
  month = sep,
  journal = {JAMA},
  volume = {320},
  number = {11},
  pages = {1099--1100},
  issn = {0098-7484},
  doi = {10.1001/jama.2018.11103},
  abstract = {In 1976, Maxmen1 predicted that artificial intelligence (AI) in the 21st century would usher in ``the post-physician era,'' with health care provided by paramedics and computers. Today, the mass extinction of physicians remains unlikely. However, as outlined by Hinton2 in a related Viewpoint, the emergence of a radically different approach to AI, called deep learning, has the potential to effect major changes in clinical medicine and health care delivery. This Viewpoint reviews some of the factors driving wide adoption of deep learning and other forms of machine learning in the health ecosystem.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/86UW9TH8/naylor c_on the_2018.pdf}
}

@book{neal_bayesian_1995,
  title = {Bayesian Learning for Neural Networks},
  author = {Neal, Radford M},
  year = {1995},
  volume = {118}
}

@article{neal_mcmc_2011,
  title = {{{MCMC}} Using {{Hamiltonian}} Dynamics},
  author = {Neal, Radford M.},
  year = {2011},
  journal = {Handbook of Markov Chain Monte Carlo},
  volume = {2},
  number = {11},
  pages = {2},
  doi = {10.1201/b10905-6},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NX9D2ABR/neal_mcmc_2011.pdf}
}

@inproceedings{neimark_cases_1959,
  title = {On Some Cases of Periodic Motions Depending on Parameters},
  booktitle = {Dokl. {{Akad}}. {{Nauk SSSR}}},
  author = {Neimark, Ju},
  year = {1959},
  volume = {129},
  pages = {736--739}
}

@article{nelder_simplex_1965,
  title = {A Simplex Method for Function Minimization},
  author = {Nelder, John A and Mead, Roger},
  year = {1965},
  journal = {The computer journal},
  volume = {7},
  number = {4},
  pages = {308--313},
  issn = {0010-4620},
  doi = {10.1093/comjnl/7.4.308},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RNIR64BM/nelder_a simplex_1965.pdf}
}

@book{nelles_nonlinear_2013,
  title = {Nonlinear System Identification: From Classical Approaches to Neural Networks and Fuzzy Models},
  author = {Nelles, Oliver},
  year = {2013},
  publisher = {Springer Science \& Business Media},
  isbn = {3-662-04323-8},
  file = {/Users/antoniohortaribeiro/Zotero/storage/E2A39IS9/nelles_nonlinear_2013.pdf}
}

@book{nelles_nonlinear_2013a,
  title = {Nonlinear {{System Identification}}: {{From Classical Approaches}} to {{Neural Networks}} and {{Fuzzy Models}}},
  author = {Nelles, Oliver},
  year = {2013},
  publisher = {Springer Science \& Business Media},
  isbn = {3-662-04323-8}
}

@book{nesterov_introductory_1998,
  title = {Introductory {{Lectures On Convex Programming}}},
  author = {Nesterov, Yu},
  year = {1998},
  publisher = {Springer Science \& Business Media},
  abstract = {1.1.1 General formulation of the problem................... 9},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AHQISIVA/nesterov_introducto_1998.pdf;/Users/antoniohortaribeiro/Zotero/storage/3PFSTBF2/summary.html}
}

@incollection{newey_large_1994,
  title = {Large Sample Estimation and Hypothesis Testing},
  booktitle = {Of {{Handbook}} of {{Econometrics}}},
  author = {Newey, Whitney K. and Mcfadden, Daniel},
  year = {1994},
  pages = {2111},
  abstract = {null},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FIJTII4R/newey_large_1994.pdf;/Users/antoniohortaribeiro/Zotero/storage/X8FXVANZ/summary.html}
}

@article{newey_large_1994a,
  title = {Large Sample Estimation and Hypothesis Testing},
  author = {Newey, Whitney K. and McFadden, Daniel},
  year = {1994},
  journal = {Handbook of econometrics},
  volume = {4},
  pages = {2111--2245},
  doi = {10.1016/S1573-4412(05)80005-4},
  file = {/Users/antoniohortaribeiro/Zotero/storage/G72R9XUM/newey_large_1994.pdf}
}

@misc{ng_cs230_2018,
  title = {{{CS230 Deep Learning}} ({{Stanford}})},
  author = {Ng, Andrew Y. and Katanforoosh, Kian},
  year = {2018},
  month = jan,
  journal = {CS230 Deep Learning},
  urldate = {2019-07-18},
  abstract = {Short tutorial detailing the best practices to split your dataset into train, dev and test sets},
  howpublished = {https://cs230-stanford.github.io/train-dev-test-split.html},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IYNZ4TUX/train-dev-test-split.html}
}

@inproceedings{ng_discriminative_2002,
  title = {On Discriminative vs. Generative Classifiers: {{A}} Comparison of Logistic Regression and Naive Bayes},
  shorttitle = {On Discriminative vs. Generative Classifiers},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ng, Andrew Y. and Jordan, Michael I.},
  year = {2002},
  pages = {841--848},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6NPCW875/ng_on_2002.pdf}
}

@misc{ng_practical_,
  title = {{Practical aspects of Deep Learning (Coursera)}},
  author = {Ng, Andrew Y.},
  journal = {Coursera},
  urldate = {2019-07-18},
  abstract = {Learn online and earn valuable credentials from top universities like Yale, Michigan, Stanford, and leading companies like Google and IBM. Join Coursera for free and transform your career with degrees, certificates, Specializations, \& MOOCs in data science, computer science, business, and dozens of other topics.},
  howpublished = {https://www.coursera.org/lecture/deep-neural-network/train-dev-test-sets-cxG1s},
  langid = {portuguese},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VTNPJPQ8/train-dev-test-sets-cxG1s.html}
}

@inproceedings{NIPS2014_ede7e2b6,
  title = {{{SAGA}}: {{A}} Fast Incremental Gradient Method with Support for Non-Strongly Convex Composite Objectives},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Defazio, Aaron and Bach, Francis and {Lacoste-Julien}, Simon},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K.Q.},
  year = {2014},
  volume = {27},
  publisher = {Curran Associates, Inc.}
}

@article{nocedal_interior_2014,
  title = {An {{Interior Point Method}} for {{Nonlinear Programming}} with {{Infeasibility Detection Capabilities}}},
  author = {Nocedal, Jorge and {\"O}ztoprak, Figen and Waltz, Richard A},
  year = {2014},
  journal = {Optimization Methods and Software},
  volume = {29},
  number = {4},
  pages = {837--854},
  issn = {1055-6788},
  doi = {10/gfjwmn}
}

@article{nocedal_interior_2014a,
  title = {An Interior Point Method for Nonlinear Programming with Infeasibility Detection Capabilities},
  author = {Nocedal, Jorge and {\"O}ztoprak, Figen and Waltz, Richard A},
  year = {2014},
  journal = {Optimization Methods and Software},
  volume = {29},
  number = {4},
  pages = {837--854},
  issn = {1055-6788},
  doi = {10.1080/10556788.2013.858156},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IQJ7CUED/nocedal_an_2014.pdf}
}

@book{nocedal_numerical_2006,
  title = {Numerical {{Optimization}}},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer Series in Operations Research},
  edition = {2nd ed},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-30303-1},
  lccn = {QA402.5 .N62 2006},
  keywords = {Mathematical optimization},
  annotation = {OCLC: ocm68629100},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5943D94U/nocedal_numerical_2006.pdf;/Users/antoniohortaribeiro/Zotero/storage/S8V87NMT/nocedal_numerical_2006.pdf}
}

@article{noel_f16_,
  title = {F-16 Aircraft Benchmark Based on Ground Vibration Test Data},
  author = {Noel, J P and Schoukens, M},
  pages = {5},
  langid = {english},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VKAEGX6N/Noel and Schoukens - F-16 aircraft benchmark based on ground vibration .pdf}
}

@article{noel_greybox_2017,
  title = {Grey-Box State-Space Identification of Nonlinear Mechanical Vibrations},
  author = {No{\"e}l, Jean-Philippe and Schoukens, Johan},
  year = {2017},
  journal = {International Journal of Control},
  pages = {1--22},
  issn = {0020-7179},
  keywords = {ðŸ”No DOI found}
}

@article{noel_greybox_2018,
  title = {Grey-Box State-Space Identification of Nonlinear Mechanical Vibrations},
  author = {No{\"e}l, J. P. and Schoukens, J.},
  year = {2018},
  month = may,
  journal = {International Journal of Control},
  volume = {91},
  number = {5},
  pages = {1118--1139},
  issn = {0020-7179},
  doi = {10/gfzgtg},
  urldate = {2019-04-15},
  abstract = {The present paper deals with the identification of nonlinear mechanical vibrations. A grey-box, or semi-physical, nonlinear state-space representation is introduced, expressing the nonlinear basis functions using a limited number of measured output variables. This representation assumes that the observed nonlinearities are localised in physical space, which is a generic case in mechanics. A two-step identification procedure is derived for the grey-box model parameters, integrating nonlinear subspace initialisation and weighted least-squares optimisation. The complete procedure is applied to an electrical circuit mimicking the behaviour of a single--input, single--output (SISO) nonlinear mechanical system and to a single--input, multiple--output (SIMO) geometrically nonlinear beam structure.},
  keywords = {grey-box modelling,nonlinear beam benchmark,nonlinear mechanical vibrations,Nonlinear system identification,semi-physical modelling,Silverbox benchmark,state-space equations},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MZMVN5YY/noÃ«l_grey-box_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/BWKA4ER8/00207179.2017.html}
}

@article{noel_hysteretic_2016,
  title = {Hysteretic Benchmark with a Dynamic Nonlinearity},
  author = {No{\"e}l, J. P. and Schoukens, M.},
  year = {2016},
  journal = {Brussels, Belgium},
  urldate = {2017-08-28},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/B28R2S2Z/noÃ«l_hysteretic_2016.pdf}
}

@article{noel_nonlinear_2017,
  title = {A Nonlinear State-Space Approach to Hysteresis Identification},
  author = {No{\"e}l, Jean-Philippe and Esfahani, A Fakhrizadeh and Kerschen, Gaetan and Schoukens, Johan},
  year = {2017},
  journal = {Mechanical Systems and Signal Processing},
  volume = {84},
  pages = {171--184},
  issn = {0888-3270},
  doi = {10.1016/j.ymssp.2016.08.025}
}

@article{noel_nonlinear_2017a,
  title = {Nonlinear System Identification in Structural Dynamics: 10 More Years of Progress},
  shorttitle = {Nonlinear System Identification in Structural Dynamics},
  author = {No{\"e}l, J. P. and Kerschen, G.},
  year = {2017},
  month = jan,
  journal = {Mechanical Systems and Signal Processing},
  volume = {83},
  pages = {2--35},
  issn = {0888-3270},
  doi = {10.1016/j.ymssp.2016.07.020},
  abstract = {Nonlinear system identification is a vast research field, today attracting a great deal of attention in the structural dynamics community. Ten years ago, an MSSP paper reviewing the progress achieved until then [1] concluded that the identification of simple continuous structures with localised nonlinearities was within reach. The past decade witnessed a shift in emphasis, accommodating the growing industrial need for a first generation of tools capable of addressing complex nonlinearities in larger-scale structures. The objective of the present paper is to survey the key developments which arose in the field since 2006, and to illustrate state-of-the-art techniques using a real-world satellite structure. Finally, a broader perspective to nonlinear system identification is provided by discussing the central role played by experimental models in the design cycle of engineering structures.},
  keywords = {Nonlinear system identification,Review,Structural dynamics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QF7QNRGF/noÃ«l_nonlinear_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/3QQRCAQ9/S088832701630245X.html;/Users/antoniohortaribeiro/Zotero/storage/N5KSIG7B/S088832701630245X.html}
}

@book{norgaard_neural_2000,
  title = {Neural {{Networks}} for {{Modelling}} and {{Control}} of {{Dynamic Systems-A Practitioner}}'s {{Handbook}}},
  author = {N{\o}rgaard, Peter Magnus and Ravn, Ole and Poulsen, Niels Kj{\o}lstad and Hansen, Lars Kai},
  year = {2000},
  publisher = {Springer-London}
}

@book{normey-rico_control_2007,
  title = {Control of Dead-Time Processes},
  author = {{Normey-Rico}, J. E. and Camacho, E. F.},
  year = {2007},
  series = {Advanced Textbooks in Control and Signal Processing},
  publisher = {Springer},
  address = {London},
  isbn = {978-1-84628-828-9 978-1-84628-829-6},
  lccn = {TJ213 .N58 2007},
  keywords = {Automatic control},
  annotation = {OCLC: ocm82672359},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6FQNMDJH/normey-rico_control_2007.pdf}
}

@book{norton_introduction_2009,
  title = {An {{Introduction}} to {{Identification}}},
  author = {Norton, John P},
  year = {2009},
  publisher = {Courier Corporation}
}

@article{noseworthy_artificial_2022,
  title = {Artificial Intelligence-Guided Screening for Atrial Fibrillation Using Electrocardiogram during Sinus Rhythm: A Prospective Non-Randomised Interventional Trial},
  author = {Noseworthy, Peter A and Attia, Zachi I and Behnken, Emma M and Giblon, Rachel E and Bews, Katherine A and Liu, Sijia and Gosse, Tara A and Linn, Zachery D and Deng, Yihong and Yin, Jun and Gersh, Bernard J and {Graff-Radford}, Jonathan and Rabinstein, Alejandro A and Siontis, Konstantinos C and Friedman, Paul A and Yao, Xiaoxi},
  year = {2022},
  journal = {The Lancet},
  volume = {400},
  number = {10359},
  pages = {1206--1212},
  issn = {0140-6736},
  doi = {10.1016/S0140-6736(22)01637-3},
  abstract = {Summary Background Previous atrial fibrillation screening trials have highlighted the need for more targeted approaches. We did a pragmatic study to evaluate the effectiveness of an artificial intelligence (AI) algorithm-guided targeted screening approach for identifying previously unrecognised atrial fibrillation. Methods For this non-randomised interventional trial, we prospectively recruited patients with stroke risk factors but with no known atrial fibrillation who had an electrocardiogram (ECG) done in routine practice. Participants wore a continuous ambulatory heart rhythm monitor for up to 30 days, with the data transmitted in near real time through a cellular connection. The AI algorithm was applied to the ECGs to divide patients into high-risk or low-risk groups. The primary outcome was newly diagnosed atrial fibrillation. In a secondary analysis, trial participants were propensity-score matched (1:1) to individuals from the eligible but unenrolled population who served as real-world controls. This study is registered with ClinicalTrials.gov, NCT04208971. Findings 1003 patients with a mean age of 74 years (SD 8{$\cdot$}8) from 40 US states completed the study. Over a mean 22{$\cdot$}3 days of continuous monitoring, atrial fibrillation was detected in six (1{$\cdot$}6\%) of 370 patients with low risk and 48 (7{$\cdot$}6\%) of 633 with high risk (odds ratio 4{$\cdot$}98, 95\% CI 2{$\cdot$}11--11{$\cdot$}75, p=0{$\cdot$}0002). Compared with usual care, AI-guided screening was associated with increased detection of atrial fibrillation (high-risk group: 3{$\cdot$}6\% [95\% CI 2{$\cdot$}3--5{$\cdot$}4] with usual care vs 10{$\cdot$}6\% [8{$\cdot$}3--13{$\cdot$}2] with AI-guided screening, p{\textexclamdown}0{$\cdot$}0001; low-risk group: 0{$\cdot$}9\% vs 2{$\cdot$}4\%, p=0{$\cdot$}12) over a median follow-up of 9{$\cdot$}9 months (IQR 7{$\cdot$}1--11{$\cdot$}0). Interpretation An AI-guided targeted screening approach that leverages existing clinical data increased the yield for atrial fibrillation detection and could improve the effectiveness of atrial fibrillation screening. Funding Mayo Clinic Robert D and Patricia E Kern Center for the Science of Health Care Delivery.}
}

@article{novak_bayesian_2018,
  title = {Bayesian {{Convolutional Neural Networks}} with {{Many Channels}} Are {{Gaussian Processes}}},
  author = {Novak, Roman and Xiao, Lechao and Lee, Jaehoon and Bahri, Yasaman and Abolafia, Daniel A. and Pennington, Jeffrey and {Sohl-Dickstein}, Jascha},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.05148 [cs, stat]},
  eprint = {1810.05148},
  primaryclass = {cs, stat},
  urldate = {2018-10-24},
  abstract = {There is a previously identified equivalence between wide fully connected neural networks (FCNs) and Gaussian processes (GPs). This equivalence enables, for instance, test set predictions that would have resulted from a fully Bayesian, infinitely wide trained FCN to be computed without ever instantiating the FCN, but by instead evaluating the corresponding GP. In this work, we derive an analogous equivalence for multi-layer convolutional neural networks (CNNs) both with and without pooling layers, and achieve state of the art results on CIFAR10 for GPs without trainable kernels. We also introduce a Monte Carlo method to estimate the GP corresponding to a given neural network architecture, even in cases where the analytic form has too many terms to be computationally feasible. Surprisingly, in the absence of pooling layers, the GPs corresponding to CNNs with and without weight sharing are identical. As a consequence, translation equivariance in finite-channel CNNs trained with stochastic gradient descent (SGD) has no corresponding property in the Bayesian treatment of the infinite channel limit - a qualitative difference between the two regimes that is not present in the FCN case. We confirm experimentally, that while in some scenarios the performance of SGD-trained finite CNNs approaches that of the corresponding GPs as the channel count increases, with careful tuning SGD-trained CNNs can significantly outperform their corresponding GPs, suggesting advantages from SGD training compared to fully Bayesian parameter estimation.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RRK43PJ5/novak_bayesian_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/R6NJYL5J/1810.html}
}

@misc{novak_fast_2022,
  title = {Fast {{Finite Width Neural Tangent Kernel}}},
  author = {Novak, Roman and {Sohl-Dickstein}, Jascha and Schoenholz, Samuel S.},
  year = {2022},
  month = jun,
  number = {arXiv:2206.08720},
  eprint = {2206.08720},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-07-26},
  abstract = {The Neural Tangent Kernel (NTK), defined as \${\textbackslash}Theta\_{\textbackslash}theta{\textasciicircum}f(x\_1, x\_2) = {\textbackslash}left[{\textbackslash}partial f({\textbackslash}theta, x\_1){\textbackslash}big/{\textbackslash}partial {\textbackslash}theta{\textbackslash}right] {\textbackslash}left[{\textbackslash}partial f({\textbackslash}theta, x\_2){\textbackslash}big/{\textbackslash}partial {\textbackslash}theta{\textbackslash}right]{\textasciicircum}T\$ where \${\textbackslash}left[{\textbackslash}partial f({\textbackslash}theta, {\textbackslash}cdot){\textbackslash}big/{\textbackslash}partial {\textbackslash}theta{\textbackslash}right]\$ is a neural network (NN) Jacobian, has emerged as a central object of study in deep learning. In the infinite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and generalization of NN architectures. At finite widths, the NTK is also used to better initialize NNs, compare the conditioning across models, perform architecture search, and do meta-learning. Unfortunately, the finite width NTK is notoriously expensive to compute, which severely limits its practical utility. We perform the first in-depth analysis of the compute and memory requirements for NTK computation in finite width networks. Leveraging the structure of neural networks, we further propose two novel algorithms that change the exponent of the compute and memory requirements of the finite width NTK, dramatically improving efficiency. Our algorithms can be applied in a black box fashion to any differentiable function, including those implementing neural networks. We open-source our implementations within the Neural Tangents package (arXiv:1912.02803) at https://github.com/google/neural-tangents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2EZ76DWU/Novak et al. - 2022 - Fast Finite Width Neural Tangent Kernel.pdf;/Users/antoniohortaribeiro/Zotero/storage/46Y8P48G/2206.html}
}

@article{novak_neural_2019,
  title = {Neural {{Tangents}}: {{Fast}} and {{Easy Infinite Neural Networks}} in {{Python}}},
  shorttitle = {Neural {{Tangents}}},
  author = {Novak, Roman and Xiao, Lechao and Hron, Jiri and Lee, Jaehoon and Alemi, Alexander A. and {Sohl-Dickstein}, Jascha and Schoenholz, Samuel S.},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.02803 [cs, stat]},
  eprint = {1912.02803},
  primaryclass = {cs, stat},
  urldate = {2021-11-22},
  abstract = {Neural Tangents is a library designed to enable research into infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space. The entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. Neural Tangents is available at www.github.com/google/neural-tangents. We also provide an accompanying interactive Colab notebook.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SFAN8K94/Novak et al_2019_Neural Tangents.pdf;/Users/antoniohortaribeiro/Zotero/storage/PDDSS75L/1912.html}
}

@article{novak_nonlinear_2010,
  title = {Nonlinear {{System Identification Using Exponential Swept-Sine Signal}}},
  author = {Novak, A. and Simon, L. and Kadlec, F. and Lotton, P.},
  year = {2010},
  month = aug,
  journal = {IEEE Transactions on Instrumentation and Measurement},
  volume = {59},
  number = {8},
  pages = {2220--2229},
  issn = {0018-9456},
  doi = {10.1109/TIM.2009.2031836},
  abstract = {In this paper, we propose a method for nonlinear system (NLS) identification using a swept-sine input signal and based on nonlinear convolution. The method uses a nonlinear model, namely, the nonparametric generalized polynomial Hammerstein model made of power series associated with linear filters. Simulation results show that the method identifies the nonlinear model of the system under test and estimates the linear filters of the unknown NLS. The method has also been tested on a real-world system: an audio limiter. Once the nonlinear model of the limiter is identified, a test signal can be regenerated to compare the outputs of both the real-world system and its nonlinear model. The results show good agreement between both model-based and real-world system outputs.},
  keywords = {Analysis,exponential swept sine signal,filtering theory,generalized polynomial Hammerstein model,identification,linear filter,nonlinear convolution,nonlinear filters,nonlinear system (NLS),Nonlinear system identification,nonparametric generalized polynomial Hammerstein model,polynomials,power series,series (mathematics),swept-sine},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DEVVF6NA/novak_nonlinear_2010.pdf;/Users/antoniohortaribeiro/Zotero/storage/6B5UGZZL/5299278.html;/Users/antoniohortaribeiro/Zotero/storage/UWG48CR9/5299278.html}
}

@article{nowak_fused_2011,
  title = {A Fused Lasso Latent Feature Model for Analyzing Multi-Sample {{aCGH}} Data},
  author = {Nowak, Gen and Hastie, Trevor and Pollack, Jonathan R. and Tibshirani, Robert},
  year = {2011},
  month = oct,
  journal = {Biostatistics},
  volume = {12},
  number = {4},
  pages = {776--791},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxr012},
  urldate = {2017-09-18},
  abstract = {Array-based comparative genomic hybridization (aCGH) enables the measurement of DNA copy number across thousands of locations in a genome. The main goals of analyzing aCGH data are to identify the regions of copy number variation (CNV) and to quantify the amount of CNV. Although there are many methods for analyzing single-sample aCGH data, the analysis of multi-sample aCGH data is a relatively new area of research. Further, many of the current approaches for analyzing multi-sample aCGH data do not appropriately utilize the additional information present in the multiple samples. We propose a procedure called the Fused Lasso Latent Feature Model (FLLat) that provides a statistical framework for modeling multi-sample aCGH data and identifying regions of CNV. The procedure involves modeling each sample of aCGH data as a weighted sum of a fixed number of features. Regions of CNV are then identified through an application of the fused lasso penalty to each feature. Some simulation analyses show that FLLat outperforms single-sample methods when the simulated samples share common information. We also propose a method for estimating the false discovery rate. An analysis of an aCGH data set obtained from human breast tumors, focusing on chromosomes 8 and 17, shows that FLLat and Significance Testing of Aberrant Copy number (an alternative, existing approach) identify similar regions of CNV that are consistent with previous findings. However, through the estimated features and their corresponding weights, FLLat is further able to discern specific relationships between the samples, for example, identifying 3 distinct groups of samples based on their patterns of CNV for chromosome 17.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZR2ZTICB/nowak_a fused_2011.pdf;/Users/antoniohortaribeiro/Zotero/storage/S7JWC8UT/249257.html}
}

@article{nunes_chagas_2013,
  title = {Chagas Disease: An Overview of Clinical and Epidemiological Aspects},
  shorttitle = {Chagas Disease},
  author = {Nunes, Maria Carmo Pereira and Dones, Wistremundo and Morillo, Carlos A. and Encina, Juan Justiniano and Ribeiro, Ant{\^o}nio Luiz and {Council on Chagas Disease of the Interamerican Society of Cardiology}},
  year = {2013},
  month = aug,
  journal = {Journal of the American College of Cardiology},
  volume = {62},
  number = {9},
  pages = {767--776},
  issn = {1558-3597},
  doi = {10.1016/j.jacc.2013.05.046},
  abstract = {Chagas disease, caused by the parasite Trypanosoma cruzi, is a serious health problem in Latin America and is an emerging disease in non-endemic countries. In recent decades, the epidemiological profile of the disease has changed due to new patterns of immigration and successful control in its transmission, leading to the urbanization and globalization of the disease. Dilated cardiomyopathy is the most important and severe manifestation of human chronic Chagas disease and is characterized by heart failure, ventricular arrhythmias, heart blocks, thromboembolic phenomena, and sudden death. This article will present an overview of the clinical and epidemiological aspects of Chagas disease. It will focus on several clinical aspects of the disease, such as chronic Chagas disease without detectable cardiac pathology, as well as dysautonomia, some specific features, and the principles of treatment of chronic cardiomyopathy.},
  langid = {english},
  pmid = {23770163},
  keywords = {{Death, Sudden, Cardiac},Chagas disease,Chagas Disease,ChD,dilated cardiomyopathy,E/e',early transmitral flow velocity to the early diastolic velocity of the mitral annulus,ECG,electrocardiogram,heart failure,Humans,ICD,implantable cardioverter-defibrillator,left ventricle/ventricular,LV,Prognosis,ventricular tachycardia,VT}
}

@article{nunes_incidence_2021,
  title = {Incidence and {{Predictors}} of {{Progression}} to {{Chagas Cardiomyopathy}}: {{Long-Term Follow-Up}} of {{Trypanosoma}} Cruzi-{{Seropositive Individuals}}},
  shorttitle = {Incidence and {{Predictors}} of {{Progression}} to {{Chagas Cardiomyopathy}}},
  author = {Nunes, Maria Carmo P. and Buss, Lewis F. and Silva, Jose Luiz P. and Martins, Larissa Natany A. and Oliveira, Claudia Di Lorenzo and Cardoso, Clareci Silva and Brito, Bruno Oliveira de Figueiredo and Ferreira, Ariela Mota and Oliveira, Lea Campos and Bierrenbach, Ana Luiza and Fernandes, Fabio and Busch, Michael P. and Hotta, Viviane Tiemi and Martinelli, Luiz Mario Baptista and Soeiro, Maria Carolina F. Almeida and Brentegani, Adriana and Salemi, Vera M. C. and Menezes, Marcia M. and Ribeiro, Antonio Luiz P. and Sabino, Ester Cerdeira},
  year = {2021},
  month = nov,
  journal = {Circulation},
  volume = {144},
  number = {19},
  pages = {1553--1566},
  issn = {1524-4539},
  doi = {10.1161/CIRCULATIONAHA.121.055112},
  abstract = {BACKGROUND: There are few contemporary cohorts of Trypanosoma cruzi-seropositive individuals, and the basic clinical epidemiology of Chagas disease is poorly understood. Herein, we report the incidence of cardiomyopathy and death associated with T. cruzi seropositivity. METHODS: Participants were selected in blood banks at 2 Brazilian centers. Cases were defined as T. cruzi-seropositive blood donors. T. cruzi-seronegative controls were matched for age, sex, and period of donation. Patients with established Chagas cardiomyopathy were recruited from a tertiary outpatient service. Participants underwent medical examination, blood collection, ECG, and echocardiogram at enrollment (2008-2010) and at follow-up (2018-2019). The primary outcomes were all-cause mortality and development of cardiomyopathy, defined as the presence of a left ventricular ejection fraction {$<$}50\% or QRS complex duration {$\geq$}120 ms, or both. To handle loss to follow-up, a sensitivity analysis was performed using inverse probability weights for selection. RESULTS: We enrolled 499 T. cruzi-seropositive donors (age 48{\textpm}10 years, 52\% male), 488 T. cruzi-seronegative donors (age 49{\textpm}10 years, 49\% male), and 101 patients with established Chagas cardiomyopathy (age 48{\textpm}8 years, 59\% male). The mortality in patients with established cardiomyopathy was 80.9 deaths/1000 person-years (py) (54/101, 53\%) and 15.1 deaths/1000 py (17/114, 15\%) in T. cruzi-seropositive donors with cardiomyopathy at baseline. Among T. cruzi-seropositive donors without cardiomyopathy at baseline, mortality was 3.7 events/1000 py (15/385, 4\%), which was no different from T. cruzi-seronegative donors with 3.6 deaths/1000 py (17/488, 3\%). The incidence of cardiomyopathy in T. cruzi-seropositive donors was 13.8 (95\% CI, 9.5-19.6) events/1000 py (32/262, 12\%) compared with 4.6 (95\% CI, 2.3-8.3) events/1000 py (11/277, 4\%) in seronegative controls, with an absolute incidence difference associated with T. cruzi seropositivity of 9.2 (95\% CI, 3.6-15.0) events/1000 py. T. cruzi antibody level at baseline was associated with development of cardiomyopathy (adjusted odds ratio, 1.4 [95\% CI, 1.1-1.8]). CONCLUSIONS: We present a comprehensive description of the natural history of T. cruzi seropositivity in a contemporary patient population. The results highlight the central importance of anti-T. cruzi antibody titer as a marker of Chagas disease activity and risk of progression.},
  langid = {english},
  pmcid = {PMC8578457},
  pmid = {34565171},
  keywords = {Chagas cardiomyopathy,Chagas Cardiomyopathy,Chagas disease,disease progression,Disease Progression,Female,Humans,Incidence,Male,Middle Aged,mortality,serology,Trypanosoma cruzi},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q3AFL7LM/Nunes et al. - 2021 - Incidence and Predictors of Progression to Chagas .pdf}
}

@article{nyquist_certain_1928,
  title = {Certain Topics in Telegraph Transmission Theory},
  author = {Nyquist, Harry},
  year = {1928},
  journal = {Transactions of the American Institute of Electrical Engineers},
  volume = {47},
  number = {2},
  pages = {617--644}
}

@book{ogata_modern_2010,
  title = {Modern Control Engineering},
  author = {Ogata, Katsuhiko},
  year = {2010},
  series = {Prentice-{{Hall}} Electrical Engineering Series. {{Instrumentation}} and Controls Series},
  edition = {5th ed},
  publisher = {Prentice-Hall},
  address = {Boston},
  isbn = {978-0-13-615673-4},
  lccn = {TJ213 .O28 2010},
  keywords = {Automatic control,Control theory},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZC257N7Q/ogata_modern_2010.pdf}
}

@phdthesis{oh_new_2004,
  title = {A New Quality Measure in Electrocardiogram Signal},
  author = {Oh, Sungho},
  year = {2004},
  school = {University of Florida},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PFCZIN8M/oh_a new_2004.pdf}
}

@article{okutomi_multiplebaseline_1993,
  title = {A Multiple-Baseline Stereo},
  author = {Okutomi, Masatoshi and Kanade, Takeo},
  year = {1993},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  volume = {15},
  number = {4},
  pages = {353--363},
  doi = {10.1109/34.206955}
}

@inproceedings{oliveira_explaining_2020,
  title = {Explaining Black-Box Automated Electrocardiogram Classification to Cardiologists},
  booktitle = {2020 {{Computing}} in {{Cardiology}} ({{CinC}})},
  author = {Oliveira, Derick M and Ribeiro, Antonio H and Pedrosa, Joao A O and Paixao, Gabriela M M and Ribeiro, Antonio L and Jr, Wagner Meira},
  year = {2020},
  volume = {47},
  doi = {10.22489/CinC.2020.452},
  abstract = {In this work, we present a method to explain ``end-toend'' electrocardiogram (ECG) signal classifiers, where the explanations were built along with seniors cardiologist to provide meaningful features to the final users. Our method focuses exclusively on automated ECG diagnosis and analyzes the explanation in terms of clinical accuracy for interpretability and robustness. The proposed method uses a noise-insertion strategy to quantify the impact of intervals and segments of the ECG signals on the automated classification outcome. An ECG segmentation method was applied to ECG tracings, to obtain: (1) Intervals, Segments and Axis; (2) Rate, and (3) Rhythm. Noise was added to the signal to disturb the ECG features in a realistic way. The method was tested using Monte Carlo simulation and the feature impact is estimated by the change in the model prediction averaged over 499 executions and a feature is defined as important if its mean value changes the result of the classifier. We demonstrate our method by explaining diagnoses generated by a deep convolutional neural network. The proposed method is particularly effective and useful for modern deep learning models that take raw data as input.},
  copyright = {All rights reserved},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KD6NL9CN/452_CinCFinalPDF.pdf}
}

@inproceedings{oliveira_explaining_2020a,
  title = {Explaining End-to-End {{ECG}} Automated Diagnosis Using Contextual Features},
  booktitle = {European {{Conference}} on {{Machine Learning}} and {{Principles}} and {{Practice}} of {{Knowledge Discovery}} in {{Databases}} ({{ECML-PKDD}})},
  author = {Oliveira, Derick M. and Ribeiro, Ant{\^o}nio H. and Pedrosa, Jo{\~a}o A. O. and Paixao, Gabriela M.M. and Ribeiro, Antonio Luiz P. and Meira Jr, Wagner},
  year = {2020},
  month = sep,
  volume = {12461},
  pages = {204--219},
  publisher = {Springer},
  address = {Ghent, Belgium},
  doi = {10.1007/978-3-030-67670-4_13}
}

@article{oliver_realistic_2018,
  title = {Realistic {{Evaluation}} of {{Deep Semi-Supervised Learning Algorithms}}},
  author = {Oliver, Avital and Odena, Augustus and Raffel, Colin and Cubuk, Ekin D. and Goodfellow, Ian J.},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.09170 [cs, stat]},
  eprint = {1804.09170},
  primaryclass = {cs, stat},
  urldate = {2018-12-06},
  abstract = {Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that these algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, that SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/997KF2E6/oliver_realistic_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/57EYAX6Q/1804.html}
}

@book{omnivision_ov9121_2003,
  title = {{{OV9121}} Datasheet},
  author = {{OmniVision}},
  year = {2003}
}

@article{oord_wavenet_2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  journal = {arXiv:1609.03499 [cs]},
  eprint = {1609.03499},
  primaryclass = {cs},
  urldate = {2019-03-13},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6C4AJW4G/oord_wavenet_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/WTZZGFWL/1609.html}
}

@book{oppenheim_discretetime_1999,
  title = {Discrete-Time Signal Processing},
  author = {Oppenheim, Alan V},
  year = {1999},
  publisher = {Pearson Education India},
  file = {/Users/antoniohortaribeiro/Zotero/storage/E9SNAQME/oppenheim_discrete-t_1999.pdf}
}

@book{orfanidis_introduction_1996,
  title = {Introduction to Signal Processing},
  author = {Orfanidis, Sophocles J.},
  year = {1996},
  series = {Prentice {{Hall}} Signal Processing Series},
  publisher = {Prentice Hall},
  address = {Englewood Cliffs, N.J},
  isbn = {978-0-13-209172-5},
  lccn = {TK5102.9 .O73 1996},
  keywords = {Digital techniques,signal processing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7AV86XUD/orfanidis_introducti_1996.pdf}
}

@article{orovic_compressive_2016,
  title = {Compressive {{Sensing}} in {{Signal Processing}}: {{Algorithms}} and {{Transform Domain Formulations}}},
  shorttitle = {Compressive {{Sensing}} in {{Signal Processing}}},
  author = {Orovi{\'c}, Irena and Papi{\'c}, Vladan and Ioana, Cornel and Li, Xiumei and Stankovi{\'c}, Srdjan},
  year = {2016},
  journal = {Mathematical Problems in Engineering},
  volume = {2016},
  pages = {1--16},
  issn = {1024-123X, 1563-5147},
  doi = {10.1155/2016/7616393},
  urldate = {2020-07-16},
  abstract = {Compressive sensing has emerged as an area that opens new perspectives in signal acquisition and processing. It appears as an alternative to the traditional sampling theory, endeavoring to reduce the required number of samples for successful signal reconstruction. In practice, compressive sensing aims to provide saving in sensing resources, transmission, and storage capacities and to facilitate signal processing in the circumstances when certain data are unavailable. To that end, compressive sensing relies on the mathematical algorithms solving the problem of data reconstruction from a greatly reduced number of measurements by exploring the properties of sparsity and incoherence. Therefore, this concept includes the optimization procedures aiming to provide the sparsest solution in a suitable representation domain. This work, therefore, offers a survey of the compressive sensing idea and prerequisites, together with the commonly used reconstruction methods. Moreover, the compressive sensing problem formulation is considered in signal processing applications assuming some of the commonly used transformation domains, namely, the Fourier transform domain, the polynomial Fourier transform domain, Hermite transform domain, and combined time-frequency domain.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Z8DPZUUF/OroviÄ‡ et al. - 2016 - Compressive Sensing in Signal Processing Algorith.pdf}
}

@inproceedings{osama_inferring_2019,
  title = {Inferring {{Heterogeneous Causal Effects}} in {{Presence}} of {{Spatial Confounding}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}},},
  author = {Osama, Muhammad and Zachariah, Dave and Sch{\"o}n, Thomas B},
  year = {2019},
  abstract = {We address the problem of inferring the causal effect of an exposure on an outcome across space, using observational data. The data is possibly subject to unmeasured confounding variables which, in a standard approach, must be adjusted for by estimating a nuisance function. Here we develop a method that eliminates the nuisance function, while mitigating the resulting errors-in-variables. The result is a robust and accurate inference method for spatially varying heterogeneous causal effects. The properties of the method are demonstrated on synthetic as well as real data from Germany and the US.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CU5UUFRF/Osama et al. - Inferring Heterogeneous Causal Effects in Presence.pdf}
}

@article{osborne_new_2000,
  title = {A New Approach to Variable Selection in Least Squares Problems},
  author = {Osborne, M. R. and Presnell, B. and Turlach, B. A.},
  year = {2000},
  month = jul,
  journal = {IMA Journal of Numerical Analysis},
  volume = {20},
  number = {3},
  pages = {389--403},
  issn = {0272-4979},
  doi = {10.1093/imanum/20.3.389},
  urldate = {2017-09-18},
  abstract = {The title Lasso has been suggested by Tibshirani (1996) as a colourful name for a technique of variable selection which requires the minimization of a sum of squares subject to an l1 bound {$\kappa$} on the solution. This forces zero components in the minimizing solution for small values of {$\kappa$}. Thus this bound can function as a selection parameter. This paper makes two contributions to computational problems associated with implementing the Lasso: (1) a compact descent method for solving the constrained problem for a particular value of {$\kappa$} is formulated, and (2) a homotopy method, in which the constraint bound {$\kappa$} becomes the homotopy parameter, is developed to completely describe the possible selection regimes. Both algorithms have a finite termination property. It is suggested that modified Gram-Schmidt orthogonalization applied to an augmented design matrix provides an effective basis for implementing the algorithms.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8GGCK6ST/osborne_a new_2000.pdf;/Users/antoniohortaribeiro/Zotero/storage/XI2WFMZB/osborne_a new_2000.pdf;/Users/antoniohortaribeiro/Zotero/storage/2ENKTF3J/A-new-approach-to-variable-selection-in-least.html;/Users/antoniohortaribeiro/Zotero/storage/7CADPP7T/A-new-approach-to-variable-selection-in-least.html}
}

@article{osborne_shooting_1969,
  title = {On {{Shooting Methods}} for {{Boundary Value Problems}}},
  author = {Osborne, Mike R},
  year = {1969},
  journal = {Journal of Mathematical Analysis and Applications},
  volume = {27},
  number = {2},
  pages = {417--433},
  doi = {10/cgt6pc}
}

@article{osborne_shooting_1969a,
  title = {On Shooting Methods for Boundary Value Problems},
  author = {Osborne, Mike R},
  year = {1969},
  journal = {Journal of Mathematical Analysis and Applications},
  volume = {27},
  number = {2},
  pages = {417--433},
  doi = {10.1016/0022-247X(69)90059-6},
  file = {/Users/antoniohortaribeiro/Zotero/storage/84FTZCVL/osborne_on_1969.pdf}
}

@book{osullivan_real_2010,
  title = {Real World {{Haskell}}: Code You Can Believe In},
  shorttitle = {Real World {{Haskell}}},
  author = {O'Sullivan, Bryan and Goerzen, John and Stewart, Donald Bruce},
  year = {2010},
  edition = {1. ed., [Nachdr.]},
  publisher = {O'Reilly},
  address = {Beijing},
  isbn = {978-0-596-51498-3},
  langid = {english},
  annotation = {OCLC: 837707964},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NSHIKNZA/o'sullivan_real_2010.pdf}
}

@inproceedings{ouyang_training_2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  booktitle = {Neural {{Information Processing Systems}} ({{NeurIPS}})},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.}
}

@inproceedings{overschee_closed_1997,
  title = {Closed Loop Subspace System Identification},
  booktitle = {Proceedings of the 36th {{IEEE Conference}} on {{Decision}} and {{Control}}},
  author = {Overschee, P. Van and Moor, B. De},
  year = {1997},
  month = dec,
  volume = {2},
  pages = {1848-1853 vol.2},
  doi = {10.1109/CDC.1997.657851},
  abstract = {We present a general framework for closed loop subspace system identification. This framework consists of two new projection theorems which allow the extraction of non-steady state Kalman filter states and of system related matrices directly from input output data. Three algorithms for the identification of the state space matrices can be derived from these theorems. The similarities between the theorems and algorithms, and the corresponding open loop theorems and algorithms in the literature are remarked on},
  keywords = {closed loop subspace system identification,closed loop systems,controllability,Data mining,Error correction,Feedback,Hankel matrices,Instruments,Intelligent systems,Kalman filters,MIMO,nonsteady state Kalman filter states,Optimal control,Optimization methods,projection theorems,State estimation,state space matrices,state-space methods,system identification,Toeplitz matrices},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XTPRXBVV/overschee_closed_1997.pdf;/Users/antoniohortaribeiro/Zotero/storage/4LWIALB3/657851.html;/Users/antoniohortaribeiro/Zotero/storage/ASUDHXHQ/657851.html}
}

@inproceedings{overschee_subspace_1991,
  title = {Subspace Algorithms for the Stochastic Identification Problem},
  booktitle = {[1991] {{Proceedings}} of the 30th {{IEEE Conference}} on {{Decision}} and {{Control}}},
  author = {Overschee, P. Van and Moor, B. De},
  year = {1991},
  month = dec,
  pages = {1321-1326 vol.2},
  doi = {10.1109/CDC.1991.261604},
  abstract = {The authors derive a novel algorithm to consistently identify stochastic state space models from given output data without forming the covariance matrix and using only semi-infinite block Hankel matrices. The algorithm is based on the concept of principle angles and directions. The authors describe how these can be calculated with only QR and QSVD decompositions. They also provide an interpretation of the principle directions as states of a non-steady-state Kalman filter. With a couple of examples, it is shown that the proposed algorithm is superior to the classical canonical correlation algorithms},
  keywords = {covariance matrix,Gaussian noise,H infinity control,identification,Kalman filters,Least squares approximation,matrix algebra,Matrix decomposition,nonsteady-state Kalman filter,QR decompositions,QSVD decompositions,Robustness,semi-infinite block Hankel matrices,State estimation,state-space methods,stochastic identification,stochastic processes,Stochastic resonance,stochastic state space models},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4QRAU5UE/overschee_subspace_1991.pdf;/Users/antoniohortaribeiro/Zotero/storage/V26CZ9MQ/261604.html;/Users/antoniohortaribeiro/Zotero/storage/XPMMJ5VQ/261604.html}
}

@article{paduart_identification_2010,
  title = {Identification of Nonlinear Systems Using {{Polynomial Nonlinear State Space}} Models},
  author = {Paduart, Johan and Lauwers, Lieve and Swevers, Jan and Smolders, Kris and Schoukens, Johan and Pintelon, Rik},
  year = {2010},
  month = apr,
  journal = {Automatica},
  volume = {46},
  number = {4},
  pages = {647--656},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2010.01.001},
  abstract = {In this paper, we propose a method to model nonlinear systems using polynomial nonlinear state space equations. Obtaining good initial estimates is a major problem in nonlinear modelling. It is solved here by identifying first the best linear approximation of the system under test. The proposed identification procedure is successfully applied to measurements of two physical systems.},
  keywords = {Best linear approximation,Multivariable systems,nonlinear systems,system identification},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UAUKTM2C/paduart_identifica_2010.pdf;/Users/antoniohortaribeiro/Zotero/storage/UWCEJ246/S000510981000021X.html;/Users/antoniohortaribeiro/Zotero/storage/Z9QP2PR6/S000510981000021X.html}
}

@article{paixao_clinical_2018,
  title = {Clinical {{Outcomes}} in {{Digital Electrocardiography}}: {{Evaluation}} of {{Mortality}} in {{Atrial Fibrillation}} ({{Code Study}})},
  shorttitle = {Abstract 16594},
  author = {Paixao, Gabriela and e Silva, Luis Gustavo Silva and Gomes, Paulo R. and Ferreira, Milton and Oliveira, Derick and Ribeiro, Manoel Horta and Ribeiro, Antonio H. and Nascimento, Jamil and Cardoso, Gustavo and Araujo, Rodrigo and Santos, Bruno and Canazart, Jessica and Ribeiro, Leonardo and Ribeiro, Antonio L.},
  year = {2018},
  month = nov,
  journal = {Circulation. Abstracts from American Heart Association's.},
  volume = {138},
  number = {Suppl\_1},
  pages = {A16594-A16594},
  publisher = {American Heart Association},
  urldate = {2020-12-29},
  abstract = {Introduction: Telehealth system is an important tool to improve access and quality to health assistance.Large electrocardiogram (ECG) databases, linked to mortality or hospitalization data, can be useful in determining the prognostic value of ECG markers. Atrial fibrillation (AF) is a public health problem with increasing prevalence as the population ages, associated with cardiovascular mortality and morbidity.Hypothesis: Evaluate the association between the presence of AF with overall and cardiovascular mortality in a large electronic cohort of primary care patients of Minas Gerais.Methods: This is an observational retrospective study. Patients over 16 years old who performed digital electrocardiograms by Telehealth Network of Minas Gerais from 2013 to 2016 were assessed. A probabilistic linkage between data from the national mortality information system and our ECG database was made. Clinical data were self-reported, and ECGs were interpreted by a team of trained cardiologists and automatic software (Glasgow and Minnesota).The diagnosis of AF was considered if there was concordance between the cardiologist{$\prime$}s report and one of the automatic systems. In cases of disagreement, ECGs were reviewed manually.Only the first ECG made was analysed. To assess the relation between AF and mortality, Cox regression was used, adjusted by age, sex and clinical conditions.Results: From a dataset of 1,773,689 patients, 1,075,531 were included. The mean age was 51.4 years, 40.5\% male.The prevalence of AF was 1.15\%. There were 2.9\% deaths for all causes in 2.69 years of mean follow up. In univariate analysis, AF was a risk factor for death from all causes (HR 6.98, 95\%CI 6.68-7.28). After adjustment for age, sex and comorbidities, AF remained an independent risk factor for all-cause mortality (HR 2.49; 95\% CI 2.39 - 2.61). AF was also a predictor of risk for cardiovascular mortality after adjustment for age, sex and clinical conditions (HR 2.35, 95\% CI 2.01-2.73). In multivariate analysis by sex, adjusted for age and comorbidities, AF women had higher risk of death for all causes (HR 3.06; 95\% CI 2.86-3.26) than men (HR 2.18; 95\% CI 2.06-2.32). There were no difference between sex in cardiovascular mortality (HR 2.34; 95\% CI 2.01-2.73 for male sex e HR 2.49; 95\% CI 2.14-2.90 for female).Conclusions: AF was a strong predictor of mortality for all causes and cardiovascular mortality in primary care population with increased risk in women for deaths for all cause.},
  copyright = {All rights reserved},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ICX85PEH/circ.138.suppl_1.html}
}

@article{paixao_ecgage_2020,
  title = {{{ECG-AGE FROM ARTIFICIAL INTELLIGENCE}}: {{A NEW PREDICTOR FOR MORTALITY}}? {{THE CODE}} ({{CLINICAL OUTCOMES IN DIGITAL ELECTROCARDIOGRAPHY}}) {{STUDY}}},
  author = {Paix{\~a}o, Gabriela Miana and Ribeiro, Antonio Horta and Lima, Emilly and Seewald, Bruna and Ribeiro, Manoel Horta and Oliveira, Derick and Gomes, Paulo and Castro, Nathalia and Meira, Wagner and Sch{\"o}n, Thomas and Ribeiro, Antonio L.},
  year = {2020},
  journal = {Journal of the American College of Cardiology},
  volume = {75},
  number = {11 Supplement 1},
  pages = {3672},
  issn = {0735-1097},
  doi = {10.1016/S0735-1097(20)34299-6},
  copyright = {All rights reserved}
}

@article{paixao_electrocardiographic_2021,
  title = {Electrocardiographic {{Predictors}} of {{Mortality}}: {{Data}} from a {{Primary Care Tele-Electrocardiography Cohort}} of {{Brazilian Patients}}},
  shorttitle = {Electrocardiographic {{Predictors}} of {{Mortality}}},
  author = {Paix{\~a}o, Gabriela M. M. and Lima, Emilly M. and Gomes, Paulo R. and Oliveira, Derick M. and Ribeiro, Manoel H. and Nascimento, Jamil S. and Ribeiro, Antonio H. and Macfarlane, Peter W. and Ribeiro, Antonio L. P.},
  year = {2021},
  month = dec,
  journal = {Hearts},
  volume = {2},
  number = {4},
  pages = {449--458},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/hearts2040035},
  urldate = {2021-10-24},
  abstract = {Computerized electrocardiography (ECG) has been widely used and allows linkage to electronic medical records. The present study describes the development and clinical applications of an electronic cohort derived from a digital ECG database obtained by the Telehealth Network of Minas Gerais, Brazil, for the period 2010--2017, linked to the mortality data from the national information system, the Clinical Outcomes in Digital Electrocardiography (CODE) dataset. From 2,470,424 ECGs, 1,773,689 patients were identified. A total of 1,666,778 (94\%) underwent a valid ECG recording for the period 2010 to 2017, with 1,558,421 patients over 16 years old; 40.2\% were men, with a mean age of 51.7 [SD 17.6] years. During a mean follow-up of 3.7 years, the mortality rate was 3.3\%. ECG abnormalities assessed were: atrial fibrillation (AF), right bundle branch block (RBBB), left bundle branch block (LBBB), atrioventricular block (AVB), and ventricular pre-excitation. Most ECG abnormalities (AF: Hazard ratio [HR] 2.10; 95\% CI 2.03--2.17; RBBB: HR 1.32; 95\%CI 1.27--1.36; LBBB: HR 1.69; 95\% CI 1.62--1.76; first degree AVB: Relative survival [RS]: 0.76; 95\% CI0.71--0.81; 2:1 AVB: RS 0.21 95\% CI0.09--0.52; and RS 0.36; third degree AVB: 95\% CI 0.26--0.49) were predictors of overall mortality, except for ventricular pre-excitation (HR 1.41; 95\% CI 0.56--3.57) and Mobitz I AVB (RS 0.65; 95\% CI 0.34--1.24). In conclusion, a large ECG database established by a telehealth network can be a useful tool for facilitating new advances in the fields of digital electrocardiography, clinical cardiology and cardiovascular epidemiology.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  keywords = {big data,electrocardiogram,electronic cohort,mortality,telehealth},
  file = {/Users/antoniohortaribeiro/Zotero/storage/68ITSMEN/PaixÃ£o et al_2021_Electrocardiographic Predictors of Mortality.pdf;/Users/antoniohortaribeiro/Zotero/storage/HKXCG9EX/35.html}
}

@article{paixao_evaluation_2019,
  title = {Evaluation of Mortality in Bundle Branch Block Patients from an Electronic Cohort: {{Clinical Outcomes}} in {{Digital Electrocardiography}} ({{CODE}}) Study},
  author = {Paix{\~a}o, Gabriela M. M. and Lima, Emilly M. and Gomes, Paulo R. and Ferreira, Milton P. and Oliveira, Derick M. and Ribeiro, Manoel Horta and Ribeiro, Ant{\^o}nio H. and Nascimento, Jamil and Canazart, J{\'e}ssica A. and Cardoso, Gustavo and Ribeiro, Leonardo B. and Ribeiro, Antonio Luiz P.},
  year = {2019},
  month = sep,
  journal = {Journal of Electrocardiology},
  issn = {0022-0736},
  doi = {10.1016/j.jelectrocard.2019.09.004},
  copyright = {All rights reserved},
  keywords = {Artificial intelligence,Big-data,Electrocardiography,Telehealth},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2XWSN792/PaixÃ£o et al. - 2019 - Evaluation of mortality in bundle branch block pat.pdf}
}

@article{paixao_evaluation_2020,
  title = {Evaluation of {{Mortality}} in {{Atrial Fibrillation}}: {{Clinical Outcomes}} in {{Digital Electrocardiography}} ({{CODE}}) {{Study}}},
  shorttitle = {Evaluation of {{Mortality}} in {{Atrial Fibrillation}}},
  author = {Paix{\~a}o, Gabriela M. M. and Silva, Luis Gustavo S. and Gomes, Paulo R. and Lima, Emilly M. and Ferreira, Milton P. F. and Oliveira, Derick M. and Ribeiro, Manoel H. and Ribeiro, Antonio H. and Nascimento, Jamil S. and Canazart, J{\'e}ssica A. and Ribeiro, Leonardo B. and Benjamin, Emelia J. and Macfarlane, Peter W. and Marcolino, Milena S. and Ribeiro, Antonio L.},
  year = {2020},
  month = jul,
  journal = {Global Heart},
  volume = {15},
  number = {1},
  pages = {48},
  issn = {2211-8179},
  doi = {10.5334/gh.772},
  urldate = {2020-08-07},
  abstract = {Methods: This observational retrospective study of primary care patients was developed with the digital ECG database from the Telehealth Network of Minas Gerais, Brazil. ECGs performed from 2010 to 2017 were interpreted by cardiologists and the University of Glasgow automated analysis software. An electronic cohort was obtained linking data from ECG exams and those from a national mortality information system, using standard probabilistic linkage methods. We considered only the first ECG of each patient. Patients under 16 years were excluded. Hazard ratios (HR) for mortality were adjusted for demographic and self-reported clinical factors and estimated with Cox regression. Results: From a dataset of 1,773,689 patients, 1,558,421 were included, mean age 51.6 years; 40.2\% male. There were 3.34\% deaths from all causes in 3.68 years of median follow up. The prevalence of AF was 1.33\%. AF was an independent risk factor for all-cause mortality (HR 2.10, 95\%CI 2.03--2.17) and cardiovascular mortality (HR 2.06, 95\%CI 1.86--2.29). Females with AF had a higher risk of overall and cardiovascular mortality compared with males (p {$<$} 0.001). Conclusions: AF was a strong predictor of cardiovascular and all-cause mortality in a primary care population, with increased risk in women.},
  copyright = {All rights reserved},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IPHPL6WQ/PaixÃ£o et al. - 2020 - Evaluation of Mortality in Atrial Fibrillation Cl.pdf}
}

@article{paixao_validation_2020,
  title = {Validation of a {{Deep Neural Network Electrocardiographic-Age}} as a {{Mortality Predictor}}: {{The CODE Study}}},
  shorttitle = {Abstract 16883},
  author = {Paixao, Gabriela and Lima, Emilly M and Ribeiro, Antonio H and Gomes, Paulo R and Oliveira, Derick and Junior, Marcelo M. Pinto and Sabino, Ester and Barreto, Sandhi and Giatti, Luana and A, Paulo Andrade Lotufo and Bruce, Duncan and Wagner, Meira Junior and Thomas B. Schon and Ribeiro, Antonio L.},
  year = {2020},
  month = nov,
  journal = {Circulation},
  volume = {142},
  number = {Suppl\_3},
  pages = {A16883-A16883},
  publisher = {American Heart Association},
  doi = {10.1161/circ.142.suppl_3.16883},
  urldate = {2020-12-29},
  abstract = {Introduction: Aging affects the electrocardiogram (ECG) with a higher incidence of abnormalities in older patients. ECG-age can be predicted by artificial intelligence (AI) and can be used as a measure of cardiovascular health.Hypothesis: ECG-age predicted by AI is a risk factor for overall mortality.Methods: The Clinical Outcomes in Digital Electrocardiography (CODE) study is a retrospective cohort with a mean follow-up of 3.67 years.The dataset consists of Brazilian patients, mainly from primary care centers. Two established cohorts, ELSA-Brasil, of Brazilian public servants, and SaMi-Trop, of Chagas disease patients, were used for external validation. 2,322,513 ECGs from 1,558,421 patients over 16 years old that underwent an ECG from 2010 to 2017 were included. A deep convolutional neural network was trained in order to predict the age of the patient based solely on ECG 12-lead tracings. The ECG database was split into 85-15\% training and test datasets, respectively. Death was ascertained using probabilistic linkage with Brazil{$\prime$}s mortality information data. The Cox regression model, adjusted by age and sex, was used for statistical analysis. The model was validated in two cohorts: ELSA-Brasil (n=14,263) and SaMi-Trop (n=1,631).Results: he mean predicted ECG-age was 52.0 years ({\textpm}18.7) with a mean absolute error of 8.38 ({\textpm}7.0) years. Patients with ECG-age {$>$}8y older than chronological age had higher mortality rate (HR 1.79, 95\%CI 1.69-1.90; p{$<$}0.001), whereas those ECG-age {$>$}8y younger than chronological age were associated with a lower mortality rate (HR 0.78, 95\%CI 0.74-0.83; p{$<$}0.001). These results were similar in ELSA-Brasil and SaMi-Trop external validation cohorts (HR 1.75, 95\%CI 1.35-2.27; p{$<$}0.001;HR 2.42, 95\%CI 1.53-3.83; p{$<$}0.001 for {$>$}8y difference, retrospectively; HR 0.74, 95\%CI 0.63-0.88; p{$<$}0.001;HR 0.89, 95\%CI 0.52-1.54; p=0.68 for {$<$}8y difference, respectively).Conclusions: ECG-age, predicted by AI, can be useful as a tool for risk stratification of mortality.},
  copyright = {All rights reserved}
}

@inproceedings{paperno_lambada_2016,
  title = {The {{LAMBADA}} Dataset: {{Word}} Prediction Requiring a Broad Discourse Context},
  shorttitle = {The {{LAMBADA}} Dataset},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Ngoc-Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  year = {2016},
  month = aug,
  pages = {1525--1534},
  doi = {10/gf7pwc},
  urldate = {2019-09-08},
  langid = {american},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4WDWA8ED/paperno_the_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/I9ZL4U97/P16-1144.html}
}

@inproceedings{papineni_bleu_2002,
  title = {{{BLEU}}: A Method for Automatic Evaluation of Machine Translation},
  shorttitle = {{{BLEU}}},
  booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  year = {2002},
  pages = {311--318},
  publisher = {Association for Computational Linguistics},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2B8EQKPK/papineni_bleu_2002.pdf}
}

@book{papoulis_probability_2002,
  title = {Probability, Random Variables, and Stochastic Processes},
  author = {Papoulis, Athanasios and Pillai, S. Unnikrishna},
  year = {2002},
  edition = {4th ed},
  publisher = {McGraw-Hill},
  address = {Boston},
  isbn = {978-0-07-366011-0 978-0-07-112256-6},
  lccn = {QA273 .P2 2002},
  keywords = {Probabilities,Random variables,stochastic processes},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JEMCJMJ6/papoulis_probabilit_2002.pdf}
}

@article{parhi_banach_2021,
  title = {Banach {{Space Representer Theorems}} for {{Neural Networks}} and {{Ridge Splines}}},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2021},
  month = feb,
  journal = {arXiv:2006.05626 [cs, stat]},
  eprint = {2006.05626},
  primaryclass = {cs, stat},
  urldate = {2021-11-05},
  abstract = {We develop a variational framework to understand the properties of the functions learned by neural networks fit to data. We propose and study a family of continuous-domain linear inverse problems with total variation-like regularization in the Radon domain subject to data fitting constraints. We derive a representer theorem showing that finite-width, single-hidden layer neural networks are solutions to these inverse problems. We draw on many techniques from variational spline theory and so we propose the notion of polynomial ridge splines, which correspond to single-hidden layer neural networks with truncated power functions as the activation function. The representer theorem is reminiscent of the classical reproducing kernel Hilbert space representer theorem, but we show that the neural network problem is posed over a non-Hilbertian Banach space. While the learning problems are posed in the continuous-domain, similar to kernel methods, the problems can be recast as finite-dimensional neural network training problems. These neural network training problems have regularizers which are related to the well-known weight decay and path-norm regularizers. Thus, our result gives insight into functional characteristics of trained neural networks and also into the design neural network regularizers. We also show that these regularizers promote neural network solutions with desirable generalization properties.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GCHS6RSI/Parhi_Nowak_2021_Banach Space Representer Theorems for Neural Networks and Ridge Splines.pdf;/Users/antoniohortaribeiro/Zotero/storage/FW7SJQKN/2006.html}
}

@article{parhi_nearminimax_2021,
  title = {Near-{{Minimax Optimal Estimation With Shallow ReLU Neural Networks}}},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.08844 [cs, math, stat]},
  eprint = {2109.08844},
  primaryclass = {cs, math, stat},
  urldate = {2021-11-05},
  abstract = {We study the problem of estimating an unknown function from noisy data using shallow (single-hidden layer) ReLU neural networks. The estimators we study minimize the sum of squared data-fitting errors plus a regularization term proportional to the Euclidean norm of the network weights. This minimization corresponds to the common approach of training a neural network with weight decay. We quantify the performance (mean-squared error) of these neural network estimators when the data-generating function belongs to the space of functions of second-order bounded variation in the Radon domain. This space of functions was recently proposed as the natural function space associated with shallow ReLU neural networks. We derive a minimax lower bound for the estimation problem for this function space and show that the neural network estimators are minimax optimal up to logarithmic factors. We also show that this is a "mixed variation" function space that contains classical multivariate function spaces including certain Sobolev spaces and certain spectral Barron spaces. Finally, we use these results to quantify a gap between neural networks and linear methods (which include kernel methods). This paper sheds light on the phenomenon that neural networks seem to break the curse of dimensionality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/24KGFZY6/Parhi_Nowak_2021_Near-Minimax Optimal Estimation With Shallow ReLU Neural Networks.pdf;/Users/antoniohortaribeiro/Zotero/storage/I5SVYE27/2109.html}
}

@article{parhi_role_2020,
  title = {The {{Role}} of {{Neural Network Activation Functions}}},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2020},
  journal = {IEEE Signal Processing Letters},
  volume = {27},
  eprint = {1910.02333},
  pages = {1779--1783},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2020.3027517},
  urldate = {2021-11-05},
  abstract = {A wide variety of activation functions have been proposed for neural networks. The Rectified Linear Unit (ReLU) is especially popular today. There are many practical reasons that motivate the use of the ReLU. This paper provides new theoretical characterizations that support the use of the ReLU, its variants such as the leaky ReLU, as well as other activation functions in the case of univariate, single-hidden layer feedforward neural networks. Our results also explain the importance of commonly used strategies in the design and training of neural networks such as "weight decay" and "path-norm" regularization, and provide a new justification for the use of "skip connections" in network architectures. These new insights are obtained through the lens of spline theory. In particular, we show how neural network training problems are related to infinite-dimensional optimizations posed over Banach spaces of functions whose solutions are well-known to be fractional and polynomial splines, where the particular Banach space (which controls the order of the spline) depends on the choice of activation function.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5N7ZM2RJ/Parhi_Nowak_2020_The Role of Neural Network Activation Functions.pdf;/Users/antoniohortaribeiro/Zotero/storage/X9X9GNRR/1910.html}
}

@article{parhi_what_2021,
  title = {What {{Kinds}} of {{Functions}} Do {{Deep Neural Networks Learn}}? {{Insights}} from {{Variational Spline Theory}}},
  shorttitle = {What {{Kinds}} of {{Functions}} Do {{Deep Neural Networks Learn}}?},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2021},
  month = sep,
  journal = {arXiv:2105.03361 [cs, stat]},
  eprint = {2105.03361},
  primaryclass = {cs, stat},
  urldate = {2021-11-05},
  abstract = {We develop a variational framework to understand the properties of functions learned by fitting deep neural networks with rectified linear unit activations to data. We propose a new function space, which is reminiscent of classical bounded variation-type spaces, that captures the compositional structure associated with deep neural networks. We derive a representer theorem showing that deep ReLU networks are solutions to regularized data fitting problems over functions from this space. The function space consists of compositions of functions from the Banach spaces of second-order bounded variation in the Radon domain. These are Banach spaces with sparsity-promoting norms, giving insight into the role of sparsity in deep neural networks. The neural network solutions have skip connections and rank bounded weight matrices, providing new theoretical support for these common architectural choices. The variational problem we study can be recast as a finite-dimensional neural network training problem with regularization schemes related to the notions of weight decay and path-norm regularization. Finally, our analysis builds on techniques from variational spline theory, providing new connections between deep neural networks and splines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/F79KSYZP/Parhi_Nowak_2021_What Kinds of Functions do Deep Neural Networks Learn.pdf;/Users/antoniohortaribeiro/Zotero/storage/87NXLDNG/2105.html}
}

@inproceedings{park_realtime_2007,
  title = {Real-Time Stereo Vision {{FPGA}} Chip with Low Error Rate},
  booktitle = {Multimedia and {{Ubiquitous Engineering}}, 2007. {{MUE}}'07. {{International Conference}} On},
  author = {Park, Sungchan and Jeong, Hong},
  year = {2007},
  pages = {751--756},
  publisher = {IEEE},
  annotation = {00000}
}

@article{park_universal_1991,
  title = {Universal {{Approximation Using Radial-Basis-Function Networks}}},
  author = {Park, Jooyoung and Sandberg, Irwin W},
  year = {1991},
  journal = {Neural computation},
  volume = {3},
  number = {2},
  pages = {246--257},
  doi = {10/d3sv6t},
  annotation = {03486}
}

@article{park_universal_1991a,
  title = {Universal {{Approximation Using Radial-Basis-Function Networks}}},
  author = {Park, J. and Sandberg, I. W.},
  year = {1991},
  month = jun,
  journal = {Neural Computation},
  volume = {3},
  number = {2},
  pages = {246--257},
  publisher = {MIT Press},
  issn = {0899-7667},
  doi = {10.1162/neco.1991.3.2.246},
  urldate = {2020-11-24},
  abstract = {There have been several recent studies concerning feedforward networks and the problem of approximating arbitrary functionals of a finite number of real variables. Some of these studies deal with cases in which the hidden-layer nonlinearity is not a sigmoid. This was motivated by successful applications of feedforward networks with nonsigmoidal hidden-layer units. This paper reports on a related study of radial-basis-function (RBF) networks, and it is proved that RBF networks having one hidden layer are capable of universal approximation. Here the emphasis is on the case of typical RBF networks, and the results show that a certain class of RBF networks with the same smoothing factor in each kernel node is broad enough for universal approximation.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RRGNXZKR/neco.1991.3.2.html}
}

@article{park_universal_1991b,
  title = {Universal Approximation Using Radial-Basis-Function Networks},
  author = {Park, Jooyoung and Sandberg, Irwin W},
  year = {1991},
  journal = {Neural computation},
  volume = {3},
  number = {2},
  pages = {246--257},
  keywords = {â“Multiple DOI},
  annotation = {00000}
}

@book{parziale_tcp_2006,
  title = {{{TCP}}/{{IP Tutorial}} and {{Technical Overview}}},
  author = {Parziale, L. and Liu, W. and Matthews, C. and Rosselot, N. and Davis, C. and Forrester, J. and Britt, D.T. and Redbooks, {\relax IBM}},
  year = {2006},
  series = {{{IBM}} Redbooks},
  publisher = {IBM Redbooks},
  isbn = {978-0-7384-9468-5},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/M7HHUW94/parziale_tcp-ip_2006.pdf}
}

@misc{pascanu_difficulty_2013,
  title = {On the {{Difficulty}} of {{Training Recurrent Neural Networks}}},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  year = {2013},
  journal = {Proceedings of the 30th International Conference on International Conference on Machine Learning},
  volume = {28},
  pages = {1310--1318},
  abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  keywords = {â›” No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2KX6A2R7/1211.5063.pdf}
}

@article{pastika_artificial_2024,
  title = {Artificial Intelligence--Enabled Electrocardiogram for Mortality and Cardiovascular Risk Estimation: {{An}} Actionable, Explainable and Biologically Plausible Platform},
  author = {Pastika, Libor and Sau, Arunashis and Patlatzoglou, Konstantinos and Sieliwonczyk, Ewa and Ribeiro, Antonio H. and McGurk, Kathryn A. and Scott, William R and Ware, James S. and Ribeiro, Antonio Luiz P. and Kramer, Daniel B. and Waks, Jonathan W. and Ng, Fu Siong},
  year = {2024},
  journal = {npj Digital Medicine},
  volume = {7},
  number = {167},
  doi = {10.1038/s41746-024-01170-0}
}

@article{pastur_random_2020,
  title = {On {{Random Matrices Arising}} in {{Deep Neural Networks}}. {{Gaussian Case}}},
  author = {Pastur, Leonid},
  year = {2020},
  month = apr,
  journal = {arXiv:2001.06188},
  eprint = {2001.06188},
  urldate = {2021-03-12},
  abstract = {The paper deals with distribution of singular values of product of random matrices arising in the analysis of deep neural networks. The matrices resemble the product analogs of the sample covariance matrices, however, an important difference is that the population covariance matrices, which are assumed to be non-random in the standard setting of statistics and random matrix theory, are now random, moreover, are certain functions of random data matrices. The problem has been considered in recent work [21] by using the techniques of free probability theory. Since, however, free probability theory deals with population matrices which are independent of the data matrices, its applicability in this case requires an additional justification. We present this justification by using a version of the standard techniques of random matrix theory under the assumption that the entries of data matrices are independent Gaussian random variables. In the subsequent paper [18] we extend our results to the case where the entries of data matrices are just independent identically distributed random variables with several finite moments. This, in particular, extends the property of the so-called macroscopic universality on the considered random matrices.},
  archiveprefix = {arXiv},
  keywords = {{Primary 15B52, Secondary 92B20},Mathematical Physics,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/72PC5PTZ/Pastur - 2020 - On Random Matrices Arising in Deep Neural Networks.pdf;/Users/antoniohortaribeiro/Zotero/storage/QUHCQIG6/2001.html}
}

@techreport{paszke_automatic_2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year = {2017},
  keywords = {â›” No DOI found}
}

@incollection{paszke_pytorch_2019,
  title = {{{PyTorch}}: {{An}} Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  pages = {8024--8035}
}

@article{patan_nonlinear_2012,
  title = {Nonlinear Model Predictive Control of a Boiler Unit: {{A}} Fault Tolerant Control Study},
  author = {Patan, Krzysztof and Korbicz, J{\'o}zef},
  year = {2012},
  journal = {International Journal of Applied Mathematics and Computer Science},
  volume = {22},
  number = {1},
  pages = {225--237},
  doi = {10.2478/v10006-012-0017-6},
  annotation = {00000}
}

@article{patan_nonlinear_2012a,
  title = {Nonlinear {{Model Predictive Control}} of a {{Boiler Unit}}: {{A Fault Tolerant Control Study}}},
  author = {Patan, Krzysztof and Korbicz, J{\'o}zef},
  year = {2012},
  journal = {International Journal of Applied Mathematics and Computer Science},
  volume = {22},
  number = {1},
  pages = {225--237},
  doi = {10/gfjwmj}
}

@article{pathak_modelfree_2018,
  title = {Model-{{Free Prediction}} of {{Large Spatiotemporally Chaotic Systems}} from {{Data}}: {{A Reservoir Computing Approach}}},
  shorttitle = {Model-{{Free Prediction}} of {{Large Spatiotemporally Chaotic Systems}} from {{Data}}},
  author = {Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
  year = {2018},
  month = jan,
  journal = {Physical Review Letters},
  volume = {120},
  number = {2},
  pages = {024102},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.120.024102},
  urldate = {2021-04-02},
  abstract = {We demonstrate the effectiveness of using machine learning for model-free prediction of spatiotemporally chaotic systems of arbitrarily large spatial extent and attractor dimension purely from observations of the system's past evolution. We present a parallel scheme with an example implementation based on the reservoir computing paradigm and demonstrate the scalability of our scheme using the Kuramoto-Sivashinsky equation as an example of a spatiotemporally chaotic system.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HR38MNNS/Pathak et al. - 2018 - Model-Free Prediction of Large Spatiotemporally Ch.pdf;/Users/antoniohortaribeiro/Zotero/storage/MYTKYAH5/PhysRevLett.120.html}
}

@article{pathak_using_2017,
  title = {Using Machine Learning to Replicate Chaotic Attractors and Calculate {{Lyapunov}} Exponents from Data},
  author = {Pathak, Jaideep and Lu, Zhixin and Hunt, Brian R. and Girvan, Michelle and Ott, Edward},
  year = {2017},
  month = dec,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {27},
  number = {12},
  pages = {121102},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5010300},
  urldate = {2021-02-19},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/45VL5GBC/Pathak et al. - 2017 - Using machine learning to replicate chaotic attrac.pdf}
}

@book{patterson_computer_2005,
  title = {Computer Organization and Design: The Hardware/Software Interface},
  shorttitle = {Computer Organization and Design},
  author = {Patterson, David A. and Hennessy, John L.},
  year = {2005},
  edition = {3. ed},
  publisher = {Kaufmann},
  address = {Amsterdam},
  isbn = {978-1-55860-604-3 978-0-12-088433-9},
  langid = {english},
  annotation = {00000 \\
OCLC: 249857976},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GMCNIZ9B/patterson_computer_2005.pdf}
}

@article{pauwels_comparison_2012,
  title = {A Comparison of {{FPGA}} and {{GPU}} for Real-Time Phase-Based Optical Flow, Stereo, and Local Image Features},
  author = {Pauwels, Karl and Tomasi, Matteo and Diaz Alonso, Javier and Ros, Eduardo and Van Hulle, Marc M},
  year = {2012},
  journal = {Computers, IEEE Transactions on},
  volume = {61},
  number = {7},
  pages = {999--1012},
  doi = {10.1109/TC.2011.120},
  annotation = {00000}
}

@article{peche_note_2019,
  title = {A Note on the {{Pennington-Worah}} Distribution},
  author = {P{\'e}ch{\'e}, S.},
  year = {2019},
  month = jan,
  journal = {Electronic Communications in Probability},
  volume = {24},
  number = {none},
  issn = {1083-589X},
  doi = {10.1214/19-ECP262},
  urldate = {2021-06-24},
  abstract = {This paper is concerned with a new expression of the so-called Pennington-Worah distribution, characterizing the asymptotic empirical eigenvalue distribution of some non linear random matrix ensembles.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PJU84Q5C/PÃ©chÃ© - 2019 - A note on the Pennington-Worah distribution.pdf}
}

@article{peifer_parameter_2007,
  title = {Parameter Estimation in Ordinary Differential Equations for Biochemical Processes Using the Method of Multiple Shooting},
  author = {Peifer, M and Timmer, J},
  year = {2007},
  journal = {IET Systems Biology},
  volume = {1},
  number = {2},
  pages = {78--88},
  doi = {10.1049/iet-syb:20060067},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SJSS7ZJU/peifer_parameter_2007.pdf}
}

@article{peng_eventtriggered_2017,
  title = {Event-Triggered Fault Detection Framework Based on Subspace Identification Method for the Networked Control Systems},
  author = {Peng, Kaixiang and Wang, Mengyuan and Dong, Jie},
  year = {2017},
  journal = {Neurocomputing},
  volume = {239},
  pages = {257--267},
  doi = {10.1016/j.neucom.2017.02.027},
  annotation = {00000}
}

@article{pennington_emergence_2018,
  title = {The {{Emergence}} of {{Spectral Universality}} in {{Deep Networks}}},
  author = {Pennington, Jeffrey and Schoenholz, Samuel S. and Ganguli, Surya},
  year = {2018},
  journal = {21st International Conference on Artificial Intelligence and Statistics (AISTATS)},
  abstract = {Recent work has shown that tight concentration of the entire spectrum of singular values of a deep network's input-output Jacobian around one at initialization can speed up learning by orders of magnitude. Therefore, to guide important design choices, it is important to build a full theoretical understanding of the spectra of Jacobians at initialization. To this end, we leverage powerful tools from free probability theory to provide a detailed analytic understanding of how a deep network's Jacobian spectrum depends on various hyperparameters including the nonlinearity, the weight and bias distributions, and the depth. For a variety of nonlinearities, our work reveals the emergence of new universal limiting spectral distributions that remain concentrated around one even as the depth goes to infinity.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RCNL7KKX/Pennington et al. - 2018 - The Emergence of Spectral Universality in Deep Net.pdf;/Users/antoniohortaribeiro/Zotero/storage/83984SID/1802.html}
}

@article{pennington_geometry_,
  title = {Geometry of {{Neural Network Loss Surfaces}} via {{Random Matrix Theory}}},
  author = {Pennington, Jeffrey and Bahri, Yasaman},
  pages = {9},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KY4FAVG2/Pennington and Bahri - Geometry of Neural Network Loss Surfaces via Rando.pdf}
}

@inproceedings{pennington_glove_2014,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WC2BWDPN/pennington_glove_2014.pdf}
}

@inproceedings{pennington_nonlinear_2017,
  title = {Nonlinear Random Matrix Theory for Deep Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pennington, Jeffrey and Worah, Pratik},
  year = {2017},
  pages = {2637--2646},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GVAEBXGV/Pennington and Worah - 2017 - Nonlinear random matrix theory for deep learning.pdf;/Users/antoniohortaribeiro/Zotero/storage/PDTXKQLV/NIPS-2017-nonlinear-random-matrix-theory-for-deep-learning-Paper.pdf}
}

@inproceedings{pennington_spectrum_2018,
  title = {The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Pennington, Jeffrey and Worah, Pratik},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QBVUJ4KT/Pennington and Worah - 2018 - The spectrum of the fisher information matrix of a.pdf}
}

@inproceedings{perdomo_performative_2020,
  title = {Performative Prediction},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Perdomo, Juan and Zrnic, Tijana and {Mendler-D{\"u}nner}, Celestine and Hardt, Moritz},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020-07-13/2020-07-18},
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {7599--7609},
  publisher = {PMLR},
  abstract = {When predictions support decisions they may influence the outcome they aim to predict. We call such predictions performative; the prediction influences the target. Performativity is a well-studied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining. We develop a risk minimization framework for performative prediction bringing together concepts from statistics, game theory, and causality. A conceptual novelty is an equilibrium notion we call performative stability. Performative stability implies that the predictions are calibrated not against past outcomes, but against the future outcomes that manifest from acting on the prediction. Our main results are necessary and sufficient conditions for the convergence of retraining to a performatively stable point of nearly minimal loss. In full generality, performative prediction strictly subsumes the setting known as strategic classification. We thus also give the first sufficient conditions for retraining to overcome strategic feedback effects.},
  pdf = {http://proceedings.mlr.press/v119/perdomo20a/perdomo20a.pdf}
}

@article{perry_proteomic_2024,
  title = {Proteomic Analysis of Cardiorespiratory Fitness for Prediction of Mortality and Multisystem Disease Risks},
  author = {Perry, Andrew S. and {Farber-Eger}, Eric and Gonzales, Tomas and Tanaka, Toshiko and Robbins, Jeremy M. and Murthy, Venkatesh L. and Stolze, Lindsey K. and Zhao, Shilin and Huang, Shi and Colangelo, Laura A. and Deng, Shuliang and Hou, Lifang and {Lloyd-Jones}, Donald M. and Walker, Keenan A. and Ferrucci, Luigi and Watts, Eleanor L. and Barber, Jacob L. and Rao, Prashant and Mi, Michael Y. and Gabriel, Kelley Pettee and Hornikel, Bjoern and Sidney, Stephen and Houstis, Nicholas and Lewis, Gregory D. and Liu, Gabrielle Y. and Thyagarajan, Bharat and Khan, Sadiya S. and Choi, Bina and Washko, George and Kalhan, Ravi and Wareham, Nick and Bouchard, Claude and Sarzynski, Mark A. and Gerszten, Robert E. and Brage, Soren and Wells, Quinn S. and Nayor, Matthew and Shah, Ravi V.},
  year = {2024},
  month = jun,
  journal = {Nature Medicine},
  pages = {1--11},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-024-03039-x},
  urldate = {2024-06-04},
  abstract = {Despite the wide effects of cardiorespiratory fitness (CRF) on metabolic, cardiovascular, pulmonary and neurological health, challenges in the feasibility and reproducibility of CRF measurements have impeded its use for clinical decision-making. Here we link proteomic profiles to CRF in 14,145 individuals across four international cohorts with diverse CRF ascertainment methods to establish, validate and characterize a proteomic CRF score. In a cohort of around 22,000 individuals in the UK Biobank, a proteomic CRF score was associated with a reduced risk of all-cause mortality (unadjusted hazard ratio 0.50 (95\% confidence interval 0.48--0.52) per 1\,s.d. increase). The proteomic CRF score was also associated with multisystem disease risk and provided risk reclassification and discrimination beyond clinical risk factors, as well as modulating high polygenic risk of certain diseases. Finally, we observed dynamicity of the proteomic CRF score in individuals who undertook a 20-week exercise training program and an association of the score with the degree of the effect of training on CRF, suggesting potential use of the score for personalization of exercise recommendations. These results indicate that population-based proteomics provides biologically relevant molecular readouts of CRF that are additive to genetic risk, potentially modifiable and clinically translatable.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Epidemiology,Prognostic markers},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YMN7C7LW/Perry et al. - 2024 - Proteomic analysis of cardiorespiratory fitness fo.pdf}
}

@article{peters_deep_2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.05365 [cs]},
  eprint = {1802.05365},
  primaryclass = {cs},
  urldate = {2019-06-08},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8SUIUHN4/peters_deep_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/NQYCQCW8/1802.html}
}

@book{peters_elements_2017,
  title = {Elements of Causal Inference: Foundations and Learning Algorithms},
  shorttitle = {Elements of Causal Inference},
  author = {Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year = {2017},
  urldate = {2022-03-13},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5GIXJJFI/Shanmugam - 2018 - Elements of causal inference foundations and lear.pdf}
}

@article{petrovic_kalman_2013,
  title = {Kalman Filter and {{NARX}} Neural Network for Robot Vision Based Human Tracking},
  author = {Petrovi{\'c}, Emina and {\'C}ojba{\v s}i{\'c}, {\v Z}arko and {Risti{\'c}-Durrant}, Danijela and Nikoli{\'c}, Vlastimir and {\'C}iri{\'c}, Ivan and jan Mati{\'c}, Sr{\textbackslash}d},
  year = {2013},
  journal = {Facta Universitatis, Series: Automatic Control And Robotics},
  volume = {12},
  number = {1},
  pages = {43--51},
  keywords = {ðŸ”No DOI found},
  annotation = {00000}
}

@article{petrovic_kalman_2013a,
  title = {Kalman {{Filter}} and {{NARX Neural Network}} for {{Robot Vision Based Human Tracking}}},
  author = {Petrovi{\'c}, Emina and {\'C}ojba{\v s}i{\'c}, {\v Z}arko and {Risti{\'c}-Durrant}, Danijela and Nikoli{\'c}, Vlastimir and {\'C}iri{\'c}, Ivan and {jan Mati{\'c}}, Sr{\textbackslash}d},
  year = {2013},
  journal = {Facta Universitatis, Series: Automatic Control And Robotics},
  volume = {12},
  number = {1},
  pages = {43--51},
  keywords = {ðŸ”No DOI found},
  annotation = {00009}
}

@book{pfanzagl_parametric_1994,
  title = {Parametric Statistical Theory},
  author = {Pfanzagl, Johann},
  year = {1994},
  publisher = {Walter de Gruyter},
  annotation = {00000}
}

@article{pillonetto_deep_2023,
  title = {Deep Networks for System Identification: A {{Survey}}},
  author = {Pillonetto, Gianluigi and Aravkin, Aleksandr and Gedon, Daniel and Ljung, Lennart and Ribeiro, Antonio H. and Sch{\"o}n, Thomas Bo},
  year = {2023},
  journal = {Provisionally accepted at Automatica},
  doi = {10.48550/arXiv.2301.12832},
  copyright = {All rights reserved}
}

@article{pillonetto_kernel_2014,
  title = {Kernel Methods in System Identification, Machine Learning and Function Estimation: {{A}} Survey},
  shorttitle = {Kernel Methods in System Identification, Machine Learning and Function Estimation},
  author = {Pillonetto, Gianluigi and Dinuzzo, Francesco and Chen, Tianshi and De Nicolao, Giuseppe and Ljung, Lennart},
  year = {2014},
  journal = {Automatica},
  volume = {50},
  number = {3},
  pages = {657--682},
  doi = {10/f236r8},
  urldate = {2019-04-10},
  abstract = {Most of the currently used techniques for linear system identification are based on classical estimation paradigms coming from mathematical statistics. In particular, maximum likelihood and prediction error methods represent the mainstream approaches to identification of linear dynamic systems, with a long history of theoretical and algorithmic contributions. Parallel to this, in the machine learning community alternative techniques have been developed. Until recently, there has been little contact between these two worlds. The first aim of this survey is to make accessible to the control community the key mathematical tools and concepts as well as the computational aspects underpinning these learning techniques. In particular, we focus on kernel-based regularization and its connections with reproducing kernel Hilbert spaces and Bayesian estimation of Gaussian processes. The second aim is to demonstrate that learning techniques tailored to the specific features of dynamic systems may outperform conventional parametric approaches for identification of stable linear systems.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/X94UG5B7/Pillonetto et al. - 2014 - Kernel methods in system identification, machine l.pdf}
}

@article{piroddi_identification_2003,
  title = {An Identification Algorithm for Polynomial {{NARX}} Models Based on Simulation Error Minimization},
  author = {Piroddi, L. and Spinelli, W.},
  year = {2003},
  month = nov,
  journal = {International Journal of Control},
  volume = {76},
  number = {17},
  pages = {1767--1781},
  issn = {0020-7179, 1366-5820},
  doi = {10/cc8w93},
  urldate = {2019-04-14},
  langid = {english}
}

@article{piroddi_identification_2003a,
  title = {An Identification Algorithm for Polynomial {{NARX}} Models Based on Simulation Error Minimization},
  author = {Piroddi, Luigi and Spinelli, William},
  year = {2003},
  journal = {International Journal of Control},
  volume = {76},
  number = {17},
  pages = {1767--1781},
  doi = {10.1080/00207170310001635419},
  annotation = {00000}
}

@article{piroddi_identification_2003b,
  title = {An {{Identification Algorithm}} for {{Polynomial NARX Models Based}} on {{Simulation Error Minimization}}},
  author = {Piroddi, Luigi and Spinelli, William},
  year = {2003},
  journal = {International Journal of Control},
  volume = {76},
  number = {17},
  pages = {1767--1781},
  doi = {10/cc8w93},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ICHBQRV5/Piroddi and Spinelli - 2003 - An identification algorithm for polynomial NARX mo.pdf}
}

@article{piroddi_simulation_2008,
  title = {Simulation {{Error Minimisation Methods}} for {{NARX Model Identification}}},
  author = {Piroddi, Luigi},
  year = {2008},
  journal = {International Journal of Modelling, Identification and Control},
  volume = {3},
  number = {4},
  pages = {392--403},
  doi = {10/bmvskt},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WE5XJ6BS/piroddi_simulation_2008.pdf}
}

@article{plantenga_trust_1998,
  title = {A Trust Region Method for Nonlinear Programming Based on Primal Interior-Point Techniques},
  author = {Plantenga, Todd},
  year = {1998},
  journal = {SIAM journal on Scientific Computing},
  volume = {20},
  number = {1},
  pages = {282--305},
  doi = {10.1137/S1064827595284403},
  urldate = {2017-08-20},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RSLDMVTE/plantenga_a trust_1998.pdf;/Users/antoniohortaribeiro/Zotero/storage/WP4QLTAB/plantenga_a trust_1998.pdf}
}

@article{plantenga_trust_1998a,
  title = {A {{Trust Region Method}} for {{Nonlinear Programming Based}} on {{Primal Interior-Point Techniques}}},
  author = {Plantenga, Todd},
  year = {1998},
  journal = {SIAM journal on Scientific Computing},
  volume = {20},
  number = {1},
  pages = {282--305},
  doi = {10/cg4zbf},
  urldate = {2017-08-20}
}

@article{polak_note_1969,
  title = {Note Sur La Convergence de M{\'e}thodes de Directions Conjugu{\'e}es},
  author = {Polak, Elijah and Ribiere, Gerard},
  year = {1969},
  journal = {Revue fran{\c c}aise d'informatique et de recherche op{\'e}rationnelle. S{\'e}rie rouge},
  volume = {3},
  number = {16},
  pages = {35--43},
  issn = {0373-8000},
  doi = {10.1051/m2an/196903R100351},
  annotation = {00000}
}

@book{poljak_introduction_1987,
  title = {Introduction to Optimization},
  author = {Poljak, Boris T.},
  year = {1987},
  publisher = {Optimization Software}
}

@article{poon_modelbased_2017,
  title = {Model-Based Fault Detection and Identification for Switching Power Converters},
  author = {Poon, Jason and Jain, Palak and Konstantakopoulos, Ioannis C and Spanos, Costas and Panda, Sanjib Kumar and Sanders, Seth R},
  year = {2017},
  journal = {IEEE Transactions on Power Electronics},
  volume = {32},
  number = {2},
  pages = {1419--1430},
  doi = {10.1109/TPEL.2016.2541342},
  annotation = {00000}
}

@article{poorthuis_utility_2021,
  title = {Utility of Risk Prediction Models to Detect Atrial Fibrillation in Screened Participants},
  author = {Poorthuis, Michiel H. F. and Jones, Nicholas R. and Sherliker, Paul and Clack, Rachel and {de Borst}, Gert J. and Clarke, Robert and Lewington, Sarah and Halliday, Alison and Bulbulia, Richard},
  year = {2021},
  month = may,
  journal = {European Journal of Preventive Cardiology},
  volume = {28},
  number = {6},
  pages = {586--595},
  issn = {2047-4881},
  doi = {10.1093/eurjpc/zwaa082},
  abstract = {AIMS: Atrial fibrillation (AF) is associated with higher risk of stroke. While the prevalence of AF is low in the general population, risk prediction models might identify individuals for selective screening of AF. We aimed to systematically identify and compare the utility of established models to predict prevalent AF. METHODS AND RESULTS: Systematic search of PubMed and EMBASE for risk prediction models for AF. We adapted established risk prediction models and assessed their predictive performance using data from 2.5M individuals who attended vascular screening clinics in the USA and the UK and in the subset of 1.2M individuals with CHA2DS2-VASc {$\geq$}2. We assessed discrimination using area under the receiver operating characteristic (AUROC) curves and agreement between observed and predicted cases using calibration plots. After screening 6959 studies, 14 risk prediction models were identified. In our cohort, 10~464 (0.41\%) participants had AF. For discrimination, six prediction model had AUROC curves of 0.70 or above in all individuals and those with CHA2DS2-VASc {$\geq$}2. In these models, calibration plots showed very good concordance between predicted and observed risks of AF. The two models with the highest observed prevalence in the highest decile of predicted risk, CHARGE-AF and MHS, showed an observed prevalence of AF of 1.6\% with a number needed to screen of 63. Selective screening of the 10\% highest risk identified 39\% of cases with AF. CONCLUSION: Prediction models can reliably identify individuals at high risk of AF. The best performing models showed an almost fourfold higher prevalence of AF by selective screening of individuals in the highest decile of risk compared with systematic screening of all cases. REGISTRATION: This systematic review was registered (PROSPERO CRD42019123847).},
  langid = {english},
  pmcid = {PMC8651014},
  pmid = {33624100},
  keywords = {Atrial fibrillation,Atrial Fibrillation,Cohort Studies,External validation,Humans,Predictive Value of Tests,Risk Assessment,Risk Factors,Risk prediction models,ROC Curve,Selective screening,Stroke},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6HWJ3ARB/Poorthuis et al_2021_Utility of risk prediction models to detect atrial fibrillation in screened.pdf}
}

@incollection{powell_direct_1994,
  title = {A Direct Search Optimization Method That Models the Objective and Constraint Functions by Linear Interpolation},
  booktitle = {Advances in Optimization and Numerical Analysis},
  author = {Powell, Michael JD},
  year = {1994},
  pages = {51--67},
  publisher = {Springer},
  annotation = {00000}
}

@article{powell_direct_1998,
  title = {Direct Search Algorithms for Optimization Calculations},
  author = {Powell, {\relax MJD}},
  year = {1998},
  journal = {Acta numerica},
  volume = {7},
  pages = {287--336},
  issn = {1474-0508},
  doi = {10.1017/S0962492900002841},
  annotation = {00000}
}

@article{powell_efficient_1964,
  title = {An Efficient Method for Finding the Minimum of a Function of Several Variables without Calculating Derivatives},
  author = {Powell, Michael JD},
  year = {1964},
  journal = {The computer journal},
  volume = {7},
  number = {2},
  pages = {155--162},
  issn = {0010-4620},
  doi = {10.1093/comjnl/7.2.155},
  annotation = {00000}
}

@article{powell_new_1970,
  title = {A New Algorithm for Unconstrained Optimization},
  author = {Powell, Michael JD},
  year = {1970},
  journal = {Nonlinear programming},
  pages = {31--65},
  doi = {10.1016/B978-0-12-597050-1.50006-3},
  annotation = {00000}
}

@article{powell_new_1970a,
  title = {A {{New Algorithm}} for {{Unconstrained Optimization}}},
  author = {Powell, Michael JD},
  year = {1970},
  journal = {Nonlinear programming},
  pages = {31--65},
  doi = {10/gfjwq4},
  annotation = {00551}
}

@incollection{powell_newuoa_2006,
  title = {The {{NEWUOA}} Software for Unconstrained Optimization without Derivatives},
  booktitle = {Large-{{Scale Nonlinear Optimization}}},
  author = {Powell, Michael J. D.},
  year = {2006},
  pages = {255--297},
  publisher = {Springer},
  annotation = {00000}
}

@article{powell_search_1973,
  title = {On Search Directions for Minimization Algorithms},
  author = {Powell, Michael JD},
  year = {1973},
  journal = {Mathematical Programming},
  volume = {4},
  number = {1},
  pages = {193--201},
  doi = {10.1007/BF01584660},
  annotation = {00000}
}

@article{powell_view_2007,
  title = {A View of Algorithms for Optimization without Derivatives},
  author = {Powell, Michael JD},
  year = {2007},
  journal = {Mathematics Today-Bulletin of the Institute of Mathematics and its Applications},
  volume = {43},
  number = {5},
  pages = {170--174},
  issn = {1361-2042},
  keywords = {ðŸ”No DOI found},
  annotation = {00000}
}

@article{prescott_improved_2021,
  title = {Improved {{Estimation}} of {{Concentration Under}} \${\textbackslash}ell\_p\$-{{Norm Distance Metrics Using Half Spaces}}},
  author = {Prescott, Jack and Zhang, Xiao and Evans, David},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.12913 [cs, stat]},
  eprint = {2103.12913},
  primaryclass = {cs, stat},
  urldate = {2021-08-06},
  abstract = {Concentration of measure has been argued to be the fundamental cause of adversarial vulnerability. Mahloujifar et al. presented an empirical way to measure the concentration of a data distribution using samples, and employed it to find lower bounds on intrinsic robustness for several benchmark datasets. However, it remains unclear whether these lower bounds are tight enough to provide a useful approximation for the intrinsic robustness of a dataset. To gain a deeper understanding of the concentration of measure phenomenon, we first extend the Gaussian Isoperimetric Inequality to non-spherical Gaussian measures and arbitrary \${\textbackslash}ell\_p\$-norms (\$p {\textbackslash}geq 2\$). We leverage these theoretical insights to design a method that uses half-spaces to estimate the concentration of any empirical dataset under \${\textbackslash}ell\_p\$-norm distance metrics. Our proposed algorithm is more efficient than Mahloujifar et al.'s, and our experiments on synthetic datasets and image benchmarks demonstrate that it is able to find much tighter intrinsic robustness bounds. These tighter estimates provide further evidence that rules out intrinsic dataset concentration as a possible explanation for the adversarial vulnerability of state-of-the-art classifiers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GDRWK5F3/Prescott et al. - 2021 - Improved Estimation of Concentration Under $ell_p.pdf;/Users/antoniohortaribeiro/Zotero/storage/NQUSXII8/2103.html}
}

@book{press_numerical_1992,
  title = {Numerical Recipes in {{C}}: The Art of Scientific Computing},
  shorttitle = {Numerical Recipes in {{C}}},
  editor = {Press, William H.},
  year = {1992},
  edition = {2nd ed},
  publisher = {Cambridge University Press},
  address = {Cambridge ; New York},
  isbn = {978-0-521-43108-8 978-0-521-43720-2},
  lccn = {QA76.73.C15 N865 1992},
  keywords = {C (Computer program language)},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AIHV6FRF/press_numerical_1992.pdf}
}

@book{prineas_minnesota_2009,
  title = {The {{Minnesota}} Code Manual of Electrocardiographic Findings},
  author = {Prineas, Ronald J and Crow, Richard S and Zhang, Zhu-Ming},
  year = {2009},
  publisher = {Springer Science \& Business Media},
  isbn = {1-84882-778-4}
}

@book{prineas_minnesota_2010,
  title = {The {{Minnesota Code Manual}} of {{Electrocardiographic Findings}}},
  author = {Prineas, Ronald J. and Crow, Richard S. and Zhang, Zhu-Ming},
  year = {2010},
  publisher = {Springer London},
  address = {London},
  doi = {10.1007/978-1-84882-778-3},
  urldate = {2020-07-08},
  isbn = {978-1-84882-777-6 978-1-84882-778-3},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WJA5WA4Y/Prineas et al. - 2010 - The Minnesota Code Manual of Electrocardiographic .pdf}
}

@article{qian_generalized_2017,
  title = {Generalized {{Hybrid Constructive Learning Algorithm}} for {{Multioutput RBF Networks}}},
  author = {Qian, X. and Huang, H. and Chen, X. and Huang, T.},
  year = {2017},
  month = nov,
  journal = {IEEE Transactions on Cybernetics},
  volume = {47},
  number = {11},
  pages = {3634--3648},
  issn = {2168-2267},
  doi = {10/gfwvbc},
  keywords = {Algorithm design and analysis,Approximation algorithms,compact network,computational complexity,Convergence,efficient generalized hybrid constructive learning algorithm,generalisation (artificial intelligence),generalization capability,generalized hidden matrix,Generalized hidden matrix,generalized hybrid constructive (GHC) learning algorithm,GHC learning algorithm,growing algorithm,improved incremental constructive scheme,initialization method,learning (artificial intelligence),least squares approximations,least-square method,Levenberg-Marquardt algorithm,LM training,local minima,matrix algebra,memory limitation problem,memory reduction,multioutput radial basis function (RBF) networks,multioutput radial basis function networks,multioutput RBF network training,Neurons,optimal network structure,optimisation,Optimization,pruning algorithm,radial basis function networks,Radial basis function networks,structured parameter optimization (SPO),structured parameter optimization algorithm,Training}
}

@article{quam_hierarchical_1984,
  title = {Hierarchical Warp Stereo},
  author = {Quam, Lynn H and Center, Artificial Intelligence},
  year = {1984},
  journal = {Readings in computer vision},
  pages = {80--86},
  keywords = {ðŸ”No DOI found},
  annotation = {00000}
}

@article{rabiner_tutorial_1989,
  title = {A Tutorial on Hidden {{Markov}} Models and Selected Applications in Speech Recognition},
  author = {Rabiner, L. R.},
  year = {1989},
  month = feb,
  journal = {Proceedings of the IEEE},
  volume = {77},
  number = {2},
  pages = {257--286},
  issn = {0018-9219},
  doi = {10.1109/5.18626},
  abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
  keywords = {balls-in-urns system,coin-tossing,discrete Markov chains,Distortion,ergodic models,Hidden Markov models,hidden states,left-right models,Markov processes,Mathematical model,Multiple signal classification,probabilistic function,signal processing,speech recognition,Statistical analysis,stochastic processes,Temperature measurement,Tutorial},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NM8ZAVM8/rabiner_a_1989.pdf;/Users/antoniohortaribeiro/Zotero/storage/8RMZ4BB5/18626.html}
}

@article{rackauckas_comparison_2018,
  title = {A {{Comparison}} of {{Automatic Differentiation}} and {{Continuous Sensitivity Analysis}} for {{Derivatives}} of {{Differential Equation Solutions}}},
  author = {Rackauckas, Christopher and Ma, Yingbo and Dixit, Vaibhav and Guo, Xingjian and Innes, Mike and Revels, Jarrett and Nyberg, Joakim and Ivaturi, Vijay},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.01892 [cs]},
  eprint = {1812.01892},
  primaryclass = {cs},
  urldate = {2018-12-14},
  abstract = {The derivatives of differential equation solutions are commonly used as model diagnostics and as part of parameter estimation routines. In this manuscript we investigate an implementation of Discrete local Sensitivity Analysis via Automatic Differentiation (DSAAD). A non-stiff Lotka-Volterra model, a discretization of the two dimensional (\$N {\textbackslash}times N\$) Brusselator stiff reaction-diffusion PDE, a stiff non-linear air pollution and a non-stiff pharmacokinetic/pharmacodynamic (PK/PD) model were used as prototype models for this investigation. Our benchmarks show that on sufficiently small ({$<$}100 parameters) stiff and non-stiff systems of ODEs, forward-mode DSAAD is more efficient than both reverse-mode DSAAD and continuous forward/adjoint sensitivity analysis. The scalability of continuous adjoint methods is shown to result in better efficiency for larger ODE systems such as PDE discretizations. In addition to testing efficiency, results on test equations demonstrate the applicability of DSAAD to differential-algebraic equations, delay differential equations, and hybrid differential equation systems where the event timing and effects are dependent on model parameters. Together, these results show that language-level automatic differentiation is an efficient method for calculating local sensitivities of a wide range of differential equation models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Numerical Analysis},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CPNILU3E/Rackauckas et al_2018_A Comparison of Automatic Differentiation and Continuous Sensitivity Analysis.pdf;/Users/antoniohortaribeiro/Zotero/storage/XQURYGZZ/1812.html}
}

@article{radford_improving_,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  pages = {12},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9NCXJQCY/radford_improving_.pdf}
}

@article{radford_improving_2018,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  pages = {12},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  keywords = {â›” No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8UX7GGYN/Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}

@article{radford_language_2019,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YPR7RXPV/radford_language_2019.pdf}
}

@article{radford_learning_2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  journal = {International Conference on Machine Learning (ICML)},
  abstract = {SOTA computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study performance on over 30 different computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers nontrivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/G97SWXHZ/Radford et al. - Learning Transferable Visual Models From Natural Language Supervision.pdf}
}

@article{raghunath_prediction_2020,
  title = {Prediction of Mortality from 12-Lead Electrocardiogram Voltage Data Using a Deep Neural Network},
  author = {Raghunath, Sushravya and Ulloa Cerna, Alvaro E. and Jing, Linyuan and {vanMaanen}, David P. and Stough, Joshua and Hartzel, Dustin N. and Leader, Joseph B. and Kirchner, H. Lester and Stumpe, Martin C. and Hafez, Ashraf and Nemani, Arun and Carbonati, Tanner and Johnson, Kipp W. and Young, Katelyn and Good, Christopher W. and Pfeifer, John M. and Patel, Aalpen A. and Delisle, Brian P. and Alsaid, Amro and Beer, Dominik and Haggerty, Christopher M. and Fornwalt, Brandon K.},
  year = {2020},
  month = may,
  journal = {Nature Medicine},
  issn = {1078-8956, 1546-170X},
  doi = {10.1038/s41591-020-0870-z},
  urldate = {2020-06-16},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2Y8FS4WP/Raghunath et al. - 2020 - Prediction of mortality from 12-lead electrocardio.pdf}
}

@article{rahhal_deep_2016,
  title = {Deep Learning Approach for Active Classification of Electrocardiogram Signals},
  author = {Rahhal, M.M. Al and Bazi, Yakoub and AlHichri, Haikel and Alajlan, Naif and Melgani, Farid and Yager, R.R.},
  year = {2016},
  month = jun,
  journal = {Information Sciences},
  volume = {345},
  pages = {340--354},
  issn = {00200255},
  doi = {10.1016/j.ins.2016.01.082},
  urldate = {2018-10-21},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BCRAG4KF/rahhal_deep_2016.pdf}
}

@incollection{rahimi_random_2008,
  title = {Random {{Features}} for {{Large-Scale Kernel Machines}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 20},
  author = {Rahimi, Ali and Recht, Benjamin},
  year = {2008},
  pages = {1177--1184},
  urldate = {2020-08-10},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YGQM43B5/Rahimi_Recht_2008_Random Features for Large-Scale Kernel Machines.pdf;/Users/antoniohortaribeiro/Zotero/storage/P5PC57DX/3182-random-features-for-large-scale-kernel-machines.html}
}

@inproceedings{rahimi_weighted_2009,
  title = {Weighted Sums of Random Kitchen Sinks: {{Replacing}} Minimization with Randomization in Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Rahimi, Ali and Recht, Benjamin},
  editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
  year = {2009},
  volume = {21},
  publisher = {Curran Associates, Inc.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/44MPSGZE/Rahimi and Recht - 2009 - Weighted sums of random kitchen sinks Replacing m.pdf}
}

@article{rahman_neural_2000,
  title = {Neural Network Approach for Linearizing Control of Nonlinear Process Plants},
  author = {Rahman, MHR Fazlur and Devanathan, Rajagopalan and Kuanyi, Zhu},
  year = {2000},
  journal = {IEEE Transactions on Industrial Electronics},
  volume = {47},
  number = {2},
  pages = {470--477},
  keywords = {ðŸ”No DOI found},
  annotation = {00000}
}

@article{rahman_neural_2000a,
  title = {Neural {{Network Approach}} for {{Linearizing Control}} of {{Nonlinear Process Plants}}},
  author = {Rahman, MHR Fazlur and Devanathan, Rajagopalan and Kuanyi, Zhu},
  year = {2000},
  journal = {IEEE Transactions on Industrial Electronics},
  volume = {47},
  number = {2},
  pages = {470--477},
  doi = {10/b7qwf3},
  annotation = {00030}
}

@article{raj_algorithmic_,
  title = {Algorithmic {{Stability}} of {{Heavy-Tailed Stochastic Gradient Descent}} on {{Least Squares}}},
  author = {Raj, Anant},
  abstract = {Recent studies have shown that heavy tails can emerge in stochastic optimization and that the heaviness of the tails have links to the generalization error. While these studies have shed light on interesting aspects of the generalization behavior in modern settings, they relied on strong topological and statistical regularity assumptions, which are hard to verify in practice. Furthermore, it has been empirically illustrated that the relation between heavy tails and generalization might not always be monotonic in practice, contrary to the conclusions of existing theory. In this study, we establish novel links between the tail behavior and generalization properties of stochastic gradient descent (SGD), through the lens of algorithmic stability. We consider a quadratic optimization problem and use a heavy-tailed stochastic differential equation (and its Euler discretization) as a proxy for modeling the heavy-tailed behavior emerging in SGD. We then prove uniform stability bounds, which reveal the following outcomes: (i) Without making any exotic assumptions, we show that SGD will not be stable if the stability is measured with the squared-loss x {$\rightarrow$} x2, whereas it in turn becomes stable if the stability is instead measured with a surrogate loss x {$\rightarrow$} {\textbar}x{\textbar}p with some p {$<$} 2. (ii) Depending on the variance of the data, there exists a `threshold of heavy-tailedness' such that the generalization error decreases as the tails become heavier, as long as the tails are lighter than this threshold. This suggests that the relation between heavy tails and generalization is not globally monotonic. (iii) We prove matching lower-bounds on uniform stability, implying that our bounds are tight in terms of the heaviness of the tails. We support our theory with synthetic and real neural network experiments.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8J7AT6A6/Raj - Algorithmic Stability of Heavy-Tailed Stochastic G.pdf}
}

@article{rajpurkar_ai_2022,
  title = {{{AI}} in Health and Medicine},
  author = {Rajpurkar, Pranav and Chen, Emma and Banerjee, Oishi and Topol, Eric J.},
  year = {2022},
  month = jan,
  journal = {Nature Medicine},
  volume = {28},
  pages = {31--38},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-021-01614-0},
  urldate = {2022-01-25},
  abstract = {Artificial intelligence (AI) is poised to broadly reshape medicine, potentially improving the experiences of both clinicians and patients. We discuss key findings from a 2-year weekly effort to track and share key developments in medical AI. We cover prospective studies and advances in medical image analysis, which have reduced the gap between research and deployment. We also address several promising avenues for novel medical AI research, including non-image data sources, unconventional problem formulations and human--AI collaboration. Finally, we consider serious technical and ethical challenges in issues spanning from data scarcity to racial bias. As these challenges are addressed, AI's potential may be realized, making healthcare more accurate, efficient and accessible for patients worldwide.},
  copyright = {2022 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Medical research},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Reviews\\
Subject\_term: Computational biology and bioinformatics;Medical research\\
Subject\_term\_id: computational-biology-and-bioinformatics;medical-research},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TXA6784N/Rajpurkar et al_2022_AI in health and medicine.pdf;/Users/antoniohortaribeiro/Zotero/storage/XTHIKUWX/s41591-021-01614-0.html}
}

@article{rajpurkar_cardiologistlevel_2017,
  title = {Cardiologist-{{Level Arrhythmia Detection}} with {{Convolutional Neural Networks}}},
  author = {Rajpurkar, Pranav and Hannun, Awni Y. and Haghpanahi, Masoumeh and Bourn, Codie and Ng, Andrew Y.},
  year = {2017},
  month = jul,
  journal = {arXiv:1707.01836},
  eprint = {1707.01836},
  abstract = {We develop an algorithm which exceeds the performance of board certified cardiologists in detecting a wide range of heart arrhythmias from electrocardiograms recorded with a single-lead wearable monitor. We build a dataset with more than 500 times the number of unique patients than previously studied corpora. On this dataset, we train a 34-layer convolutional neural network which maps a sequence of ECG samples to a sequence of rhythm classes. Committees of board-certified cardiologists annotate a gold standard test set on which we compare the performance of our model to that of 6 other individual cardiologists. We exceed the average cardiologist performance in both recall (sensitivity) and precision (positive predictive value).},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6IBEJBJX/rajpurkar_cardiologi_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/HRXCWE9T/rajpurkar_cardiologi_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/REQ8PPB5/rajpurkar_cardiologi_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/GBXJNUI3/1707.html;/Users/antoniohortaribeiro/Zotero/storage/SEDS2V5I/1707.html;/Users/antoniohortaribeiro/Zotero/storage/UWVAUE4B/1707.html}
}

@article{rakhlin_statistical_,
  title = {Statistical {{Learning}} and {{Sequential Prediction}}},
  author = {Rakhlin, Alexander and Sridharan, Karthik},
  pages = {259},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UE6TNU8D/Rakhlin and Sridharan - Statistical Learning and Sequential Prediction.pdf}
}

@incollection{rangapuram_deep_2018,
  title = {Deep {{State Space Models}} for {{Time Series Forecasting}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {7796--7805},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-13},
  file = {/Users/antoniohortaribeiro/Zotero/storage/J8GD4VRW/rangapuram_deep_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/G84ZUCY3/8004-deep-state-space-models-for-time-series-forecasting.html}
}

@article{rani_systematic_2018,
  title = {A {{Systematic Review}} of {{Compressive Sensing}}: {{Concepts}}, {{Implementations}} and {{Applications}}},
  shorttitle = {A {{Systematic Review}} of {{Compressive Sensing}}},
  author = {Rani, Meenu and Dhok, S. B. and Deshmukh, R. B.},
  year = {2018},
  journal = {IEEE Access},
  volume = {6},
  pages = {4875--4894},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2793851},
  abstract = {Compressive Sensing (CS) is a new sensing modality, which compresses the signal being acquired at the time of sensing. Signals can have sparse or compressible representation either in original domain or in some transform domain. Relying on the sparsity of the signals, CS allows us to sample the signal at a rate much below the Nyquist sampling rate. Also, the varied reconstruction algorithms of CS can faithfully reconstruct the original signal back from fewer compressive measurements. This fact has stimulated research interest toward the use of CS in several fields, such as magnetic resonance imaging, high-speed video acquisition, and ultrawideband communication. This paper reviews the basic theoretical concepts underlying CS. To bridge the gap between theory and practicality of CS, different CS acquisition strategies and reconstruction approaches are elaborated systematically in this paper. The major application areas where CS is currently being used are reviewed here. This paper also highlights some of the challenges and research directions in this field.},
  keywords = {compressed sensing,Compressed sensing,compressible representation,compressive measurements,compressive sensing,Compressive sensing,CS acquisition strategies,CS applications,CS reconstruction algorithms,Image reconstruction,Mathematical model,Nyquist sampling rate,OMP,random demodulator,Reconstruction algorithms,sensing modality,Sensors,signal reconstruction,signal representation,signal sampling,Sparse matrices,sparse representation,sparsity,systematic review,transform domain,transforms,Transforms,varied reconstruction algorithms},
  file = {/Users/antoniohortaribeiro/Zotero/storage/G5U9APDX/Rani et al. - 2018 - A Systematic Review of Compressive Sensing Concep.pdf;/Users/antoniohortaribeiro/Zotero/storage/ADURQHQK/8260873.html}
}

@book{rao_engineering_2009,
  title = {Engineering Optimization: Theory and Practice},
  shorttitle = {Engineering Optimization},
  author = {Rao, Singiresu S.},
  year = {2009},
  edition = {4th ed},
  publisher = {John Wiley \& Sons},
  address = {Hoboken, N.J},
  isbn = {978-0-470-18352-6},
  lccn = {TA342 .R36 2009},
  keywords = {Engineering,Mathematical models,Mathematical optimization},
  annotation = {OCLC: ocn320352991},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CWAW6XB2/rao_engineerin_2009.pdf}
}

@book{rashid_power_2011,
  title = {Power Electronics Handbook: Devices, Circuits, and Applications Handbook},
  shorttitle = {Power Electronics Handbook},
  editor = {Rashid, Muhammad H.},
  year = {2011},
  edition = {3rd ed},
  publisher = {Elsevier},
  address = {Burlington, MA},
  abstract = {"Designed to appeal to a new generation of engineering professionals, Power Electronics Handbook, 3rd Edition features four new chapters covering renewable energy, energy transmission, energy storage, as well as an introduction to Distributed and Cogeneration (DCG) technology, including gas turbines, gensets, microturbines, wind turbines, variable speed generators, photovoltaics and fuel cells, has been gaining momentum for quite some time now.smart grid technology. With this book readers should be able to provide technical design leadership on assigned power electronics design projects and lead the design from the concept to production involving significant scope and complexity"--},
  isbn = {978-0-12-382036-5},
  lccn = {TK7881.15 .P6733 2011},
  keywords = {Encyclopedias,Power electronics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FUUT4U6U/rashid_power_2011.pdf}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-18253-9},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WNFAJN95/rasmussen_gaussian_2006.pdf}
}

@article{rassi_american_2012,
  title = {American {{Trypanosomiasis}} ({{Chagas Disease}})},
  author = {Rassi, Anis and Rassi, Anis and {Marcondes de Rezende}, Joffre},
  year = {2012},
  month = jun,
  journal = {Infectious Disease Clinics of North America},
  series = {Tropical {{Diseases}}},
  volume = {26},
  number = {2},
  pages = {275--291},
  issn = {0891-5520},
  doi = {10.1016/j.idc.2012.03.002},
  urldate = {2021-11-25},
  langid = {english},
  keywords = {American trypanosomiasis,Chagas disease,Chagas heart disease,Epidemiology,Treatment},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ADB6IFAB/S0891552012000116.html}
}

@article{rautaharju_aha_2009,
  title = {{{AHA}}/{{ACCF}}/{{HRS Recommendations}} for the {{Standardization}} and {{Interpretation}} of the {{Electrocardiogram}}: {{Part IV}}: {{The ST Segment}}, {{T}} and {{U Waves}}, and the {{QT Interval A Scientific Statement From}} the {{American Heart Association Electrocardiography}} and {{Arrhythmias Committee}}, {{Council}} on {{Clinical Cardiology}}; the {{American College}} of {{Cardiology Foundation}}; and the {{Heart Rhythm Society Endorsed}} by the {{International Society}} for {{Computerized Electrocardiology}}},
  author = {Rautaharju, Pentti M. and Surawicz, Borys and Gettes, Leonard S.},
  year = {2009},
  month = mar,
  journal = {Journal of the American College of Cardiology},
  volume = {53},
  number = {11},
  pages = {982--991},
  issn = {0735-1097},
  doi = {10/c7vp73},
  keywords = {ACCF Expert Consensus Documents,electrocardiography,electrophysiology,ion channels,long-QT syndrome}
}

@article{recht_tour_2018,
  title = {A {{Tour}} of {{Reinforcement Learning}}: {{The View}} from {{Continuous Control}}},
  shorttitle = {A {{Tour}} of {{Reinforcement Learning}}},
  author = {Recht, Benjamin},
  year = {2018},
  month = jun,
  urldate = {2018-09-19},
  langid = {english},
  keywords = {ðŸ”No DOI found},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/32EZW2TD/recht_a tour of_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/C9IDK5DR/1806.html}
}

@inproceedings{reddi_convergence_2018,
  title = {On the Convergence of Adam and Beyond},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
  year = {2018},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SZZ4K97U/reddi_on the_2018.pdf}
}

@article{redmon_yolo9000_2016,
  title = {{{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  shorttitle = {{{YOLO9000}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  year = {2016},
  month = dec,
  journal = {arXiv:1612.08242 [cs]},
  eprint = {1612.08242},
  primaryclass = {cs},
  abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QTA4E2IR/redmon_yolo9000_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/ZVJ7KIM8/1612.html}
}

@inproceedings{redmon_you_2016,
  title = {You Only Look Once: {{Unified}}, Real-Time Object Detection},
  shorttitle = {You Only Look Once},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  pages = {779--788},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/28ACWJC5/redmon_you only_2016.pdf}
}

@article{redmond_datadriven_2002,
  title = {A Data-Driven Software Tool for Enabling Cooperative Information Sharing among Police Departments},
  author = {Redmond, Michael and Baveja, Alok},
  year = {2002},
  month = sep,
  journal = {European Journal of Operational Research},
  volume = {141},
  number = {3},
  pages = {660--678},
  issn = {03772217},
  doi = {10.1016/S0377-2217(01)00264-8},
  urldate = {2024-05-21},
  abstract = {Semantic Scholar extracted view of "A data-driven software tool for enabling cooperative information sharing among police departments" by Michael Redmond et al.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@inproceedings{rehmer_using_2019,
  title = {On {{Using Gated Recurrent Units}} for {{Nonlinear System Identification}}},
  booktitle = {2019 18th {{European Control Conference}} ({{ECC}})},
  author = {Rehmer, Alexander and Kroll, Andreas},
  year = {2019},
  month = jun,
  pages = {2504--2509},
  doi = {10.23919/ECC.2019.8795631},
  abstract = {During recent years Deep Learning (DL) methods facilitated impressive progress on various fields of research: Deep Convolutional Neural Networks (CNN) enabled object classification with to this day unmatched precision, while state of the art results in speech recognition and natural language processing (NLP) were achieved via gated units such as the LSTM. Although recurrent neural network architectures are long established in the field of system identification as a realization of an internal dynamics approach [1] [2], little research has yet been dedicated towards gated units. The purpose of this paper is to evaluate the architectures of recurrent gated units from the viewpoint of system identification and test their performance on a nonlinear system identification task.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DZP9J2KI/Rehmer_Kroll_2019_On Using Gated Recurrent Units for Nonlinear System Identification.pdf;/Users/antoniohortaribeiro/Zotero/storage/U2KIRFBK/8795631.html}
}

@article{relan_datadriven_2017,
  title = {Data-{{Driven Nonlinear Identification}} of {{Li-Ion Battery Based}} on a {{Frequency Domain Nonparametric Analysis}}},
  author = {Relan, R. and Firouz, Y. and Timmermans, J. M. and Schoukens, J.},
  year = {2017},
  month = sep,
  journal = {IEEE Transactions on Control Systems Technology},
  volume = {25},
  number = {5},
  pages = {1825--1832},
  issn = {1063-6536},
  doi = {10.1109/TCST.2016.2616380},
  abstract = {Lithium ion batteries are attracting significant and growing interest, because their high energy and high power density render them an excellent option for energy storage, particularly in hybrid and electric vehicles. In this brief, a data-driven polynomial nonlinear state-space model is proposed for the operating points at the cusp of linear and nonlinear regimes of the battery's electrical operation, based on the thorough nonparametric frequency domain characterization and quantification of the battery's behavior in terms of its linear and nonlinear behavior at different levels of the state of charge.},
  keywords = {Analytical models,Batteries,Computational modeling,Frequency-domain analysis,Hidden Markov models,Input--output response,Integrated circuit modeling,lithium ion (Li-ion) battery,Mathematical model,Nonlinear system identification,nonparametric characterization,polynomial nonlinear state space (PNLSS)},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2KCNG2FD/relan_data-drive_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/DW2J6ZGQ/relan_data-drive_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/5443F4PI/7726050.html;/Users/antoniohortaribeiro/Zotero/storage/7KEPEN6F/7726050.html;/Users/antoniohortaribeiro/Zotero/storage/9JRAHQF4/7726050.html;/Users/antoniohortaribeiro/Zotero/storage/GADHRDJY/7726050.html}
}

@inproceedings{ren_faster_2015,
  title = {Faster {{R-CNN}}: {{Towards}} Real-Time Object Detection with Region Proposal Networks},
  shorttitle = {Faster {{R-CNN}}},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2015},
  pages = {91--99},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WNW2BQBH/ren_faster_2015.pdf}
}

@article{renda_comparing_2020,
  title = {Comparing {{Rewinding}} and {{Fine-tuning}} in {{Neural Network Pruning}}},
  author = {Renda, Alex and Frankle, Jonathan and Carbin, Michael},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.02389 [cs, stat]},
  eprint = {2003.02389},
  primaryclass = {cs, stat},
  urldate = {2020-06-29},
  abstract = {Many neural network pruning algorithms proceed in three steps: train the network to completion, remove unwanted structure to compress the network, and retrain the remaining structure to recover lost accuracy. The standard retraining technique, fine-tuning, trains the unpruned weights from their final trained values using a small fixed learning rate. In this paper, we compare fine-tuning to alternative retraining techniques. Weight rewinding (as proposed by Frankle et al., (2019)), rewinds unpruned weights to their values from earlier in training and retrains them from there using the original training schedule. Learning rate rewinding (which we propose) trains the unpruned weights from their final values using the same learning rate schedule as weight rewinding. Both rewinding techniques outperform fine-tuning, forming the basis of a network-agnostic pruning algorithm that matches the accuracy and compression ratios of several more network-specific state-of-the-art techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/K8XJZHGP/Renda et al. - 2020 - Comparing Rewinding and Fine-tuning in Neural Netw.pdf;/Users/antoniohortaribeiro/Zotero/storage/3R8BLQ6C/2003.html}
}

@article{reyland_generalized_2014,
  title = {Generalized {{Wiener}} System Identification: General Backlash Nonlinearity and Finite Impulse Response Linear Part},
  author = {Reyland, John and Bai, Er-Wei},
  year = {2014},
  journal = {International Journal of Adaptive Control and Signal Processing},
  volume = {28},
  number = {11},
  pages = {1174--1188},
  issn = {1099-1115},
  doi = {10.1002/acs.2437},
  annotation = {00000}
}

@article{ribeiro_automatic_2018,
  title = {Automatic {{Diagnosis}} of {{Short-Duration}} 12-{{Lead ECG}} Using a {{Deep Convolutional Network}}},
  author = {Ribeiro, Ant{\^o}nio H. and Ribeiro, Manoel Horta and Paix{\~a}o, Gabriela and Oliveira, Derick and Gomes, Paulo R. and Canazart, J{\'e}ssica A. and Pifano, Milton and Meira Jr., Wagner and Sch{\"o}n, Thomas B. and Ribeiro, Antonio Luiz},
  year = {2018},
  journal = {Machine Learning for Health (ML4H) Workshop at NeurIPS},
  eprint = {1811.12194},
  abstract = {We present a model for predicting electrocardiogram (ECG) abnormalities in short-duration 12-lead ECG signals which outperformed medical doctors on the 4th year of their cardiology residency. Such exams can provide a full evaluation of heart activity and have not been studied in previous end-to-end machine learning papers. Using the database of a large telehealth network, we built a novel dataset with more than 2 million ECG tracings, orders of magnitude larger than those used in previous studies. Moreover, our dataset is more realistic, as it consist of 12-lead ECGs recorded during standard in-clinics exams. Using this data, we trained a residual neural network with 9 convolutional layers to map 7 to 10 second ECG signals to 6 classes of ECG abnormalities. Future work should extend these results to cover a large range of ECG abnormalities, which could improve the accessibility of this diagnostic tool and avoid wrong diagnosis from medical doctors.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CYBJ2QLB/ribeiro_automatic_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/DAFGE2QL/1811.html}
}

@inproceedings{ribeiro_automatic_2020,
  title = {Automatic 12-Lead {{ECG}} Classification Using a Convolutional Network Ensemble},
  booktitle = {Computing in {{Cardiology}} ({{CinC}})},
  author = {Ribeiro, Antonio H and Gedon, Daniel and Teixeira, Daniel Martins and Ribeiro, Manoel Horta and Ribeiro, Antonio L Pinho and Schon, Thomas B and Jr, Wagner Meira},
  year = {2020},
  doi = {10.22489/CinC.2020.130},
  abstract = {The 12-lead electrocardiogram (ECG) is a major diagnostic test for cardiovascular diseases and enhanced automated analysis tools might lead to more reliable diagnosis and improved clinical practice. Deep neural networks are models composed of stacked transformations that learn tasks by examples. Inspired by the success of these models in computer vision, we propose an end-to-end approach for the task at hand. We trained deep convolutional neural network models in the heterogeneous dataset provided in the Physionet 2020 Challenge and used an ensemble of seven of these convolutional models for the classification of abnormalities present in the ECG records. Ensembles use the output of multiple models to generate a combined prediction and are known to improve performance and generalization when compared to the individual models. In our submission, we use an ensemble of neural networks with the architecture similar to the one described in Nat Commun 11, 1760 (2020) for 12-lead ECGs classification. On the partially hidden test dataset from the challenge, the best-scored entry for our team (the ``Code Team'') had a performance of 0.657, which place us in the 7-th place team-wise in the challenge leaderboard.},
  copyright = {All rights reserved},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ETK5JRDA/130_CinCFinalPDF.pdf}
}

@article{ribeiro_automatic_2020a,
  title = {Automatic Diagnosis of the 12-Lead {{ECG}} Using a Deep Neural Network},
  author = {Ribeiro, Ant{\^o}nio H. and Ribeiro, Manoel Horta and Paix{\~a}o, Gabriela M. M. and Oliveira, Derick M. and Gomes, Paulo R. and Canazart, J{\'e}ssica A. and Ferreira, Milton P. S. and Andersson, Carl R. and Macfarlane, Peter W. and Meira Jr., Wagner and Sch{\"o}n, Thomas B. and Ribeiro, Antonio Luiz P.},
  year = {2020},
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  eprint = {1904.01949},
  pages = {1760},
  doi = {10.1038/s41467-020-15432-4},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FXKPXRYL/ribeiro_automatic_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/NGL48WY6/1904.html}
}

@article{ribeiro_deep_2019,
  title = {Deep {{Convolutional Networks}} Are {{Useful}} in {{System Identification}}},
  author = {Ribeiro, Antonio H and Andersson, Carl and Tiels, Koen and Wahlstrom, Niklas and Schon, Thomas B},
  year = {2019},
  journal = {Workshop on Nonlinear System Identification},
  copyright = {All rights reserved},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VP2VC28Y/2019-NonLinWorkshop.pdf}
}

@inproceedings{ribeiro_exploding_2020,
  title = {Beyond Exploding and Vanishing Gradients: Attractors and Smoothness in the Analysis of Recurrent Neural Network Training},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Ribeiro, Ant{\^o}nio H. and Tiels, Koen and Aguirre, Luis A. and Sch{\"o}n, Thomas B.},
  year = {2020},
  volume = {108},
  eprint = {1906.08482},
  pages = {2370--2380},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Dynamical Systems,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BZFY2G6Q/ribeiro_the_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/Y8W8BKZM/1906.html}
}

@inproceedings{ribeiro_how_2021,
  title = {How Convolutional Neural Networks Deal with Aliasing},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ribeiro, Antonio H. and Schon, Thomas B.},
  year = {2021},
  pages = {2755--2759},
  publisher = {IEEE},
  doi = {10.1109/ICASSP39728.2021.9414627}
}

@phdthesis{ribeiro_implementacao_2015,
  type = {{{BSc Thesis}}},
  title = {Implementa{\c c}{\~a}o de Uma {{C{\^a}mera Est{\'e}reo}}},
  author = {Ribeiro, Antonio H.},
  year = {2015},
  month = dec,
  address = {Belo Horizonte, Brazil},
  abstract = {A stereo camera is a camera that simultaneously captures two or more images  in order to estimate depth. A stereo camera prototype was implemented using two image sensors. Its project was based on v200, a commercial camera produced by Invent Vision (iVision), company for which this prototype was developed. The major steps needed to estimate depth using a stereo camera were followed. They are: calibration (estimation of camera geometry and parameters), retification (aligning images), correspondence (finding correspondent points on the two images and the distance between them) and reconstruction (estimation of features depth using triangulation). All algorithms needed were implemented using C++ and integrated to the prototype. The prototype is able to estimate depth in a range of 0.7 to 20 meters, it has a resolution on the order of a few centimeters and can reach a frame rate of 5 frames per second. The industrial camera v200 has all its internal processing computed by a FPGA. It is suggested, as a future work, to use the FPGA to speed up the retification and correspondence algorithms. The aim of this work was to take a first step towards a stereo camera based on v200 to be a competitive commercial product, apt to be sold by iVision.},
  copyright = {All rights reserved},
  school = {Universidade Federal de Minas Gerais},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PJV2THKW/H. Ribeiro - 2015 - ImplementaÃ§Ã£o de uma CÃ¢mera EstÃ©reo.pdf}
}

@inproceedings{ribeiro_lasso_2018,
  title = {Lasso {{Regularization Paths}} for {{NARMAX Models}} via {{Coordinate Descent}}},
  booktitle = {2018 {{Annual American Control Conference}} ({{ACC}})},
  author = {Ribeiro, Antonio H. and Aguirre, Luis A.},
  year = {2018-06-27/2018-06-29},
  pages = {5268--5273},
  doi = {10.23919/ACC.2018.8430924},
  copyright = {All rights reserved},
  isbn = {2378-5861},
  keywords = {Approximation algorithms,autoregressive moving average processes,Computational modeling,Data models,entire regularization path,error regressors,Estimation,L1-norm penalty,Lasso estimation,Lasso regularization paths,linear combination,linear models,Mathematical model,Minimization,NARMAX models,nonlinear regression problem,Optimization,polynomial models,regression analysis,regressor matrix},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9CXGF7RJ/Ribeiro and Aguirre - 2018 - Lasso Regularization Paths for NARMAX Models via C.pdf;/Users/antoniohortaribeiro/Zotero/storage/TUFIHJCL/Ribeiro and Aguirre - 2018 - Lasso Regularization Paths for NARMAX Models via C.pdf}
}

@phdthesis{ribeiro_learning_2020,
  title = {Learning Nonlinear Differentiable Models for Signals and Systems:  With Applications},
  author = {Ribeiro, Ant{\^o}nio H.},
  year = {2020},
  address = {Belo Horizonte, Brazil},
  copyright = {All rights reserved},
  school = {Universidade Federal de Minas Gerais},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BIJ737DW/1480D.pdf}
}

@inproceedings{ribeiro_occam_2021a,
  title = {Beyond {{Occam}}'s {{Razor}} in {{System Identification}}: {{Double-Descent}} When {{Modeling Dynamics}}},
  booktitle = {Workshop on {{Nonlinear System Identification}}},
  author = {Ribeiro, Ant{\^o}nio H. and Hendriks, Johannes N. and Wills, Adrian G. and Sch{\"o}n, Thomas B.},
  year = {2021},
  copyright = {All rights reserved},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4SNUJR3C/Ribeiro et al. - 2020 - Beyond Occam's Razor in System Identification Dou.pdf;/Users/antoniohortaribeiro/Zotero/storage/9QU3I45U/16-432.pdf;/Users/antoniohortaribeiro/Zotero/storage/9YG2LA47/2012.html}
}

@inproceedings{ribeiro_occam_2021b,
  title = {Beyond {{Occam}}'s {{Razor}} in {{System Identification}}: {{Double-Descent}} When {{Modeling Dynamics}}},
  shorttitle = {Beyond {{Occam}}'s {{Razor}} in {{System Identification}}},
  booktitle = {{{IFAC Symposium}} on {{System Identification}} ({{SYSID}})},
  author = {Ribeiro, Ant{\^o}nio H. and Hendriks, Johannes N. and Wills, Adrian G. and Sch{\"o}n, Thomas B.},
  year = {2021},
  volume = {54},
  eprint = {2012.06341},
  pages = {97--102},
  doi = {10.1016/j.ifacol.2021.08.341},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DBQLMKFP/Ribeiro et al. - 2020 - Beyond Occam's Razor in System Identification Dou.pdf;/Users/antoniohortaribeiro/Zotero/storage/FRQQWJVZ/2012.html}
}

@article{ribeiro_overparameterized_2023,
  title = {Overparameterized {{Linear Regression}} under {{Adversarial Attacks}}},
  author = {Ribeiro, Ant{\^o}nio H. and Sch{\"o}n, Thomas B.},
  year = {2023},
  journal = {IEEE Transactions on Signal Processing},
  eprint = {2204.06274},
  doi = {10.1109/TSP.2023.3246228},
  urldate = {2022-04-22},
  abstract = {As machine learning models start to be used in critical applications, their vulnerabilities and brittleness become a pressing concern. Adversarial attacks are a popular framework for studying these vulnerabilities. In this work, we study the error of linear regression in the face of adversarial attacks. We provide bounds of the error in terms of the traditional risk and the parameter norm and show how these bounds can be leveraged and make it possible to use analysis from non-adversarial setups to study the adversarial risk. The usefulness of these results is illustrated by shedding light on whether or not overparameterized linear models can be adversarially robust. We show that adding features to linear models might be either a source of additional robustness or brittleness. We show that these differences appear due to scaling and how the \${\textbackslash}ell\_1\$ and \${\textbackslash}ell\_2\$ norms of random projections concentrate. We also show how the reformulation we propose allows for solving adversarial training as a convex optimization problem. This is then used as a tool to study how adversarial training and other regularization methods might affect the robustness of the estimated models.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/56J77WTJ/Ribeiro_SchÃ¶n_2023_Overparameterized Linear Regression under Adversarial Attacks.pdf}
}

@inproceedings{ribeiro_overparametrized_2021,
  title = {Overparametrized {{Regression Under L2 Adversarial Attacks}}},
  booktitle = {Workshop on the {{Theory}} of {{Overparameterized Machine Learning}} ({{TOPML}})},
  author = {Ribeiro, Antonio H and Sch{\"o}n, Thomas B},
  year = {2021},
  month = apr,
  copyright = {All rights reserved}
}

@article{ribeiro_parallel_2017,
  title = {"{{Parallel Training Considered Harmful}}?": {{Comparing Series-Parallel}} and {{Parallel Feedforward Network Training}}},
  shorttitle = {"{{Parallel Training Considered Harmful}}?},
  author = {Ribeiro, Ant{\^o}nio H. and Aguirre, Luis A.},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.07119 [cs]},
  eprint = {1706.07119},
  eprintclass = {cs},
  primaryclass = {cs},
  abstract = {Neural network models for dynamic systems can be trained either in parallel or in series-parallel configurations. Influenced by early arguments, several papers justify the choice of series-parallel rather than parallel configuration claiming it has a lower computational cost, better stability properties during training and provides more accurate results. The purpose of this work is to review some of those arguments and to present both methods in an unifying framework, showing that parallel and series-parallel training actually results from optimal predictors that use different noise models. A numerical example illustrate that each method provides better results when the noise model they implicit consider are consistent with the error in the data. Furthermore, it is argued that for feedforward networks with bounded activation functions the possible lack of stability does not jeopardize the training; and, a novel complexity analysis indicates the computational cost in the two configurations is not significantly different. This is confirmed through numerical examples.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Learning,Computer Science - Systems and Control},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4T8M8MFN/1706.html;/Users/antoniohortaribeiro/Zotero/storage/CNRADJDE/1706.html;/Users/antoniohortaribeiro/Zotero/storage/DF5VHJJE/1706.html}
}

@article{ribeiro_parallel_2018,
  title = {''{{Parallel Training Considered Harmful}}?'': {{Comparing}} Series-Parallel and Parallel Feedforward Network Training},
  author = {Ribeiro, Ant{\^o}nio H. and Aguirre, Luis A.},
  year = {2018},
  month = nov,
  journal = {Neurocomputing},
  volume = {316},
  pages = {222--231},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2018.07.071},
  abstract = {Neural network models for dynamic systems can be trained either in parallel or in series-parallel configurations. Influenced by early arguments, several papers justify the choice of series-parallel rather than parallel configuration claiming it has a lower computational cost, better stability properties during training and provides more accurate results. Other published results, on the other hand, defend parallel training as being more robust and capable of yielding more accurate long-term predictions. The main contribution of this paper is to present a study comparing both methods under the same unified framework with special attention to three aspects: (i)~robustness of the estimation in the presence of noise; (ii)~computational cost; and, (iii)~convergence. A unifying mathematical framework and simulation studies show situations where each training method provides superior validation results and suggest that parallel training is generally better in more realistic scenarios. An example using measured data seems to reinforce such a claim. Complexity analysis and numerical examples show that both methods have similar computational cost although series-parallel training is more amenable to parallelization. Some informal discussion about stability and convergence properties is presented and explored in the examples.},
  copyright = {All rights reserved},
  keywords = {Neural network,Output error models,Parallel training,Series-parallel training,System identification},
  file = {/Users/antoniohortaribeiro/Zotero/storage/C6LUNPTK/Ribeiro and Aguirre - 2018 - ''Parallel Training Considered Harmful'' Compari.pdf;/Users/antoniohortaribeiro/Zotero/storage/MC9RWQP3/S0925231218309068.html}
}

@phdthesis{ribeiro_recurrent_2017,
  type = {{{MSc Dissertation}}},
  title = {Recurrent {{Structures}} in {{System Identification}}},
  author = {Ribeiro, Ant{\^o}nio H.},
  year = {2017},
  address = {Belo Horizonte, Brazil},
  copyright = {All rights reserved},
  school = {Universidade Federal de Minas Gerais},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9D6BFDJA/1480M.pdf}
}

@article{ribeiro_regularization_2023,
  title = {Regularization Properties of Adversarially-Trained Linear Regression},
  author = {Ribeiro, Ant{\^o}nio H and Zachariah, Dave and Bach, Francis and Sch{\"o}n, Thomas B.},
  year = {2023},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{ribeiro_relacoes_2014,
  title = {Rela{\c c}oes {{Est{\'a}ticas}} de {{Modelos NARX MISO}} e Sua {{Representa{\c c}ao}} de {{Hammerstein}}},
  booktitle = {{{XX Congresso Brasileiro}} de {{Autom{\'a}tica}}},
  author = {Ribeiro, Ant{\^o}nio H. and Aguirre, Luis A.},
  year = {2014},
  copyright = {All rights reserved},
  file = {/Users/antoniohortaribeiro/Zotero/storage/T2KW8WFU/ribeiro_relaÃ§oes_2014.pdf}
}

@article{ribeiro_selecting_2015,
  title = {Selecting Transients Automatically for the Identification of Models for an Oil Well},
  author = {Ribeiro, Ant{\^o}nio H. and Aguirre, Luis A.},
  year = {2015},
  journal = {IFAC Workshop on Automatic Control in Offshore Oil and Gas Production},
  volume = {48},
  number = {6},
  pages = {154--158},
  doi = {10.1016/j.ifacol.2015.08.024},
  copyright = {All rights reserved},
  keywords = {Automatic transient selection,intelligent oil fields,soft sensors,system identification},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2FJ9YGMR/S2405896315008915.html;/Users/antoniohortaribeiro/Zotero/storage/7674TESZ/S2405896315008915.html;/Users/antoniohortaribeiro/Zotero/storage/QD5XETR6/S2405896315008915.html;/Users/antoniohortaribeiro/Zotero/storage/WGHTMZK9/S2405896315008915.html;/Users/antoniohortaribeiro/Zotero/storage/YQTYLTSZ/S2405896315008915.html}
}

@article{ribeiro_shooting_2017,
  title = {Shooting {{Methods}} for {{Parameter Estimation}} of {{Output Error Models}}},
  author = {Ribeiro, Ant{\^o}nio H. and Aguirre, Luis A.},
  year = {2017},
  month = jul,
  journal = {IFAC World Congress},
  volume = {50},
  number = {1},
  pages = {13998--14003},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2017.08.2421},
  abstract = {This paper studies parameter estimation of output error (OE) models. The commonly used approach of minimizing the free-run simulation error is called single shooting in contrast with the new multiple shooting approach proposed in this paper, for which the free-run simulation error of sub-datasets is minimized subject to equality constraints. The names ``single shooting'' and ``multiple shooting'' are used due to the similarities with techniques for estimating ODE (ordinary differential equation) parameters. Examples with nonlinear polynomial models illustrate the advantages of OE models as well as the capability of the multiple shooting approach to avoid undesirable local minima.},
  copyright = {All rights reserved},
  keywords = {Multiple shooting,nonlinear least-squares,output error models,simulation error minimization},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9TG88I35/ribeiro_shooting_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/JKXXZ9Z4/S2405896317332469.html}
}

@article{ribeiro_smoothness_2020,
  title = {On the Smoothness of Nonlinear System Identification},
  author = {Ribeiro, Ant{\^o}nio H. and Tiels, Koen and Umenberger, Jack and Sch{\"o}n, Thomas B. and Aguirre, Luis A.},
  year = {2020},
  month = nov,
  journal = {Automatica},
  volume = {121},
  eprint = {1905.00820},
  pages = {109158},
  doi = {10.1016/j.automatica.2020.109158},
  abstract = {We shed new light on the smoothness of optimization problems arising in prediction error parameter estimation of linear and nonlinear systems. We show that for regions of the parameter space where the model is not contractive, the Lipschitz constant and beta-smoothness of the objective function might blow up exponentially with the simulation length, making it hard to numerically find minima within those regions or, even, to escape from them. In addition to providing theoretical understanding of this problem, this paper also proposes the use of multiple shooting as a viable solution. The proposed method minimizes the error between a prediction model and the observed values. Rather than running the prediction model over the entire dataset, multiple shooting splits the data into smaller subsets and runs the prediction model over each subset, making the simulation length a design parameter and making it possible to solve problems that would be infeasible using a standard approach. The equivalence to the original problem is obtained by including constraints in the optimization. The new method is illustrated by estimating the parameters of nonlinear systems with chaotic or unstable behavior, as well as neural networks. We also present a comparative analysis of the proposed method with multi-step-ahead prediction error minimization.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RK4QYMEL/Ribeiro et al. - 2020 - On the smoothness of nonlinear system identificati.pdf;/Users/antoniohortaribeiro/Zotero/storage/V3Z475ST/1905.html}
}

@article{ribeiro_surprises_2022,
  title = {Surprises in Adversarially-Trained Linear Regression},
  author = {Ribeiro, Ant{\^o}nio H. and Zachariah, Dave and Sch{\"o}n, Thomas B.},
  year = {2022},
  month = may,
  journal = {arXiv:2205.12695},
  eprint = {2205.12695},
  doi = {10.48550/arXiv.2205.12695},
  urldate = {2022-05-26},
  abstract = {State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is one of the most effective approaches to defend against such examples. We show that for linear regression problems, adversarial training can be formulated as a convex problem. This fact is then used to show that \${\textbackslash}ell\_{\textbackslash}infty\$-adversarial training produces sparse solutions and has many similarities to the lasso method. Similarly, \${\textbackslash}ell\_2\$-adversarial training has similarities with ridge regression. We use a robust regression framework to analyze and understand these similarities and also point to some differences. Finally, we show how adversarial training behaves differently from other regularization methods when estimating overparameterized models (i.e., models with more parameters than datapoints). It minimizes a sum of three terms which regularizes the solution, but unlike lasso and ridge regression, it can sharply transition into an interpolation mode. We show that for sufficiently many features or sufficiently small regularization parameters, the learned model perfectly interpolates the training data while still exhibiting good out-of-sample performance.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/T23TMMN3/Ribeiro et al. - 2022 - Surprises in adversarially-trained linear regressi.pdf;/Users/antoniohortaribeiro/Zotero/storage/ZE8NSPKI/2205.html}
}

@article{ribeiro_teleelectrocardiography_2019,
  title = {Tele-Electrocardiography and Bigdata: {{The CODE}} ({{Clinical Outcomes}} in {{Digital Electrocardiography}}) Study},
  author = {Ribeiro, Antonio Luiz P. and Paix{\~a}o, Gabriela M. M. and Gomes, Paulo R. and Ribeiro, Manoel Horta and Ribeiro, Ant{\^o}nio H. and Canazart, J{\'e}ssica A. and Oliveira, Derick M. and Ferreira, Milton P. and Lima, Emilly M. and {de Moraes}, Jermana Lopes and Castro, Nathalia and Ribeiro, Leonardo B. and MacFarlane, Peter W.},
  year = {2019},
  month = sep,
  journal = {Journal of Electrocardiology},
  issn = {0022-0736},
  doi = {10/gf7pwg},
  abstract = {Digital electrocardiographs are now widely available and a large number of digital electrocardiograms (ECGs) have been recorded and stored. The present study describes the development and clinical applications of a large database of such digital ECGs, namely the CODE (Clinical Outcomes in Digital Electrocardiology) study. ECGs obtained by the Telehealth Network of Minas Gerais, Brazil, from 2010 to 17, were organized in a structured database. A hierarchical free-text machine learning algorithm recognized specific ECG diagnoses from cardiologist reports. The Glasgow ECG Analysis Program provided Minnesota Codes and automatic diagnostic statements. The presence of a specific ECG abnormality was considered when both automatic and medical diagnosis were concordant; cases of discordance were decided using heuristisc rules and manual review. The ECG database was linked to the national mortality information system using probabilistic linkage methods. From 2,470,424 ECGs, 1,773,689 patients were identified. After excluding the ECGs with technical problems and patients {$<$}16\,years-old, 1,558,415 patients were studied. High performance measures were obtained using an end-to-end deep neural network trained to detect 6 types of ECG abnormalities, with F1 scores {$>$}80\% and specificity {$>$}99\% in an independent test dataset. We also evaluated the risk of mortality associated with the presence of atrial fibrillation (AF), which showed that AF was a strong predictor of cardiovascular mortality and mortality for all causes, with increased risk in women. In conclusion, a large database that comprises all ECGs performed by a large telehealth network can be useful for further developments in the field of digital electrocardiography, clinical cardiology and cardiovascular epidemiology.},
  copyright = {All rights reserved},
  keywords = {Artificial intelligence,Big-data,Electrocardiography,Telehealth},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WGK4PUV7/Ribeiro et al. - 2019 - Tele-electrocardiography and bigdata The CODE (Cl.pdf;/Users/antoniohortaribeiro/Zotero/storage/M274FR8B/S0022073619304984.html}
}

@book{rigollet_lecturenotes_2015,
  title = {{{LectureNotes}}: 18.{{S997- HighDimensionalStatistics}}},
  author = {Rigollet, Philippe},
  year = {2015},
  file = {/Users/antoniohortaribeiro/Zotero/storage/23XTFIFE/MIT18_S997S15_CourseNotes.pdf}
}

@book{rockafellar_convex_1970,
  title = {Convex {{Analysis}}},
  author = {Rockafellar, R.Tyrell},
  year = {1970},
  keywords = {\_tablet\_modified},
  file = {/Users/antoniohortaribeiro/Zotero/storage/LZBWVIK2/Rockafellar_1970_Convex Analysis.pdf}
}

@phdthesis{rojas_robust_2008,
  title = {Robust {{Experiment Design}}},
  author = {Rojas, Cristian R.},
  year = {2008},
  address = {Australia},
  school = {The University of Newcastle},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DY5KCIAR/Robust_experiment_design.pdf}
}

@article{rojas-carulla_invariant_2018,
  title = {Invariant {{Models}} for {{Causal Transfer Learning}}},
  author = {{Rojas-Carulla}, Mateo and Scholkopf, Bernhard and Turner, Richard and Peters, Jonas},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  number = {19},
  pages = {1--34},
  abstract = {Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GGGHC7UJ/Rojas-Carulla et al. - Invariant Models for Causal Transfer Learning.pdf}
}

@article{roll_piecewise_2008,
  title = {Piecewise Linear Solution Paths with Application to Direct Weight Optimization},
  author = {Roll, Jacob},
  year = {2008},
  month = nov,
  journal = {Automatica},
  volume = {44},
  number = {11},
  pages = {2745--2753},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2008.03.020},
  abstract = {Recently, pathfollowing algorithms for parametric optimization problems with piecewise linear solution paths have been developed within the field of regularized regression. This paper presents a generalization of these algorithms to a wider class of problems. It is shown that the approach can be applied to the nonparametric system identification method, Direct Weight Optimization (DWO), and be used to enhance the computational efficiency of this method. The most important design parameter in the DWO method is a parameter ({$\lambda$}) controlling the bias-variance trade-off, and the use of parametric optimization with piecewise linear solution paths means that the DWO estimates can be efficiently computed for all values of {$\lambda$} simultaneously. This allows for designing computationally attractive adaptive bandwidth selection algorithms. One such algorithm for DWO is proposed and demonstrated in two examples.},
  keywords = {Function approximation,Nonlinear system identification,Nonparametric identification,Parametric programming,Piecewise quadratic programming},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QGIQR3N2/roll_piecewise_2008.pdf;/Users/antoniohortaribeiro/Zotero/storage/N6WA2E6H/S000510980800263X.html}
}

@article{romerougalde_computational_2015,
  title = {Computational Cost Improvement of Neural Network Models in Black Box Nonlinear System Identification},
  author = {Romero Ugalde, Hector M. and Carmona, Jean-Claude and {Reyes-Reyes}, Juan and Alvarado, Victor M. and Mantilla, Juan},
  year = {2015},
  month = oct,
  journal = {Neurocomputing},
  volume = {166},
  pages = {96--108},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2015.04.022},
  abstract = {Models play an important role in many engineering fields. Therefore, the goal in system identification is to find the good balance between the accuracy, complexity and computational cost of such identification models. In a previous work (Romero-Ugalde et al., 2013 [1]), we focused on the topic of providing balanced accuracy/complexity models by proposing a dedicated neural network design and a model complexity reduction approach. In this paper, we focus on the reduction of the computational cost required to achieve these balanced models. More precisely, the improvement of the preceding method presented here leads to a significantly computational cost reduction of the neural network training phase. Even if this reduction is achieved by a convenient choice of the activation functions and the initial conditions of the synaptic weights, the proposed architecture leads to a wide range of models among the most encountered in the literature assuring the interest of such a method. To validate the proposed approach, two different systems are identified. The first one corresponds to the unavoidable Wiener--Hammerstein system proposed in SYSID2009 as a benchmark. The second system is a flexible robot arm. Results show the interest of the proposed reduction methods.},
  keywords = {Black box,Computational cost reduction,Estimation quality,Neural networks,Non-linear system identification},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9WCWPQHG/romero ugalde_computatio_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/ASEAGBG6/S0925231215004695.html;/Users/antoniohortaribeiro/Zotero/storage/PPXCUYHV/S0925231215004695.html}
}

@article{ronneberger_unet_2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  journal = {arXiv:1505.04597 [cs]},
  eprint = {1505.04597},
  primaryclass = {cs},
  urldate = {2019-06-10},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/antoniohortaribeiro/Zotero/storage/76T2XQZG/ronneberger_u-net_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/FXXEFUWV/1505.html}
}

@article{rosenblatt_perceptron_1958,
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.},
  author = {Rosenblatt, Frank},
  year = {1958},
  journal = {Psychological review},
  volume = {65},
  number = {6},
  pages = {386},
  issn = {1939-1471}
}

@incollection{rosipal_overview_2006,
  title = {Overview and {{Recent Advances}} in {{Partial Least Squares}}},
  booktitle = {Subspace, {{Latent Structure}} and {{Feature Selection}}},
  author = {Rosipal, Roman and Kr{\"a}mer, Nicole},
  editor = {Saunders, Craig and Grobelnik, Marko and Gunn, Steve and {Shawe-Taylor}, John},
  year = {2006},
  volume = {3940},
  pages = {34--51},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11752790_2},
  urldate = {2017-09-08},
  isbn = {978-3-540-34137-6 978-3-540-34138-3},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DEWVUEEM/rosipal_overview_2006.pdf}
}

@inproceedings{rossetto_improving_,
  title = {Improving {{Classification}} with {{CNNs}} Using {{Wavelet Pooling}} with {{Nesterov-Accelerated Adam}}},
  booktitle = {Proceedings of 11th {{International Conference}} on {{Bioinformatics}} and {{Computational Biology}}},
  author = {Rossetto, Allison and Zhou, Wenjin},
  pages = {84--73},
  doi = {10.29007/9c5j},
  urldate = {2020-07-07},
  abstract = {Wavelet pooling methods can improve the classification accuracy of Convolutional Neural Networks (CNNs). Combining wavelet pooling with the Nesterov-accelerated Adam (NAdam) gradient calculation method can improve both the accuracy of the CNN. We have implemented wavelet pooling with NAdam in this work using both a Haar wavelet (WavPool-NH ) and a Shannon wavelet (WavPool-NS ). The WavPool-NH and WavPoolNS methods are most accurate of the methods we considered for the MNIST and LIDCIDRI lung tumor data-sets. The WavPool-NH and WavPool-NS implementations have an accuracy of 95.92\% and 95.52\%, respectively, on the LIDC-IDRI data-set. This is an improvement from the 92.93\% accuracy obtained on this data-set with the max pooling method. The WavPool methods also avoid overfitting which is a concern with max pooling. We also found WavPool performed fairly well on the CIFAR-10 data-set, however, overfitting was an issue with all the methods we considered. Wavelet pooling, especially when combined with an adaptive gradient and wavelets chosen specifically for the data, has the potential to outperform current methods.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6AHIRLET/Rossetto and Zhou - Improving Classification with CNNs using Wavelet P.pdf}
}

@article{rossiter_modelling_2001,
  title = {Modelling and Implicit Modelling for Predictive Control},
  author = {Rossiter, J. A. and Kouvaritakis, B.},
  year = {2001},
  month = jan,
  journal = {International Journal of Control},
  volume = {74},
  number = {11},
  pages = {1085--1095},
  issn = {0020-7179, 1366-5820},
  doi = {10/dv4phx},
  urldate = {2019-04-04},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EGQ5DE2D/Rossiter and Kouvaritakis - 2001 - Modelling and implicit modelling for predictive co.pdf}
}

@article{rossler_equation_1976,
  title = {An Equation for Continuous Chaos},
  author = {R{\"o}ssler, O. E.},
  year = {1976},
  month = jul,
  journal = {Physics Letters A},
  volume = {57},
  number = {5},
  pages = {397--398},
  issn = {0375-9601},
  doi = {10.1016/0375-9601(76)90101-8},
  urldate = {2021-02-22},
  abstract = {A prototype equation to the Lorenz model of turbulence contains just one (second-order) nonlinearity in one variable. The flow in state space allows for a ``folded'' Poincar{\'e} map (horseshoe map). Many more natural and artificial systems are governed by this type of equation.},
  langid = {english}
}

@article{roth_global_2018,
  title = {Global, Regional, and National Age-Sex-Specific Mortality for 282 Causes of Death in 195 Countries and Territories, 1980--2017: A Systematic Analysis for the {{Global Burden}} of {{Disease Study}} 2017},
  shorttitle = {Global, Regional, and National Age-Sex-Specific Mortality for 282 Causes of Death in 195 Countries and Territories, 1980--2017},
  author = {Roth, Gregory A. and Abate, Degu and Abate, Kalkidan Hassen and Abay, Solomon M. and Abbafati, Cristiana and Abbasi, Nooshin and Abbastabar, Hedayat and {Abd-Allah}, Foad and Abdela, Jemal and Abdelalim, Ahmed and Abdollahpour, Ibrahim and Abdulkader, Rizwan Suliankatchi and Abebe, Haftom Temesgen and Abebe, Molla and Abebe, Zegeye and Abejie, Ayenew Negesse and Abera, Semaw F. and Abil, Olifan Zewdie and Abraha, Haftom Niguse and Abrham, Aklilu Roba and {Abu-Raddad}, Laith Jamal and Accrombessi, Manfred Mario Kokou and Acharya, Dilaram and Adamu, Abdu A. and Adebayo, Oladimeji M. and Adedoyin, Rufus Adesoji and Adekanmbi, Victor and Adetokunboh, Olatunji O. and Adhena, Beyene Meressa and Adib, Mina G. and Admasie, Amha and Afshin, Ashkan and Agarwal, Gina and Agesa, Kareha M. and Agrawal, Anurag and Agrawal, Sutapa and Ahmadi, Alireza and Ahmadi, Mehdi and Ahmed, Muktar Beshir and Ahmed, Sayem and Aichour, Amani Nidhal and Aichour, Ibtihel and Aichour, Miloud Taki Eddine and Akbari, Mohammad Esmaeil and Akinyemi, Rufus Olusola and Akseer, Nadia and {Al-Aly}, Ziyad and {Al-Eyadhy}, Ayman and {Al-Raddadi}, Rajaa M. and Alahdab, Fares and Alam, Khurshid and Alam, Tahiya and Alebel, Animut and Alene, Kefyalew Addis and Alijanzadeh, Mehran and {Alizadeh-Navaei}, Reza and Aljunid, Syed Mohamed and Alkerwi, Ala'a and Alla, Fran{\c c}ois and Allebeck, Peter and Alonso, Jordi and Altirkawi, Khalid and {Alvis-Guzman}, Nelson and Amare, Azmeraw T. and Aminde, Leopold N. and Amini, Erfan and Ammar, Walid and Amoako, Yaw Ampem and Anber, Nahla Hamed and Andrei, Catalina Liliana and Androudi, Sofia and Animut, Megbaru Debalkie and Anjomshoa, Mina and Ansari, Hossein and Ansha, Mustafa Geleto and Antonio, Carl Abelardo T. and Anwari, Palwasha and Aremu, Olatunde and {\"A}rnl{\"o}v, Johan and Arora, Amit and Arora, Monika and Artaman, Al and Aryal, Krishna K. and Asayesh, Hamid and Asfaw, Ephrem Tsegay and Ataro, Zerihun and Atique, Suleman and Atre, Sachin R. and Ausloos, Marcel and Avokpaho, Euripide F. G. A. and Awasthi, Ashish and Quintanilla, Beatriz Paulina Ayala and Ayele, Yohanes and Ayer, Rakesh and Azzopardi, Peter S. and Babazadeh, Arefeh and Bacha, Umar and Badali, Hamid and Badawi, Alaa and Bali, Ayele Geleto and Ballesteros, Katherine E. and Banach, Maciej and Banerjee, Kajori and Bannick, Marlena S. and Banoub, Joseph Adel Mattar and Barboza, Miguel A. and {Barker-Collo}, Suzanne Lyn and B{\"a}rnighausen, Till Winfried and Barquera, Simon and Barrero, Lope H. and Bassat, Quique and Basu, Sanjay and Baune, Bernhard T. and Baynes, Habtamu Wondifraw and {Bazargan-Hejazi}, Shahrzad and Bedi, Neeraj and Beghi, Ettore and Behzadifar, Masoud and Behzadifar, Meysam and B{\'e}jot, Yannick and Bekele, Bayu Begashaw and Belachew, Abate Bekele and Belay, Ezra and Belay, Yihalem Abebe and Bell, Michelle L. and Bello, Aminu K. and Bennett, Derrick A. and Bensenor, Isabela M. and Berman, Adam E. and Bernabe, Eduardo and Bernstein, Robert S. and Bertolacci, Gregory J. and Beuran, Mircea and Beyranvand, Tina and Bhalla, Ashish and Bhattarai, Suraj and Bhaumik, Soumyadeeep and Bhutta, Zulfiqar A. and Biadgo, Belete and Biehl, Molly H. and Bijani, Ali and Bikbov, Boris and Bilano, Ver and Bililign, Nigus and Sayeed, Muhammad Shahdaat Bin and Bisanzio, Donal and Biswas, Tuhin and Blacker, Brigette F. and Basara, Berrak Bora and Borschmann, Rohan and Bosetti, Cristina and Bozorgmehr, Kayvan and Brady, Oliver J. and Brant, Luisa C. and Brayne, Carol and Brazinova, Alexandra and Breitborde, Nicholas J. K. and Brenner, Hermann and Briant, Paul Svitil and Britton, Gabrielle and Brugha, Traolach and Busse, Reinhard and Butt, Zahid A. and Callender, Charlton S. K. H. and {Campos-Nonato}, Ismael R. and Rincon, Julio Cesar Campuzano and Cano, Jorge and Car, Mate and C{\'a}rdenas, Rosario and Carreras, Giulia and Carrero, Juan J. and Carter, Austin and Carvalho, F{\'e}lix and {Casta{\~n}eda-Orjuela}, Carlos A. and Rivas, Jacqueline Castillo and Castle, Chris D. and Castro, Clara and Castro, Franz and {Catal{\'a}-L{\'o}pez}, Ferr{\'a}n and Cerin, Ester and Chaiah, Yazan and Chang, Jung-Chen and Charlson, Fiona J. and Chaturvedi, Pankaj and Chiang, Peggy Pei-Chia and {Chimed-Ochir}, Odgerel and Chisumpa, Vesper Hichilombwe and Chitheer, Abdulaal and Chowdhury, Rajiv and Christensen, Hanne and Christopher, Devasahayam J. and Chung, Sheng-Chia and Cicuttini, Flavia M. and Ciobanu, Liliana G. and Cirillo, Massimo and Cohen, Aaron J. and Cooper, Leslie Trumbull and Cortesi, Paolo Angelo and Cortinovis, Monica and Cousin, Ewerton and Cowie, Benjamin C. and Criqui, Michael H. and Cromwell, Elizabeth A. and Crowe, Christopher Stephen and Crump, John A. and Cunningham, Matthew and Daba, Alemneh Kabeta and Dadi, Abel Fekadu and Dandona, Lalit and Dandona, Rakhi and Dang, Anh Kim and Dargan, Paul I. and Daryani, Ahmad and Das, Siddharth K. and Gupta, Rajat Das and Neves, Jos{\'e} Das and Dasa, Tamirat Tesfaye and Dash, Aditya Prasad and Davis, Adrian C. and Weaver, Nicole Davis and Davitoiu, Dragos Virgil and Davletov, Kairat and Hoz, Fernando Pio De La and Neve, Jan-Walter De and Degefa, Meaza Girma and Degenhardt, Louisa and Degfie, Tizta T. and Deiparine, Selina and Demoz, Gebre Teklemariam and Demtsu, Balem Betsu and {Denova-Guti{\'e}rrez}, Edgar and Deribe, Kebede and Dervenis, Nikolaos and Jarlais, Don C. Des and Dessie, Getenet Ayalew and Dey, Subhojit and Dharmaratne, Samath D. and Dicker, Daniel and Dinberu, Mesfin Tadese and Ding, Eric L. and Dirac, M. Ashworth and Djalalinia, Shirin and Dokova, Klara and Doku, David Teye and Donnelly, Christl A. and Dorsey, E. Ray and Doshi, Pratik P. and {Douwes-Schultz}, Dirk and Doyle, Kerrie E. and Driscoll, Tim R. and Dubey, Manisha and Dubljanin, Eleonora and Duken, Eyasu Ejeta and Duncan, Bruce B. and Duraes, Andre R. and Ebrahimi, Hedyeh and Ebrahimpour, Soheil and Edessa, Dumessa and Edvardsson, David and Eggen, Anne Elise and Bcheraoui, Charbel El and Zaki, Maysaa El Sayed and {El-Khatib}, Ziad and Elkout, Hajer and Ellingsen, Christian Lycke and Endres, Matthias and Endries, Aman Yesuf and Er, Benjamin and Erskine, Holly E. and Eshrati, Babak and Eskandarieh, Sharareh and Esmaeili, Reza and Esteghamati, Alireza and Fakhar, Mahdi and Fakhim, Hamed and Faramarzi, Mahbobeh and Fareed, Mohammad and Farhadi, Farzaneh and s{\'a} Farinha, Carla Sofia E. and Faro, Andre and Farvid, Maryam S. and Farzadfar, Farshad and Farzaei, Mohammad Hosein and Feigin, Valery L. and Feigl, Andrea B. and Fentahun, Netsanet and Fereshtehnejad, Seyed-Mohammad and Fernandes, Eduarda and Fernandes, Joao C. and Ferrari, Alize J. and Feyissa, Garumma Tolu and Filip, Irina and Finegold, Samuel and Fischer, Florian and Fitzmaurice, Christina and Foigt, Nataliya A. and Foreman, Kyle J. and Fornari, Carla and Frank, Tahvi D. and Fukumoto, Takeshi and Fuller, John E. and Fullman, Nancy and F{\"u}rst, Thomas and Furtado, Jo{\~a}o M. and Futran, Neal D. and Gallus, Silvano and {Garcia-Basteiro}, Alberto L. and {Garcia-Gordillo}, Miguel A. and Gardner, William M. and Gebre, Abadi Kahsu and Gebrehiwot, Tsegaye Tewelde and Gebremedhin, Amanuel Tesfay and Gebremichael, Bereket and Gebremichael, Teklu Gebrehiwo and Gelano, Tilayie Feto and Geleijnse, Johanna M. and {Genova-Maleras}, Ricard and Geramo, Yilma Chisha Dea and Gething, Peter W. and Gezae, Kebede Embaye and Ghadami, Mohammad Rasoul and Ghadimi, Reza and Falavarjani, Khalil Ghasemi and {Ghasemi-Kasman}, Maryam and Ghimire, Mamata and Gibney, Katherine B. and Gill, Paramjit Singh and Gill, Tiffany K. and Gillum, Richard F. and Ginawi, Ibrahim Abdelmageed and Giroud, Maurice and Giussani, Giorgia and Goenka, Shifalika and Goldberg, Ellen M. and Goli, Srinivas and {G{\'o}mez-Dant{\'e}s}, Hector and Gona, Philimon N. and Gopalani, Sameer Vali and Gorman, Taren M. and Goto, Atsushi and Goulart, Alessandra C. and Gnedovskaya, Elena V. and Grada, Ayman and Grosso, Giuseppe and Gugnani, Harish Chander and Guimaraes, Andre Luiz Sena and Guo, Yuming and Gupta, Prakash C. and Gupta, Rahul and Gupta, Rajeev and Gupta, Tanush and Guti{\'e}rrez, Reyna Alma and Gyawali, Bishal and Haagsma, Juanita A. and {Hafezi-Nejad}, Nima and Hagos, Tekleberhan B. and Hailegiyorgis, Tewodros Tesfa and Hailu, Gessessew Bugssa and {Haj-Mirzaian}, Arvin and {Haj-Mirzaian}, Arya and Hamadeh, Randah R. and Hamidi, Samer and Handal, Alexis J. and Hankey, Graeme J. and Harb, Hilda L. and Harikrishnan, Sivadasanpillai and Haro, Josep Maria and Hasan, Mehedi and Hassankhani, Hadi and Hassen, Hamid Yimam and Havmoeller, Rasmus and Hay, Roderick J. and Hay, Simon I. and He, Yihua and {Hedayatizadeh-Omran}, Akbar and Hegazy, Mohamed I. and Heibati, Behzad and Heidari, Mohsen and Hendrie, Delia and Henok, Andualem and Henry, Nathaniel J. and Herteliu, Claudiu and Heydarpour, Fatemeh and Heydarpour, Pouria and Heydarpour, Sousan and Hibstu, Desalegn Tsegaw and Hoek, Hans W. and Hole, Michael K. and Rad, Enayatollah Homaie and Hoogar, Praveen and Hosgood, H. Dean and Hosseini, Seyed Mostafa and Hosseinzadeh, Mehdi and Hostiuc, Mihaela and Hostiuc, Sorin and Hotez, Peter J. and Hoy, Damian G. and Hsiao, Thomas and Hu, Guoqing and Huang, John J. and Husseini, Abdullatif and Hussen, Mohammedaman Mama and Hutfless, Susan and Idrisov, Bulat and Ilesanmi, Olayinka Stephen and Iqbal, Usman and Irvani, Seyed Sina Naghibi and Irvine, Caleb Mackay Salpeter and Islam, Nazrul and Islam, Sheikh Mohammed Shariful and Islami, Farhad and Jacobsen, Kathryn H. and Jahangiry, Leila and Jahanmehr, Nader and Jain, Sudhir Kumar and Jakovljevic, Mihajlo and Jalu, Moti Tolera and James, Spencer L. and Javanbakht, Mehdi and Jayatilleke, Achala Upendra and Jeemon, Panniyammakal and Jenkins, Kathy J. and Jha, Ravi Prakash and Jha, Vivekanand and Johnson, Catherine O. and Johnson, Sarah C. and Jonas, Jost B. and Joshi, Ankur and Jozwiak, Jacek Jerzy and Jungari, Suresh Banayya and J{\"u}risson, Mikk and Kabir, Zubair and Kadel, Rajendra and Kahsay, Amaha and Kalani, Rizwan and Karami, Manoochehr and Matin, Behzad Karami and Karch, Andr{\'e} and Karema, Corine and {Karimi-Sari}, Hamidreza and Kasaeian, Amir and Kassa, Dessalegn H. and Kassa, Getachew Mullu and Kassa, Tesfaye Dessale and Kassebaum, Nicholas J. and Katikireddi, Srinivasa Vittal and Kaul, Anil and Kazemi, Zhila and Karyani, Ali Kazemi and Kazi, Dhruv Satish and Kefale, Adane Teshome and Keiyoro, Peter Njenga and Kemp, Grant Rodgers and Kengne, Andre Pascal and Keren, Andre and Kesavachandran, Chandrasekharan Nair and Khader, Yousef Saleh and Khafaei, Behzad and Khafaie, Morteza Abdullatif and Khajavi, Alireza and Khalid, Nauman and Khalil, Ibrahim A. and Khan, Ejaz Ahmad and Khan, Muhammad Shahzeb and Khan, Muhammad Ali and Khang, Young-Ho and Khater, Mona M. and Khoja, Abdullah T. and Khosravi, Ardeshir and Khosravi, Mohammad Hossein and Khubchandani, Jagdish and Kiadaliri, Aliasghar A. and Kibret, Getiye D. and Kidanemariam, Zelalem Teklemariam and Kiirithio, Daniel N. and Kim, Daniel and Kim, Young-Eun and Kim, Yun Jin and Kimokoti, Ruth W. and Kinfu, Yohannes and Kisa, Adnan and {Kissimova-Skarbek}, Katarzyna and Kivim{\"a}ki, Mika and Knudsen, Ann Kristin Skrindo and Kocarnik, Jonathan M. and Kochhar, Sonali and Kokubo, Yoshihiro and Kolola, Tufa and Kopec, Jacek A. and Koul, Parvaiz A. and Koyanagi, Ai and Kravchenko, Michael A. and Krishan, Kewal and Defo, Barthelemy Kuate and Bicer, Burcu Kucuk and Kumar, G. Anil and Kumar, Manasi and Kumar, Pushpendra and Kutz, Michael J. and Kuzin, Igor and Kyu, Hmwe Hmwe and Lad, Deepesh P. and Lad, Sheetal D. and Lafranconi, Alessandra and Lal, Dharmesh Kumar and Lalloo, Ratilal and Lallukka, Tea and Lam, Jennifer O. and Lami, Faris Hasan and Lansingh, Van C. and Lansky, Sonia and Larson, Heidi J. and Latifi, Arman and Lau, Kathryn Mei-Ming and Lazarus, Jeffrey V. and Lebedev, Georgy and Lee, Paul H. and Leigh, James and Leili, Mostafa and Leshargie, Cheru Tesema and Li, Shanshan and Li, Yichong and Liang, Juan and Lim, Lee-Ling and Lim, Stephen S. and Limenih, Miteku Andualem and Linn, Shai and Liu, Shiwei and Liu, Yang and Lodha, Rakesh and Lonsdale, Chris and Lopez, Alan D. and Lorkowski, Stefan and Lotufo, Paulo A. and Lozano, Rafael and Lunevicius, Raimundas and Ma, Stefan and Macarayan, Erlyn Rachelle King and Mackay, Mark T. and MacLachlan, Jennifer H. and Maddison, Emilie R. and Madotto, Fabiana and Razek, Hassan Magdy Abd El and Razek, Muhammed Magdy Abd El and Maghavani, Dhaval P. and Majdan, Marek and Majdzadeh, Reza and Majeed, Azeem and Malekzadeh, Reza and Malta, Deborah Carvalho and Manda, Ana-Laura and {Mandarano-Filho}, Luiz Garcia and Manguerra, Helena and Mansournia, Mohammad Ali and Mapoma, Chabila Christopher and Marami, Dadi and Maravilla, Joemer C. and Marcenes, Wagner and Marczak, Laurie and Marks, Ashley and Marks, Guy B. and Martinez, Gabriel and {Martins-Melo}, Francisco Rogerl{\^a}ndio and Martopullo, Ira and M{\"a}rz, Winfried and Marzan, Melvin B. and Masci, Joseph R. and Massenburg, Benjamin Ballard and Mathur, Manu Raj and Mathur, Prashant and Matzopoulos, Richard and Maulik, Pallab K. and Mazidi, Mohsen and McAlinden, Colm and McGrath, John J. and McKee, Martin and McMahon, Brian J. and Mehata, Suresh and Mehndiratta, Man Mohan and Mehrotra, Ravi and Mehta, Kala M. and Mehta, Varshil and Mekonnen, Tefera C. and Melese, Addisu and Melku, Mulugeta and Memiah, Peter T. N. and Memish, Ziad A. and Mendoza, Walter and Mengistu, Desalegn Tadese and Mengistu, Getnet and Mensah, George A. and Mereta, Seid Tiku and Meretoja, Atte and Meretoja, Tuomo J. and Mestrovic, Tomislav and Mezgebe, Haftay Berhane and Miazgowski, Bartosz and Miazgowski, Tomasz and Millear, Anoushka I. and Miller, Ted R. and {Miller-Petrie}, Molly Katherine and Mini, G. K. and Mirabi, Parvaneh and Mirarefin, Mojde and Mirica, Andreea and Mirrakhimov, Erkin M. and Misganaw, Awoke Temesgen and Mitiku, Habtamu and Moazen, Babak and Mohammad, Karzan Abdulmuhsin and Mohammadi, Moslem and Mohammadifard, Noushin and Mohammed, Mohammed A. and Mohammed, Shafiu and Mohan, Viswanathan and Mokdad, Ali H. and Molokhia, Mariam and Monasta, Lorenzo and Moradi, Ghobad and {Moradi-Lakeh}, Maziar and Moradinazar, Mehdi and Moraga, Paula and Morawska, Lidia and Vel{\'a}squez, Ilais Moreno and {Morgado-Da-Costa}, Joana and Morrison, Shane Douglas and Moschos, Marilita M. and Mouodi, Simin and Mousavi, Seyyed Meysam and Muchie, Kindie Fentahun and Mueller, Ulrich Otto and Mukhopadhyay, Satinath and Muller, Kate and Mumford, John Everett and Musa, Jonah and Musa, Kamarul Imran and Mustafa, Ghulam and Muthupandian, Saravanan and Nachega, Jean B. and Nagel, Gabriele and Naheed, Aliya and Nahvijou, Azin and Naik, Gurudatta and Nair, Sanjeev and Najafi, Farid and Naldi, Luigi and Nam, Hae Sung and Nangia, Vinay and Nansseu, Jobert Richie and Nascimento, Bruno Ramos and Natarajan, Gopalakrishnan and Neamati, Nahid and Negoi, Ionut and Negoi, Ruxandra Irina and Neupane, Subas and Newton, Charles R. J. and Ngalesoni, Frida N. and Ngunjiri, Josephine W. and Nguyen, Anh Quynh and Nguyen, Grant and Nguyen, Ha Thu and Nguyen, Huong Thanh and Nguyen, Long Hoang and Nguyen, Minh and Nguyen, Trang Huyen and Nichols, Emma and Ningrum, Dina Nur Anggraini and Nirayo, Yirga Legesse and Nixon, Molly R. and Nolutshungu, Nomonde and Nomura, Shuhei and Norheim, Ole F. and Noroozi, Mehdi and Norrving, Bo and Noubiap, Jean Jacques and Nouri, Hamid Reza and Shiadeh, Malihe Nourollahpour and Nowroozi, Mohammad Reza and Nyasulu, Peter S. and Odell, Christopher M. and {Ofori-Asenso}, Richard and Ogbo, Felix Akpojene and Oh, In-Hwan and Oladimeji, Olanrewaju and Olagunju, Andrew T. and Olivares, Pedro R. and Olsen, Helen Elizabeth and Olusanya, Bolajoko Olubukunola and Olusanya, Jacob Olusegun and Ong, Kanyin L. and Ong, Sok King Sk and Oren, Eyal and Orpana, Heather M. and Ortiz, Alberto and Ortiz, Justin R. and Otstavnov, Stanislav S. and {\O}verland, Simon and Owolabi, Mayowa Ojo and {\"O}zdemir, Raziye and A, Mahesh P. and Pacella, Rosana and Pakhale, Smita and Pakhare, Abhijit P. and Pakpour, Amir H. and Pana, Adrian and {Panda-Jonas}, Songhomitra and Pandian, Jeyaraj Durai and Parisi, Andrea and Park, Eun-Kee and Parry, Charles D. H. and Parsian, Hadi and Patel, Shanti and Pati, Sanghamitra and Patton, George C. and Paturi, Vishnupriya Rao and Paulson, Katherine R. and Pereira, Alexandre and Pereira, David M. and Perico, Norberto and Pesudovs, Konrad and Petzold, Max and Phillips, Michael R. and Piel, Fr{\'e}d{\'e}ric B. and Pigott, David M. and Pillay, Julian David and Pirsaheb, Meghdad and Pishgar, Farhad and Polinder, Suzanne and Postma, Maarten J. and Pourshams, Akram and Poustchi, Hossein and Pujar, Ashwini and Prakash, Swayam and Prasad, Narayan and Purcell, Caroline A. and Qorbani, Mostafa and Quintana, Hedley and Quistberg, D. Alex and Rade, Kirankumar Waman and Radfar, Amir and Rafay, Anwar and Rafiei, Alireza and Rahim, Fakher and Rahimi, Kazem and {Rahimi-Movaghar}, Afarin and Rahman, Mahfuzar and Rahman, Mohammad Hifz Ur and Rahman, Muhammad Aziz and Rai, Rajesh Kumar and Rajsic, Sasa and Ram, Usha and Ranabhat, Chhabi Lal and Ranjan, Prabhat and Rao, Puja C. and Rawaf, David Laith and Rawaf, Salman and {Razo-Garc{\'i}a}, Christian and Reddy, K. Srinath and Reiner, Robert C. and Reitsma, Marissa B. and Remuzzi, Giuseppe and Renzaho, Andre M. N. and Resnikoff, Serge and Rezaei, Satar and Rezaeian, Shahab and Rezai, Mohammad Sadegh and Riahi, Seyed Mohammad and Ribeiro, Antonio Luiz P. and {Rios-Blancas}, Maria Jesus and Roba, Kedir Teji and Roberts, Nicholas L. S. and Robinson, Stephen R. and Roever, Leonardo and Ronfani, Luca and Roshandel, Gholamreza and Rostami, Ali and Rothenbacher, Dietrich and Roy, Ambuj and Rubagotti, Enrico and Sachdev, Perminder S. and Saddik, Basema and Sadeghi, Ehsan and Safari, Hosein and Safdarian, Mahdi and Safi, Sare and Safiri, Saeid and Sagar, Rajesh and Sahebkar, Amirhossein and Sahraian, Mohammad Ali and Salam, Nasir and Salama, Joseph S. and Salamati, Payman and Saldanha, Raphael De Freitas and Saleem, Zikria and Salimi, Yahya and Salvi, Sundeep Santosh and Salz, Inbal and Sambala, Evanson Zondani and Samy, Abdallah M. and Sanabria, Juan and {Sanchez-Ni{\~n}o}, Maria Dolores and Santomauro, Damian Francesco and Santos, Itamar S. and Santos, Jo{\~a}o Vasco and Milicevic, Milena M. Santric and Jose, Bruno Piassi Sao and Sarker, Abdur Razzaque and {Sarmiento-Su{\'a}rez}, Rodrigo and Sarrafzadegan, Nizal and Sartorius, Benn and Sarvi, Shahabeddin and Sathian, Brijesh and Satpathy, Maheswar and Sawant, Arundhati R. and Sawhney, Monika and Saxena, Sonia and Sayyah, Mehdi and Schaeffner, Elke and Schmidt, Maria In{\^e}s and Schneider, Ione J. C. and Sch{\"o}ttker, Ben and Schutte, Aletta Elisabeth and Schwebel, David C. and Schwendicke, Falk and Scott, James G. and Sekerija, Mario and Sepanlou, Sadaf G. and {Serv{\'a}n-Mori}, Edson and Seyedmousavi, Seyedmojtaba and Shabaninejad, Hosein and Shackelford, Katya Anne and Shafieesabet, Azadeh and Shahbazi, Mehdi and Shaheen, Amira A. and Shaikh, Masood Ali and {Shams-Beyranvand}, Mehran and Shamsi, Mohammadbagher and Shamsizadeh, Morteza and Sharafi, Kiomars and Sharif, Mehdi and {Sharif-Alhoseini}, Mahdi and Sharma, Rajesh and She, Jun and Sheikh, Aziz and Shi, Peilin and Shiferaw, Mekonnen Sisay and Shigematsu, Mika and Shiri, Rahman and Shirkoohi, Reza and Shiue, Ivy and Shokraneh, Farhad and Shrime, Mark G. and Si, Si and Siabani, Soraya and Siddiqi, Tariq J. and Sigfusdottir, Inga Dora and Sigurvinsdottir, Rannveig and Silberberg, Donald H. and Silva, Diego Augusto Santos and Silva, Jo{\~a}o Pedro and Silva, Natacha Torres Da and Silveira, Dayane Gabriele Alves and Singh, Jasvinder A. and Singh, Narinder Pal and Singh, Prashant Kumar and Singh, Virendra and Sinha, Dhirendra Narain and Sliwa, Karen and Smith, Mari and Sobaih, Badr Hasan and Sobhani, Soheila and Sobngwi, Eug{\`e}ne and Soneji, Samir S. and Soofi, Moslem and Sorensen, Reed J. D. and Soriano, Joan B. and Soyiri, Ireneous N. and Sposato, Luciano A. and Sreeramareddy, Chandrashekhar T. and Srinivasan, Vinay and Stanaway, Jeffrey D. and Starodubov, Vladimir I. and Stathopoulou, Vasiliki and Stein, Dan J. and Steiner, Caitlyn and Stewart, Leo G. and Stokes, Mark A. and Subart, Michelle L. and Sudaryanto, Agus and Sufiyan, Mu'awiyyah Babale and Sur, Patrick John and Sutradhar, Ipsita and Sykes, Bryan L. and Sylaja, P. N. and Sylte, Dillon O. and Szoeke, Cassandra E. I. and {Tabar{\'e}s-Seisdedos}, Rafael and Tabuchi, Takahiro and Tadakamadla, Santosh Kumar and Takahashi, Ken and Tandon, Nikhil and Tassew, Segen Gebremeskel and Taveira, Nuno and {Tehrani-Banihashemi}, Arash and Tekalign, Tigist Gashaw and Tekle, Merhawi Gebremedhin and Temsah, Mohamad-Hani and Temsah, Omar and Terkawi, Abdullah Sulieman and Teshale, Manaye Yihune and Tessema, Belay and Tessema, Gizachew Assefa and Thankappan, Kavumpurathu Raman and Thirunavukkarasu, Sathish and Thomas, Nihal and Thrift, Amanda G. and Thurston, George D. and Tilahun, Binyam and To, Quyen G. and {Tobe-Gai}, Ruoyan and Tonelli, Marcello and {Topor-Madry}, Roman and Torre, Anna E. and {Tortajada-Girb{\'e}s}, Miguel and Touvier, Mathilde and {Tovani-Palone}, Marcos Roberto and Tran, Bach Xuan and Tran, Khanh Bao and Tripathi, Suryakant and Troeger, Christopher E. and Truelsen, Thomas Clement and Truong, Nu Thi and Tsadik, Afewerki Gebremeskel and Tsoi, Derrick and Car, Lorainne Tudor and Tuzcu, E. Murat and Tyrovolas, Stefanos and Ukwaja, Kingsley N. and Ullah, Irfan and Undurraga, Eduardo A. and Updike, Rachel L. and Usman, Muhammad Shariq and Uthman, Olalekan A. and Uzun, Selen Beg{\"u}m and Vaduganathan, Muthiah and Vaezi, Afsane and Vaidya, Gaurang and Valdez, Pascual R. and Varavikova, Elena and Vasankari, Tommi Juhani and Venketasubramanian, Narayanaswamy and Villafaina, Santos and Violante, Francesco S. and Vladimirov, Sergey Konstantinovitch and Vlassov, Vasily and Vollset, Stein Emil and Vos, Theo and Wagner, Gregory R. and Wagnew, Fasil Shiferaw and Waheed, Yasir and Wallin, Mitchell Taylor and Walson, Judd L. and Wang, Yanping and Wang, Yuan-Pang and Wassie, Molla Mesele and Weiderpass, Elisabete and Weintraub, Robert G. and Weldegebreal, Fitsum and Weldegwergs, Kidu Gidey and Werdecker, Andrea and Werkneh, Adhena Ayaliew and West, T. Eoin and Westerman, Ronny and Whiteford, Harvey A. and Widecka, Justyna and Wilner, Lauren B. and Wilson, Shadrach and Winkler, Andrea Sylvia and Wiysonge, Charles Shey and Wolfe, Charles D. A. and Wu, Shouling and Wu, Yun-Chun and Wyper, Grant M. A. and Xavier, Denis and Xu, Gelin and Yadgir, Simon and Yadollahpour, Ali and Jabbari, Seyed Hossein Yahyazadeh and Yakob, Bereket and Yan, Lijing L. and Yano, Yuichiro and Yaseri, Mehdi and Yasin, Yasin Jemal and Yent{\"u}r, G{\"o}kalp Kadri and Yeshaneh, Alex and Yimer, Ebrahim M. and Yip, Paul and Yirsaw, Biruck Desalegn and Yisma, Engida and Yonemoto, Naohiro and Yonga, Gerald and Yoon, Seok-Jun and Yotebieng, Marcel and Younis, Mustafa Z. and Yousefifard, Mahmoud and Yu, Chuanhua and Zadnik, Vesna and Zaidi, Zoubida and Zaman, Sojib Bin and Zamani, Mohammad and Zare, Zohreh and Zeleke, Ayalew Jejaw and Zenebe, Zerihun Menlkalew and Zhang, Anthony Lin and Zhang, Kai and Zhou, Maigeng and Zodpey, Sanjay and Zuhlke, Liesl Joanna and Naghavi, Mohsen and Murray, Christopher J. L.},
  year = {2018},
  month = nov,
  journal = {The Lancet},
  volume = {392},
  number = {10159},
  pages = {1736--1788},
  issn = {0140-6736, 1474-547X},
  doi = {10/gfhx3t},
  urldate = {2019-01-14},
  abstract = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}h3{$>$}Background{$<$}/h3{$><$}p{$>$}Global development goals increasingly rely on country-specific estimates for benchmarking a nation's progress. To meet this need, the Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2016 estimated global, regional, national, and, for selected locations, subnational cause-specific mortality beginning in the year 1980. Here we report an update to that study, making use of newly available data and improved methods. GBD 2017 provides a comprehensive assessment of cause-specific mortality for 282 causes in 195 countries and territories from 1980 to 2017.{$<$}/p{$><$}h3{$>$}Methods{$<$}/h3{$><$}p{$>$}The causes of death database is composed of vital registration (VR), verbal autopsy (VA), registry, survey, police, and surveillance data. GBD 2017 added ten VA studies, 127 country-years of VR data, 502 cancer-registry country-years, and an additional surveillance country-year. Expansions of the GBD cause of death hierarchy resulted in 18 additional causes estimated for GBD 2017. Newly available data led to subnational estimates for five additional countries---Ethiopia, Iran, New Zealand, Norway, and Russia. Deaths assigned International Classification of Diseases (ICD) codes for non-specific, implausible, or intermediate causes of death were reassigned to underlying causes by redistribution algorithms that were incorporated into uncertainty estimation. We used statistical modelling tools developed for GBD, including the Cause of Death Ensemble model (CODEm), to generate cause fractions and cause-specific death rates for each location, year, age, and sex. Instead of using UN estimates as in previous versions, GBD 2017 independently estimated population size and fertility rate for all locations. Years of life lost (YLLs) were then calculated as the sum of each death multiplied by the standard life expectancy at each age. All rates reported here are age-standardised.{$<$}/p{$><$}h3{$>$}Findings{$<$}/h3{$><$}p{$>$}At the broadest grouping of causes of death (Level 1), non-communicable diseases (NCDs) comprised the greatest fraction of deaths, contributing to 73{$\cdot$}4\% (95\% uncertainty interval [UI] 72{$\cdot$}5--74{$\cdot$}1) of total deaths in 2017, while communicable, maternal, neonatal, and nutritional (CMNN) causes accounted for 18{$\cdot$}6\% (17{$\cdot$}9--19{$\cdot$}6), and injuries 8{$\cdot$}0\% (7{$\cdot$}7--8{$\cdot$}2). Total numbers of deaths from NCD causes increased from 2007 to 2017 by 22{$\cdot$}7\% (21{$\cdot$}5--23{$\cdot$}9), representing an additional 7{$\cdot$}61 million (7{$\cdot$}20--8{$\cdot$}01) deaths estimated in 2017 versus 2007. The death rate from NCDs decreased globally by 7{$\cdot$}9\% (7{$\cdot$}0--8{$\cdot$}8). The number of deaths for CMNN causes decreased by 22{$\cdot$}2\% (20{$\cdot$}0--24{$\cdot$}0) and the death rate by 31{$\cdot$}8\% (30{$\cdot$}1--33{$\cdot$}3). Total deaths from injuries increased by 2{$\cdot$}3\% (0{$\cdot$}5--4{$\cdot$}0) between 2007 and 2017, and the death rate from injuries decreased by 13{$\cdot$}7\% (12{$\cdot$}2--15{$\cdot$}1) to 57{$\cdot$}9 deaths (55{$\cdot$}9--59{$\cdot$}2) per 100 000 in 2017. Deaths from substance use disorders also increased, rising from 284 000 deaths (268 000--289 000) globally in 2007 to 352 000 (334 000--363 000) in 2017. Between 2007 and 2017, total deaths from conflict and terrorism increased by 118{$\cdot$}0\% (88{$\cdot$}8--148{$\cdot$}6). A greater reduction in total deaths and death rates was observed for some CMNN causes among children younger than 5 years than for older adults, such as a 36{$\cdot$}4\% (32{$\cdot$}2--40{$\cdot$}6) reduction in deaths from lower respiratory infections for children younger than 5 years compared with a 33{$\cdot$}6\% (31{$\cdot$}2--36{$\cdot$}1) increase in adults older than 70 years. Globally, the number of deaths was greater for men than for women at most ages in 2017, except at ages older than 85 years. Trends in global YLLs reflect an epidemiological transition, with decreases in total YLLs from enteric infections, respiratory infections and tuberculosis, and maternal and neonatal disorders between 1990 and 2017; these were generally greater in magnitude at the lowest levels of the Socio-demographic Index (SDI). At the same time, there were large increases in YLLs from neoplasms and cardiovascular diseases. YLL rates decreased across the five leading Level 2 causes in all SDI quintiles. The leading causes of YLLs in 1990---neonatal disorders, lower respiratory infections, and diarrhoeal diseases---were ranked second, fourth, and fifth, in 2017. Meanwhile, estimated YLLs increased for ischaemic heart disease (ranked first in 2017) and stroke (ranked third), even though YLL rates decreased. Population growth contributed to increased total deaths across the 20 leading Level 2 causes of mortality between 2007 and 2017. Decreases in the cause-specific mortality rate reduced the effect of population growth for all but three causes: substance use disorders, neurological disorders, and skin and subcutaneous diseases.{$<$}/p{$><$}h3{$>$}Interpretation{$<$}/h3{$><$}p{$>$}Improvements in global health have been unevenly distributed among populations. Deaths due to injuries, substance use disorders, armed conflict and terrorism, neoplasms, and cardiovascular disease are expanding threats to global health. For causes of death such as lower respiratory and enteric infections, more rapid progress occurred for children than for the oldest adults, and there is continuing disparity in mortality rates by sex across age groups. Reductions in the death rate of some common diseases are themselves slowing or have ceased, primarily for NCDs, and the death rate for selected causes has increased in the past decade.{$<$}/p{$><$}h3{$>$}Funding{$<$}/h3{$><$}p{$>$}Bill \& Melinda Gates Foundation.{$<$}/p{$>$}},
  langid = {english},
  pmid = {30496103},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4HUVMW9G/roth_global,_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/G3PEL74W/fulltext.html}
}

@inproceedings{roy_maximumflow_1998,
  title = {A Maximum-Flow Formulation of the n-Camera Stereo Correspondence Problem},
  booktitle = {Computer {{Vision}}, 1998. {{Sixth International Conference}} On},
  author = {Roy, S{\'e}bastien and Cox, Ingemar J},
  year = {1998},
  pages = {492--499},
  publisher = {IEEE},
  annotation = {00000}
}

@inproceedings{roy_maximumflow_1998a,
  title = {A {{Maximum-Flow Formulation}} of the n-{{Camera Stereo Correspondence Problem}}},
  booktitle = {Computer {{Vision}}, 1998. {{Sixth International Conference On}}},
  author = {Roy, S{\'e}bastien and Cox, Ingemar J},
  year = {1998},
  pages = {492--499},
  publisher = {IEEE},
  annotation = {00731}
}

@article{rubin_densely_2017,
  title = {Densely {{Connected Convolutional Networks}} and {{Signal Quality Analysis}} to {{Detect Atrial Fibrillation Using Short Single-Lead ECG Recordings}}},
  author = {Rubin, Jonathan and Parvaneh, Saman and Rahman, Asif and Conroy, Bryan and Babaeizadeh, Saeed},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.05817},
  eprint = {1710.05817},
  urldate = {2018-10-21},
  abstract = {The development of new technology such as wearables that record high-quality single channel ECG, provides an opportunity for ECG screening in a larger population, especially for atrial fibrillation screening. The main goal of this study is to develop an automatic classification algorithm for normal sinus rhythm (NSR), atrial fibrillation (AF), other rhythms (O), and noise from a single channel short ECG segment (9-60 seconds). For this purpose, signal quality index (SQI) along with dense convolutional neural networks was used. Two convolutional neural network (CNN) models (main model that accepts 15 seconds ECG and secondary model that processes 9 seconds shorter ECG) were trained using the training data set. If the recording is determined to be of low quality by SQI, it is immediately classified as noisy. Otherwise, it is transformed to a time-frequency representation and classified with the CNN as NSR, AF, O, or noise. At the final step, a feature-based post-processing algorithm classifies the rhythm as either NSR or O in case the CNN model's discrimination between the two is indeterminate. The best result achieved at the official phase of the PhysioNet/CinC challenge on the blind test set was 0.80 (F1 for NSR, AF, and O were 0.90, 0.80, and 0.70, respectively).},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WK96VH5M/rubin_densely_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/QYRU4P87/1710.html}
}

@article{rubio_spectral_2011,
  title = {Spectral Convergence for a General Class of Random Matrices},
  author = {Rubio, Francisco and Mestre, Xavier},
  year = {2011},
  month = feb,
  journal = {Statistics and Probability Letters},
  volume = {81},
  number = {5},
  pages = {592},
  publisher = {Elsevier},
  doi = {10.1016/j.spl.2011.01.004},
  urldate = {2020-12-22},
  abstract = {Let be an complex random matrix with i.i.d.~entries having mean zero and variance and consider the class of matrices of the type , where , and are Hermitian nonnegative definite matrices, such that and have bounded spectral norm with being diagonal, and is the nonnegative definite square-root of . Under some assumptions on the moments of the entries of , it is proved in this paper that, for any matrix with bounded trace norm and for each complex outside the positive real line, almost surely as at the same rate, where is deterministic and solely depends on and . The previous result can be particularized to the study of the limiting behavior of the Stieltjes transform as well as the eigenvectors of the random matrix model . The study is motivated by applications in the field of statistical signal processing.},
  keywords = {Multivariate statistics,Random matrix theory,Sample covariance matrix,Separable covariance model,Stieltjes transform},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9SWFD6ZH/Rubio_Mestre_2011_Spectral convergence for a general class of random matrices.pdf}
}

@inproceedings{rudi_generalization_2017,
  title = {Generalization Properties of Learning with Random Features},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Rudi, Alessandro and Rosasco, Lorenzo},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QTKSKWUV/Rudi and Rosasco - Generalization Properties of Learning with Random .pdf}
}

@inproceedings{rudi_less_2015,
  title = {Less Is More: {{Nystr{\"o}m}} Computational Regularization},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
  editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/U9NYTXJN/Rudi et al. - 2015 - Less is more NystrÃ¶m computational regularization.pdf}
}

@book{rudin_principles_1964,
  title = {Principles of Mathematical Analysis},
  author = {Rudin, W.},
  year = {1964},
  series = {International Series in Pure and Applied Mathematics},
  publisher = {McGraw-Hill},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5FCCZHHE/Hwk1Sol.pdf;/Users/antoniohortaribeiro/Zotero/storage/IRNUAF7C/rudin ch 7.pdf;/Users/antoniohortaribeiro/Zotero/storage/JJPVY34Y/rudin ch 5.pdf;/Users/antoniohortaribeiro/Zotero/storage/MXC2U2DC/rudin_principles_1964.pdf;/Users/antoniohortaribeiro/Zotero/storage/VKKWU8GH/rudin ch 6.pdf;/Users/antoniohortaribeiro/Zotero/storage/WMAZMXNT/rudin_principles_1964.pdf}
}

@book{rudin_real_1987,
  title = {Real and Complex Analysis},
  author = {Rudin, Walter},
  year = {1987},
  edition = {3rd ed},
  publisher = {McGraw-Hill},
  address = {New York},
  isbn = {978-0-07-054234-1},
  langid = {english},
  lccn = {QA300 .R82 1987},
  keywords = {Mathematical analysis},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WMQ4MD32/Rudin - 1987 - Real and complex analysis.pdf}
}

@techreport{rumelhart_learning_1985,
  title = {Learning Internal Representations by Error Propagation},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1985},
  institution = {California Univ San Diego La Jolla Inst for Cognitive Science},
  urldate = {2017-09-10},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/USSXVRX6/rumelhart_learning_1985.pdf}
}

@article{rumelhart_learning_1988,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year = {1988},
  journal = {Cognitive modeling},
  volume = {5},
  number = {3},
  pages = {1}
}

@inproceedings{ruslan_flood_2014,
  title = {Flood Water Level Modeling and Prediction Using {{NARX}} Neural Network: {{Case}} Study at {{Kelang}} River},
  booktitle = {Signal {{Processing}} \& Its {{Applications}} ({{CSPA}}), 2014 {{IEEE}} 10th {{International Colloquium}} On},
  author = {Ruslan, Fazlina Ahmat and Samad, Abd Manan and Zain, Zainazlan Md and Adnan, Ramli},
  year = {2014},
  pages = {204--207},
  publisher = {IEEE},
  annotation = {00000}
}

@article{saad_adaptive_1994,
  title = {Adaptive Robot Control Using Neural Networks},
  author = {Saad, Maarouf and Bigras, Pascal and Dessaint, L-A and {Al-Haddad}, Kamal},
  year = {1994},
  journal = {IEEE Transactions on Industrial Electronics},
  volume = {41},
  number = {2},
  pages = {173--181},
  doi = {10.1109/41.293877},
  annotation = {00000}
}

@article{sabino_tenyear_2013,
  title = {Ten-{{Year Incidence}} of {{Chagas Cardiomyopathy Among Asymptomatic Trypanosoma}} Cruzi--{{Seropositive Former Blood Donors}}},
  author = {Sabino, Ester C. and Ribeiro, Antonio L. and Salemi, Vera M.C. and Di Lorenzo Oliveira, Claudia and Antunes, Andre P. and Menezes, Marcia M. and Ianni, Barbara M. and Nastari, Luciano and Fernandes, Fabio and Patavino, Giuseppina M. and Sachdev, Vandana and Capuani, Ligia and {de Almeida-Neto}, Cesar and Carrick, Danielle M. and Wright, David and Kavounis, Katherine and Goncalez, Thelma T. and {Carneiro-Proietti}, Anna Barbara and Custer, Brian and Busch, Michael P. and Murphy, Edward L.},
  year = {2013},
  month = mar,
  journal = {Circulation},
  volume = {127},
  number = {10},
  pages = {1105--1115},
  publisher = {American Heart Association},
  doi = {10.1161/CIRCULATIONAHA.112.123612},
  urldate = {2021-11-25},
  abstract = {Background--- Very few studies have measured disease penetrance and prognostic factors of Chagas cardiomyopathy among asymptomatic Trypanosoma cruzi--infected persons. Methods and Results--- We performed a retrospective cohort study among initially healthy blood donors with an index T cruzi--seropositive donation and age-, sex-, and period-matched seronegatives in 1996 to 2002 in the Brazilian cities of S{\~a}o Paulo and Montes Claros. In 2008 to 2010, all subjects underwent medical history, physical examination, ECGs, and echocardiograms. ECG and echocardiogram results were classified by blinded core laboratories, and records with abnormal results were reviewed by a blinded panel of 3 cardiologists who adjudicated the outcome of Chagas cardiomyopathy. Associations with Chagas cardiomyopathy were tested with multivariate logistic regression. Mean follow-up time between index donation and outcome assessment was 10.5 years for the seropositives and 11.1 years for the seronegatives. Among 499 T cruzi seropositives, 120 (24\%) had definite Chagas cardiomyopathy, and among 488 T cruzi seronegatives, 24 (5\%) had cardiomyopathy, for an incidence difference of 1.85 per 100 person-years attributable to T cruzi infection. Of the 120 seropositives classified as having Chagas cardiomyopathy, only 31 (26\%) presented with ejection fraction {$<$}50\%, and only 11 (9\%) were classified as New York Heart Association class II or higher. Chagas cardiomyopathy was associated (P{$<$}0.01) with male sex, a history of abnormal ECG, and the presence of an S3 heart sound. Conclusions--- There is a substantial annual incidence of Chagas cardiomyopathy among initially asymptomatic T cruzi--seropositive blood donors, although disease was mild at diagnosis.},
  keywords = {blood donors,Brazil,Chagas cardiomyopathy,Chagas disease,incidence},
  file = {/Users/antoniohortaribeiro/Zotero/storage/P6FDQISF/Sabino et al_2013_Ten-Year Incidence of Chagas Cardiomyopathy Among Asymptomatic Trypanosoma.pdf}
}

@article{sacker_invariant_1967,
  title = {{{ON INVARIANT SURFACES AND BIFURCATION OF PERIODIC SOLUTIONS OF ORDINARY DIFFERENTIAL EQUATIONS}}.},
  author = {Sacker, Robert John},
  year = {1967}
}

@incollection{sacramento_dendritic_2018,
  title = {Dendritic Cortical Microcircuits Approximate the Backpropagation Algorithm},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Sacramento, Jo{\~a}o and Ponte Costa, Rui and Bengio, Yoshua and Senn, Walter},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {8734--8745},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-04},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CLAYF4WM/sacramento_dendritic_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/Q3CAA5VM/8089-dendritic-cortical-microcircuits-approximate-the-backpropagation-algorithm.html}
}

@inproceedings{saggar_system_2007,
  title = {System Identification for the {{Hodgkin-Huxley}} Model Using Artificial Neural Networks},
  booktitle = {Neural {{Networks}}, 2007. {{IJCNN}} 2007. {{International Joint Conference}} On},
  author = {Saggar, Manish and Meri{\c c}li, Tekin and Andoni, Sari and Miikkulainen, Risto},
  year = {2007},
  pages = {2239--2244},
  publisher = {IEEE},
  annotation = {00000}
}

@inproceedings{saggar_system_2007a,
  title = {System {{Identification}} for the {{Hodgkin-Huxley Model Using Artificial Neural Networks}}},
  booktitle = {Neural {{Networks}}, 2007. {{IJCNN}} 2007. {{International Joint Conference On}}},
  author = {Saggar, Manish and Meri{\c c}li, Tekin and Andoni, Sari and Miikkulainen, Risto},
  year = {2007},
  pages = {2239--2244},
  publisher = {IEEE},
  annotation = {00016}
}

@article{saito_precisionrecall_2015,
  title = {The {{Precision-Recall Plot Is More Informative}} than the {{ROC Plot When Evaluating Binary Classifiers}} on {{Imbalanced Datasets}}},
  author = {Saito, Takaya and Rehmsmeier, Marc},
  year = {2015},
  month = apr,
  journal = {PLOS ONE},
  volume = {10},
  number = {3},
  pages = {e0118432},
  issn = {1932-6203},
  doi = {10/f69237},
  urldate = {2019-02-08},
  abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
  langid = {english},
  keywords = {Bioinformatics,Caenorhabditis elegans,Exponential functions,Genome analysis,Genome-wide association studies,Interpolation,MicroRNAs,Support vector machines},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZCWVN2Q8/saito_the_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/UCUTJ2N8/article.html}
}

@article{salehinejad_recent_2017,
  title = {Recent {{Advances}} in {{Recurrent Neural Networks}}},
  author = {Salehinejad, Hojjat and Sankar, Sharan and Barfett, Joseph and Colak, Errol and Valaee, Shahrokh},
  year = {2017},
  month = dec,
  journal = {arXiv:1801.01078 [cs]},
  eprint = {1801.01078},
  primaryclass = {cs},
  urldate = {2019-05-23},
  abstract = {Recurrent neural networks (RNNs) are capable of learning features and long term dependencies from sequential and time-series data. The RNNs have a stack of non-linear units where at least one connection between units forms a directed cycle. A well-trained RNN can model any dynamical system; however, training RNNs is mostly plagued by issues in learning long-term dependencies. In this paper, we present a survey on RNNs and several new advances for newcomers and professionals in the field. The fundamentals and recent advances are explained and the research challenges are introduced.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BCM7KHL4/salehinejad_recent_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/85HPS63L/1801.html}
}

@article{salman_adversarially_2020,
  title = {Do {{Adversarially Robust ImageNet Models Transfer Better}}?},
  author = {Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.08489 [cs, stat]},
  eprint = {2007.08489},
  primaryclass = {cs, stat},
  urldate = {2020-07-26},
  abstract = {Transfer learning is a widely-used paradigm in deep learning, where models pre-trained on standard datasets can be efficiently adapted to downstream tasks. Typically, better pre-trained models yield better transfer results, suggesting that initial accuracy is a key aspect of transfer learning performance. In this work, we identify another such aspect: we find that adversarially robust models, while less accurate, often perform better than their standard-trained counterparts when used for transfer learning. Specifically, we focus on adversarially robust ImageNet classifiers, and show that they yield improved accuracy on a standard suite of downstream classification tasks. Further analysis uncovers more differences between robust and standard models in the context of transfer learning. Our results are consistent with (and in fact, add to) recent hypotheses stating that robustness leads to improved feature representations. Our code and models are available at https://github.com/Microsoft/robust-models-transfer .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/X8G7W7GK/Salman et al. - 2020 - Do Adversarially Robust ImageNet Models Transfer B.pdf;/Users/antoniohortaribeiro/Zotero/storage/V2QYIVJN/2007.html}
}

@article{samek_evaluating_2015,
  title = {Evaluating the Visualization of What a {{Deep Neural Network}} Has Learned},
  author = {Samek, Wojciech and Binder, Alexander and Montavon, Gr{\'e}goire and Bach, Sebastian and M{\"u}ller, Klaus-Robert},
  year = {2015},
  month = sep,
  journal = {arXiv:1509.06321 [cs]},
  eprint = {1509.06321},
  primaryclass = {cs},
  abstract = {Deep Neural Networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multi-layer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the ''importance'' of individual pixels wrt the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012 and MIT Places data sets. Our main result is that the recently proposed Layer-wise Relevance Propagation (LRP) algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of neural network performance.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HQS8FXPZ/samek_evaluating_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/NCFGT3H3/1509.html}
}

@article{sangha_automated_2022,
  title = {Automated Multilabel Diagnosis on Electrocardiographic Images and Signals},
  author = {Sangha, Veer and Mortazavi, Bobak J. and Haimovich, Adrian D. and Ribeiro, Ant{\^o}nio H. and Brandt, Cynthia A. and Jacoby, Daniel L. and Schulz, Wade L. and Krumholz, Harlan M. and Ribeiro, Antonio Luiz P. and Khera, Rohan},
  year = {2022},
  journal = {Nature Communications},
  volume = {13},
  pages = {1583},
  doi = {10.1038/s41467-022-29153-3},
  annotation = {https://www.medrxiv.org/content/10.1101/2021.09.22.21263926v1},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XCLD9S3N/Sangha et al_2021_Automated Multilabel Diagnosis on Electrocardiographic Images and Signals.pdf;/Users/antoniohortaribeiro/Zotero/storage/34PZ7PRX/2021.09.22.html}
}

@article{sangha_detection_2023,
  title = {Detection of {{Left Ventricular Systolic Dysfunction}} from {{Electrocardiographic Images}}},
  author = {Sangha, Veer and Nargesi, Arash A. and Dhingra, Lovedeep S. and Mortazavi, Bobak J. and Ribeiro, Ant{\^o}nio H. and Brandt, Cynthia A. and Miller, Edward J. and Ribeiro, Antonio Luiz P. and Velazquez, Eric J. and Krumholz, Harlan M. and Khera, Rohan},
  year = {2023},
  journal = {Circulation},
  doi = {10.1161/CIRCULATIONAHA.122.062646},
  abstract = {BACKGROUND: Left ventricular (LV) systolic dysfunction is associated with a {$>$}8-fold increased risk of heart failure and a 2-fold risk of premature death. The use of ECG signals in screening for LV systolic dysfunction is limited by their availability to clinicians. We developed a novel deep learning--based approach that can use ECG images for the screening of LV systolic dysfunction. METHODS: Using 12-lead ECGs plotted in multiple different formats, and corresponding echocardiographic data recorded within 15 days from the Yale New Haven Hospital between 2015 and 2021, we developed a convolutional neural network algorithm to detect an LV ejection fraction {$<$}40\%. The model was validated within clinical settings at Yale New Haven Hospital and externally on ECG images from Cedars Sinai Medical Center in Los Angeles, CA; Lake Regional Hospital in Osage Beach, MO; Memorial Hermann Southeast Hospital in Houston, TX; and Methodist Cardiology Clinic of San Antonio, TX. In addition, it was validated in the prospective Brazilian Longitudinal Study of Adult Health. Gradient-weighted class activation mapping was used to localize class-discriminating signals in ECG images. RESULTS: Overall, 385\,601 ECGs with paired echocardiograms were used for model development. The model demonstrated high discrimination power across various ECG image formats and calibrations in internal validation (area under receiving operation characteristics [AUROCs], 0.91; area under precision-recall curve [AUPRC], 0.55); and external sets of ECG images from Cedars Sinai (AUROC, 0.90 and AUPRC, 0.53), outpatient Yale New Haven Hospital clinics (AUROC, 0.94 and AUPRC, 0.77), Lake Regional Hospital (AUROC, 0.90 and AUPRC, 0.88), Memorial Hermann Southeast Hospital (AUROC, 0.91 and AUPRC 0.88), Methodist Cardiology Clinic (AUROC, 0.90 and AUPRC, 0.74), and Brazilian Longitudinal Study of Adult Health cohort (AUROC, 0.95 and AUPRC, 0.45). An ECG suggestive of LV systolic dysfunction portended a {$>$}27-fold higher odds of LV systolic dysfunction on transthoracic echocardiogram (odds ratio, 27.5 [95\% CI, 22.3--33.9] in the held-out set). Class-discriminative patterns localized to the anterior and anteroseptal leads (V2 to V3), corresponding to the left ventricle regardless of the ECG layout. A positive ECG screen in individuals with an LV ejection fraction {$\geq$}40\% at the time of initial assessment was associated with a 3.9-fold increased risk of developing incident LV systolic dysfunction in the future (hazard ratio, 3.9 [95\% CI, 3.3--4.7]; median follow-up, 3.2 years). CONCLUSIONS: We developed and externally validated a deep learning model that identifies LV systolic dysfunction from ECG images. This approach represents an automated and accessible screening strategy for LV systolic dysfunction, particularly in low-resource settings.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6EHYQGWE/Sangha et al. - 2022 - Detection of Left Ventricular Systolic Dysfunction.pdf;/Users/antoniohortaribeiro/Zotero/storage/VAFETY52/2022.06.04.html}
}

@article{sangiorgio_robustness_2020,
  title = {Robustness of {{LSTM}} Neural Networks for Multi-Step Forecasting of Chaotic Time Series},
  author = {Sangiorgio, Matteo and Dercole, Fabio},
  year = {2020},
  month = oct,
  journal = {Chaos, Solitons \& Fractals},
  volume = {139},
  pages = {110045},
  issn = {0960-0779},
  doi = {10.1016/j.chaos.2020.110045},
  urldate = {2022-06-15},
  abstract = {Recurrent neurons (and in particular LSTM cells) demonstrated to be efficient when used as basic blocks to build sequence to sequence architectures, which represent the state-of-the-art approach in many sequential tasks related to natural language processing. In this work, these architectures are proposed as general purposes, multi-step predictors for nonlinear time series. We analyze artificial, noise-free data generated by chaotic oscillators and compare LSTM nets with the benchmarks set by feed-forward, one-step-recursive and multi-output predictors. We focus on two different training methods for LSTM nets. The traditional one makes use of the so-called teacher forcing, i.e. the ground truth data are used as input for each time step ahead, rather than the outputs predicted for the previous steps. Conversely, the second feeds the previous predictions back into the recurrent neurons, as it happens when the network is used in forecasting. LSTM predictors robustly show the strengths of the two benchmark competitors, i.e., the good short-term performance of one-step-recursive predictors and greatly improved mid-long-term predictions with respect to feed-forward, multi-output predictors. Training LSTM predictors without teacher forcing is recommended to improve accuracy and robustness, and ensures a more uniform distribution of the predictive power within the chaotic attractor. We also show that LSTM architectures maintain good performances when the number of time lags included in the input differs from the actual embedding dimension of the dataset, a feature that is very important when working on real data.},
  langid = {english},
  keywords = {Deterministic chaos,Exposure bias,Multi-step prediction,Nonlinear time series,Recurrent neural networks,Teacher forcing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PWXAJGTM/S0960077920304422.html}
}

@article{santurkar_how_2018,
  title = {How {{Does Batch Normalization Help Optimization}}?},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  year = {2018},
  month = may,
  journal = {arXiv:1805.11604 [cs, stat]},
  eprint = {1805.11604},
  primaryclass = {cs, stat},
  urldate = {2018-12-04},
  abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VYK2M4Y9/santurkar_how does_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/3TZTFWIM/1805.html}
}

@book{sarkka_bayesian_2013,
  title = {Bayesian Filtering and Smoothing},
  author = {S{\"a}rkk{\"a}, Simo},
  year = {2013},
  series = {Institute of {{Mathematical Statistics}} Textbooks},
  number = {3},
  publisher = {Cambridge University Press},
  address = {Cambridge, U.K. ; New York},
  isbn = {978-1-107-03065-7 978-1-107-61928-9},
  lccn = {QA279.5 .S27 2013},
  keywords = {Bayesian statistical decision theory,Filters (Mathematics),Smoothing (Statistics)},
  annotation = {OCLC: ocn840462877},
  file = {/Users/antoniohortaribeiro/Zotero/storage/576ZW9GS/sÃ¤rkkÃ¤_bayesian_2013.pdf}
}

@article{sarode_embedded_2015,
  title = {Embedded {{Multiple Shooting Methodology}} in a {{Genetic Algorithm Framework}} for {{Parameter Estimation}} and {{State Identification}} of {{Complex Systems}}},
  author = {Sarode, Ketan Dinkar and Kumar, V Ravi and Kulkarni, {\relax BD}},
  year = {2015},
  journal = {Chemical Engineering Science},
  volume = {134},
  pages = {605--618},
  issn = {0009-2509},
  doi = {10/f7mqwg}
}

@article{sassi_pdfecg_2017,
  title = {{{PDF-ECG}} in Clinical Practice: {{A}} Model for Long-Term Preservation of Digital 12-Lead {{ECG}} Data},
  shorttitle = {{{PDF-ECG}} in Clinical Practice},
  author = {Sassi, Roberto and Bond, Raymond R. and Cairns, Andrew and Finlay, Dewar D. and Guldenring, Daniel and Libretti, Guido and Isola, Lamberto and Vaglio, Martino and Poeta, Roberto and Campana, Marco and Cuccia, Claudio and Badilini, Fabio},
  year = {2017 Nov - Dec},
  journal = {Journal of Electrocardiology},
  volume = {50},
  number = {6},
  pages = {776--780},
  issn = {1532-8430},
  doi = {10.1016/j.jelectrocard.2017.08.001},
  abstract = {BACKGROUND: In clinical practice, data archiving of resting 12-lead electrocardiograms (ECGs) is mainly achieved by storing a PDF report in the hospital electronic health record (EHR). When available, digital ECG source data (raw samples) are only retained within the ECG management system. OBJECTIVE: The widespread availability of the ECG source data would undoubtedly permit successive analysis and facilitate longitudinal studies, with both scientific and diagnostic benefits. METHODS \& RESULTS: PDF-ECG is a hybrid archival format which allows to store in the same file both the standard graphical report of an ECG together with its source ECG data (waveforms). Using PDF-ECG as a model to address the challenge of ECG data portability, long-term archiving and documentation, a real-world proof-of-concept test was conducted in a northern Italy hospital. A set of volunteers undertook a basic ECG using routine hospital equipment and the source data captured. Using dedicated web services, PDF-ECG documents were then generated and seamlessly uploaded in the hospital EHR, replacing the standard PDF reports automatically generated at the time of acquisition. Finally, the PDF-ECG files could be successfully retrieved and re-analyzed. CONCLUSION: Adding PDF-ECG to an existing EHR had a minimal impact on the hospital's workflow, while preserving the ECG digital data.},
  langid = {english},
  pmid = {28843654},
  keywords = {Clinical ECG,ECG digital data,ECG long-time preservation,Electrocardiography,Electronic Health Records,Humans,Information Storage and Retrieval,PDF report,Software,Systems Integration,Workflow}
}

@article{sau_artificial_2024,
  title = {Artificial Intelligence--Enabled Electrocardiogram for Mortality and Cardiovascular Risk Estimation: {{An}} Actionable, Explainable and Biologically Plausible Platform},
  shorttitle = {Artificial Intelligence--Enabled Electrocardiogram for Mortality and Cardiovascular Risk Estimation},
  author = {Sau, Arunashis and Pastika, Libor and Sieliwonczyk, Ewa and Patlatzoglou, Konstantinos and Ribeiro, Antonio H. and McGurk, Kathryn A. and Zeidaabadi, Boroumand and Zhang, Henry and Macierzanka, Krzysztof and Mandic, Danilo and Sabino, Ester and Giatti, Luana and Barreto, Sandhi M. and Camelo, Lidyane do Valle and Tzoulaki, Ioanna and O'Regan, Declan P. and Peters, Nicholas S. and Ware, James S. and Ribeiro, Antonio Luiz P. and Kramer, Daniel B. and Waks, Jonathan W. and Ng, Fu Siong},
  year = {2024},
  month = jan,
  journal = {medXiv},
  pages = {2024.01.13.24301267},
  doi = {10.1101/2024.01.13.24301267},
  urldate = {2024-02-26},
  abstract = {Background and Aims Artificial intelligence-enhanced electrocardiograms (AI-ECG) can be used to predict risk of future disease and mortality but has not yet been adopted into clinical practice. Existing model predictions lack actionability at an individual patient level, explainability and biological plausibility. We sought to address these limitations of previous AI-ECG approaches by developing the AI-ECG risk estimator (AIRE) platform. Methods and Results The AIRE platform was developed in a secondary care dataset of 1,163,401 ECGs from 189,539 patients, using deep learning with a discrete-time survival model to create a subject-specific survival curve using a single ECG. Therefore, AIRE predicts not only risk of mortality, but time-to-mortality. AIRE was validated in five diverse, transnational cohorts from the USA, Brazil and the UK, including volunteers, primary care and secondary care subjects. AIRE accurately predicts risk of all-cause mortality (C-index 0.775 (0.773-0.776)), cardiovascular (CV) death 0.832 (0.831-0.834), non-CV death (0.749 (0.747-0.751)), future ventricular arrhythmia (0.760 (0.756-0.763)), future atherosclerotic cardiovascular disease (0.696 (0.694-0.698)) and future heart failure (0.787 (0.785-0.889))). Through phenome- and genome-wide association studies, we identified candidate biological pathways for the prediction of increased risk, including changes in cardiac structure and function, and genes associated with cardiac structure, biological aging and metabolic syndrome. Conclusion AIRE is an actionable, explainable and biologically plausible AI-ECG risk estimation platform that has the potential for use worldwide across a wide range of clinical contexts for short- and long-term risk estimation. {$<$}img class="highwire-fragment fragment-image" alt="Figure" src="https://www.medrxiv.org/content/medrxiv/early/2024/01/15/2024.01.13.24301267/F1.medium.gif" width="440" height="373"/{$>$}Download figureOpen in new tab},
  copyright = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KZZ2KSNF/Sau et al. - 2024 - Artificial intelligenceâ€“enabled electrocardiogram .pdf}
}

@article{sau_neural_2023,
  title = {Neural Network-Derived Electrocardiographic Features Have Prognostic Significance and Important Phenotypic and Genotypic Associations},
  author = {Sau, Arunashis and Ribeiro, Ant{\^o}nio H. and McGurk, Kathryn and Pastika, Libor and Bajaj, Nikesh and Ardissino, Maddalena and Chen, Jun Yu and Wu, Huiyi and Shi, Xili and Hnatkova, Katerina and Zheng, Sean and Britton, Annie and Shipley, Martin and Andr{\v s}ov{\'a}, Irena and Novotn{\'y}, Tom{\'a}{\v s} and Sabino, Ester and Giatti, Luana and Barreto, Sandhi and Waks, Jonathan and Kramer, Daniel and Mandic, Danilo and Peters, Nicholas and O'Regan, Declan and Malik, Marek and Ware, James and Ribeiro, Antonio L. P. and Ng, Fu Siong},
  year = {2023},
  journal = {medRxiv},
  copyright = {All rights reserved}
}

@article{sau_neural_2023a,
  title = {Neural Network-Derived Electrocardiographic Features Have Prognostic Significance and Important Phenotypic and Genotypic Associations},
  author = {Sau, A and Ribeiro, A H and Mcgurk, K A and Pastika, L and Chen, J Y and Ardissino, M and Sabino, E and Giatti, L and Barreto, S M and Mandic, D and Peters, N S and Malik, M and Ware, J and Ribeiro, A L P and Ng, F S},
  year = {2023},
  month = nov,
  journal = {European Heart Journal},
  volume = {44},
  number = {Supplement\_2},
  pages = {ehad655.2921},
  issn = {0195-668X},
  doi = {10.1093/eurheartj/ehad655.2921},
  urldate = {2024-06-04},
  abstract = {Subtle, prognostically-meaningful ECG features may not be apparent to physicians. In the course of supervised machine learning (ML) training, many thousands of ECG features are identified. These are not limited to conventional ECG parameters and morphology.To investigate novel neural network (NN)-derived ECG features, that may have clinical, phenotypic and genotypic associations and prognostic significance.We extracted 5120 NN-derived ECG features from an AI-ECG model trained for six simple diagnoses and applied unsupervised machine learning to identify three phenogroups. The derivation set, the Clinical Outcomes in Digital Electrocardiography (CODE) cohort (n = 1,558,421), is a database of ECGs recorded in primary care in Brazil. There were four external validation cohorts. A cohort of British civil servants (WH II, n = 5,066). A longitudinal study of volunteers in the UK (UK Biobank, n = 42,386). A longitudinal cohort of Brazilian public servants (ELSA-Brasil, n = 13,739). Lastly, a cohort of patients with chronic Chagas cardiomyopathy (SaMi-Trop, n = 1,631) .In the derivation cohort (CODE), the three phenogroups had significantly different mortality profiles (Figure 1). After adjusting for known covariates, phenogroup B had a 1.2-fold increase in long-term mortality compared to phenogroup A (HR 1.20, 95\% CI 1.17-1.23, p \&lt; 0.0001). We externally validated our findings in four diverse cohorts. Phenogroup C was poorly represented in the volunteer cohorts and therefore was excluded from those analyses. We found phenogroup B had a significantly greater risk of mortality in all cohorts (Figure 1). We performed a phenome-wide association study (PheWAS) in the UK Biobank. We found ECG phenogroup significantly associated with cardiac and non-cardiac phenotypes, including cardiac chamber volumes and cardiac output (Figure 2A). A single-trait genome-wide association study (GWAS) was conducted. The GWAS yielded four loci (Figure 2B). SCN10A, SCN5A and CAV1 have well described roles in cardiac conduction and arrhythmia. ARHGAP24 has been previously associated with ECG parameters, however, our analysis has identified for the first time ARHGAP24 as a gene associated with a prognostically significant phenogroup. Mendelian randomisation demonstrated the higher risk ECG phenogroup was causally associated with higher odds of atrioventricular (AV) block but lower odds of atrial fibrillation and ischaemic heart disease.NN-derived ECG features have important applications beyond the original model from which they are derived and may be transferable and applicable for risk prediction in a wide range of settings, in addition to mortality prediction. We have shown the significant potential of NN-derived ECG features, as a highly transferable and potentially universal risk marker, that may be applied to a wide range of clinical contexts.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5PUAPCHX/Sau et al. - 2023 - Neural network-derived electrocardiographic featur.pdf;/Users/antoniohortaribeiro/Zotero/storage/EUTE4UPN/7391359.html}
}

@book{savitch_absolute_2013,
  title = {Absolute {{Java}}: {{Walter Savitch}} ; Contributor, {{Kenrick Mock}}},
  shorttitle = {Absolute {{Java}}},
  author = {Savitch, Walter J. and Mock, Kenrick},
  year = {2013},
  edition = {5th ed},
  publisher = {Addison-Wesley},
  address = {Boston},
  isbn = {978-0-13-283031-7},
  lccn = {QA76.73.J38 S265 2013},
  keywords = {Java (Computer program language)},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BGG7VEFG/savitch_absolute_2013.pdf}
}

@article{saxe_exact_2014,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  year = {2014},
  month = feb,
  journal = {International Conference on Learning Representations (ICLR)},
  eprint = {1312.6120},
  urldate = {2020-08-27},
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/94KESFKR/Saxe et al. - 2014 - Exact solutions to the nonlinear dynamics of learn.pdf;/Users/antoniohortaribeiro/Zotero/storage/5IT3YTEB/1312.html}
}

@misc{scetbon_robust_2023,
  title = {Robust {{Linear Regression}}: {{Gradient-descent}}, {{Early-stopping}}, and {{Beyond}}},
  shorttitle = {Robust {{Linear Regression}}},
  author = {Scetbon, Meyer and Dohmatob, Elvis},
  year = {2023},
  month = jan,
  number = {arXiv:2301.13486},
  eprint = {2301.13486},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.13486},
  urldate = {2023-02-13},
  abstract = {In this work we study the robustness to adversarial attacks, of early-stopping strategies on gradient-descent (GD) methods for linear regression. More precisely, we show that early-stopped GD is optimally robust (up to an absolute constant) against Euclidean-norm adversarial attacks. However, we show that this strategy can be arbitrarily sub-optimal in the case of general Mahalanobis attacks. This observation is compatible with recent findings in the case of classification{\textasciitilde}{\textbackslash}cite\{Vardi2022GradientMP\} that show that GD provably converges to non-robust models. To alleviate this issue, we propose to apply instead a GD scheme on a transformation of the data adapted to the attack. This data transformation amounts to apply feature-depending learning rates and we show that this modified GD is able to handle any Mahalanobis attack, as well as more general attacks under some conditions. Unfortunately, choosing such adapted transformations can be hard for general attacks. To the rescue, we design a simple and tractable estimator whose adversarial risk is optimal up to within a multiplicative constant of 1.1124 in the population regime, and works for any norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/I83U8TUZ/Scetbon and Dohmatob - 2023 - Robust Linear Regression Gradient-descent, Early-.pdf;/Users/antoniohortaribeiro/Zotero/storage/GB7R34FU/2301.html}
}

@article{scharstein_taxonomy_2002,
  title = {A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms},
  author = {Scharstein, Daniel and Szeliski, Richard},
  year = {2002},
  journal = {International journal of computer vision},
  volume = {47},
  number = {1-3},
  pages = {7--42},
  keywords = {ðŸ”No DOI found},
  annotation = {00000}
}

@incollection{scherer_evaluation_2010,
  title = {Evaluation of {{Pooling Operations}} in {{Convolutional Architectures}} for {{Object Recognition}}},
  booktitle = {Artificial {{Neural Networks}} -- {{ICANN}}},
  author = {Scherer, Dominik and Muller, Andreas and Behnke, Sven},
  year = {2010},
  volume = {6354},
  pages = {92--101},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-15825-4_10},
  urldate = {2020-03-23},
  abstract = {A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant properties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57\% on the NORB normalized-uniform dataset and 5.6\% on the NORB jittered-cluttered dataset.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/V435F76T/Hutchison et al. - 2010 - Evaluation of Pooling Operations in Convolutional .pdf}
}

@article{schittkowski_convergence_1983,
  title = {On the Convergence of a Sequential Quadratic Programming Method with an Augmented Lagrangian Line Search Function},
  author = {Schittkowski, Klaus},
  year = {1983},
  month = jan,
  journal = {Mathematische Operationsforschung und Statistik. Series Optimization},
  volume = {14},
  number = {2},
  pages = {197--216},
  issn = {0323-3898},
  doi = {10.1080/02331938308842847},
  annotation = {00000}
}

@article{schittkowski_nonlinear_1982,
  title = {The Nonlinear Programming Method of {{Wilson}}, {{Han}}, and {{Powell}} with an Augmented {{Lagrangian}} Type Line Search Function. {{Part}} 1: {{Convergence}} Analysis.},
  author = {Schittkowski, Klaus},
  year = {1982},
  month = feb,
  journal = {Numerische Mathematik},
  volume = {38},
  number = {1},
  pages = {83--114},
  issn = {0029-599X, 0945-3245},
  doi = {10.1007/BF01395810},
  urldate = {2018-03-28},
  abstract = {SummaryThe paper represents an outcome of an extensive comparative study of nonlinear optimization algorithms. This study indicates that quadratic approximation methods which are characterized by solving a sequence of quadratic subproblems recursively, belong to the most efficient and reliable nonlinear programming algorithms available at present. The purpose of this paper is to analyse the theoretical convergence properties and to investigate the numerical performance in more detail. In Part 1, the exactL1-penalty function of Han and Powell is replaced by a differentiable augmented Lagrange function for the line search computation to be able to prove the global convergence and to show that the steplength one is chosen in the neighbourhood of a solution. In Part 2, the quadratic subproblem is exchanged by a linear least squares problem to improve the efficiency, and to test the dependence of the performance from different solution methods for the quadratic or least squares subproblem.},
  langid = {english},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6SEDVXV6/schittkowski_the_1982.pdf;/Users/antoniohortaribeiro/Zotero/storage/BDX688WM/BF01395810.html}
}

@article{schittkowski_nonlinear_1982a,
  title = {The Nonlinear Programming Method of {{Wilson}}, {{Han}}, and {{Powell}} with an Augmented {{Lagrangian}} Type Line Search Function. {{Part}} 2: {{An}} Efficient Implementation with Linear Least Squares Subproblems},
  author = {Schittkowski, Klaus},
  year = {1982},
  month = feb,
  journal = {Numerische Mathematik},
  volume = {38},
  number = {1},
  pages = {115--127},
  issn = {0029-599X, 0945-3245},
  doi = {10.1007/BF01395811},
  urldate = {2018-03-28},
  abstract = {SummaryThe paper represents an outcome of an extensive comparative study of nonlinear optimization algorithms. This study indicates that quadratic approximation methods which are characterized by solving a sequence of quadratic subproblems recursively, belong to the most efficient and reliable nonlinear programming algorithms available at present. The purpose of this paper is to analyse the theoretical convergence properties and to investigate the numerical performance in more detail. In Part 1, the exactL1-penalty function of Han and Powell is replaced by a differentiable augmented Lagrange function for the line search computation to the able to prove the global convergence and to show that the steplength one is chosen in the neighbourhood of a solution. In Part 2, the quadratic subproblem is exchanged by a linear least squares problem to improve the efficiency, and to test the dependence of the performance from different solution methods for the quadratic or least squares subproblems.},
  langid = {english},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ENRP3JDB/schittkowski_the_1982.pdf;/Users/antoniohortaribeiro/Zotero/storage/N3JBU6CM/BF01395811.html}
}

@article{schlapfer_computerinterpreted_2017,
  title = {Computer-{{Interpreted Electrocardiograms}}: {{Benefits}} and {{Limitations}}},
  author = {Schl{\"a}pfer, J{\"u}rg and Wellens, Hein J.},
  year = {2017},
  month = aug,
  journal = {Journal of the American College of Cardiology},
  volume = {70},
  number = {9},
  pages = {1183},
  doi = {10/gbs2q5},
  abstract = {Computerized interpretation of the electrocardiogram (CIE) was introduced to improve the correct interpretation of the electrocardiogram (ECG), facilitating health care decision making and reducing costs. Worldwide, millions of ECGs are recorded annually, with the majority automatically analyzed, followed by an immediate interpretation. Limitations in the diagnostic accuracy of CIE were soon recognized and still persist, despite ongoing improvement in ECG algorithms. Unfortunately, inexperienced physicians ordering the ECG may fail to recognize interpretation mistakes and accept the automated diagnosis without criticism. Clinical mismanagement may result, with the risk of exposing patients to useless~investigations or potentially dangerous treatment. Consequently, CIE over-reading and confirmation by an experienced ECG reader are essential and are repeatedly recommended in published reports. Implementation of new ECG knowledge is also important. The current status of automated ECG interpretation is reviewed, with suggestions for improvement.}
}

@article{scholkopf_kernel_1997,
  title = {Kernel {{Principal Component Analysis}}},
  author = {Scholkopf, Bernhard and Smola, Alexander and Muller, Klaus Robert},
  year = {1997},
  journal = {Artificial Neural Networks --- ICANN},
  pages = {583--588},
  doi = {10.1007/BFb0020217},
  abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can e ciently compute principal components in high dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/W5PWSNGA/Scholkopf et al. - Kernel Principal Component Analysis.pdf}
}

@phdthesis{schon_estimation_2006,
  title = {{Estimation of Nonlinear dynamic systems: theory and applications}},
  shorttitle = {{Estimation of Nonlinear dynamic systems}},
  author = {Sch{\"o}n, Thomas B},
  year = {2006},
  address = {Link{\"o}ping},
  langid = {Med sammanfattning p{\aa} svenska.},
  school = {Univ.},
  annotation = {00000 \\
OCLC: 185211029},
  file = {/Users/antoniohortaribeiro/Zotero/storage/A25U46XI/schÃ¶n_estimation_2006.pdf}
}

@misc{schon_manipulating_2011,
  title = {Manipulating the Multivariate Gaussian Density},
  author = {Sch{\"o}n, Thomas B. and Lindsten, Fredrik},
  year = {2011},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VJNH74WS/schÃ¶n_manipulati_2011.pdf}
}

@article{schon_system_2011,
  title = {System Identification of Nonlinear State-Space Models},
  author = {Sch{\"o}n, Thomas B. and Wills, Adrian and Ninness, Brett},
  year = {2011},
  month = jan,
  journal = {Automatica},
  volume = {47},
  number = {1},
  pages = {39--49},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2010.10.013},
  abstract = {This paper is concerned with the parameter estimation of a general class of nonlinear dynamic systems in state-space form. More specifically, a Maximum Likelihood (ML) framework is employed and an Expectation Maximisation (EM) algorithm is derived to compute these ML estimates. The Expectation (E) step involves solving a nonlinear state estimation problem, where the smoothed estimates of the states are required. This problem lends itself perfectly to the particle smoother, which provides arbitrarily good estimates. The maximisation (M) step is solved using standard techniques from numerical optimisation theory. Simulation examples demonstrate the efficacy of our proposed solution.},
  keywords = {Dynamic systems,Expectation maximisation algorithm,Monte Carlo method,Nonlinear models,Particle methods,Smoothing filters,system identification},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MC6GCVG2/schÃ¶n_system_2011.pdf;/Users/antoniohortaribeiro/Zotero/storage/VQNJCTWQ/schÃ¶n_system_2011.pdf;/Users/antoniohortaribeiro/Zotero/storage/4GTCJ4AP/S0005109810004279.html;/Users/antoniohortaribeiro/Zotero/storage/VE49T4UT/S0005109810004279.html}
}

@article{schoukens_cascaded_2016,
  title = {Cascaded Tanks Benchmark Combining Soft and Hard Nonlinearities},
  author = {Schoukens, M and Mattsson, P and Wigren, T and Noel, J P},
  year = {2016},
  pages = {4},
  langid = {english},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4WFK4V7V/Schoukens et al. - 2016 - Cascaded tanks benchmark combining soft and hard n.pdf}
}

@article{schoukens_identification_2017,
  title = {Identification of Block-Oriented Nonlinear Systems Starting from Linear Approximations: {{A}} Survey},
  shorttitle = {Identification of Block-Oriented Nonlinear Systems Starting from Linear Approximations},
  author = {Schoukens, Maarten and Tiels, Koen},
  year = {2017},
  month = nov,
  journal = {Automatica},
  volume = {85},
  pages = {272--292},
  issn = {00051098},
  doi = {10.1016/j.automatica.2017.06.044},
  abstract = {Block-oriented nonlinear models are popular in nonlinear system identification because of their advantages of being simple to understand and easy to use. Many different identification approaches were developed over the years to estimate the parameters of a wide range of block-oriented nonlinear models. One class of these approaches uses linear approximations to initialize the identification algorithm. The best linear approximation framework and the \${\textbackslash}epsilon\$-approximation framework, or equivalent frameworks, allow the user to extract important information about the system, guide the user in selecting good candidate model structures and orders, and prove to be a good starting point for nonlinear system identification algorithms. This paper gives an overview of the different block-oriented nonlinear models that can be identified using linear approximations, and of the identification algorithms that have been developed in the past. A non-exhaustive overview of the most important other block-oriented nonlinear system identification approaches is also provided throughout this paper.},
  keywords = {Computer Science - Systems and Control},
  file = {/Users/antoniohortaribeiro/Zotero/storage/88HZWGDP/schoukens_identifica_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/E9F8SP4X/1607.html}
}

@article{schoukens_nonlinear_2019,
  title = {Nonlinear {{System Identification}}: {{A User-Oriented Road Map}}},
  shorttitle = {Nonlinear {{System Identification}}},
  author = {Schoukens, Johan and Ljung, Lennart},
  year = {2019},
  month = dec,
  journal = {IEEE Control Systems Magazine},
  volume = {39},
  number = {6},
  pages = {28--99},
  issn = {1941-000X},
  doi = {10.1109/MCS.2019.2938121},
  abstract = {Nonlinear system identification is an extremely broad topic, since every system that is not linear is nonlinear. That makes it impossible to give a full overview of all aspects of the fi eld. For this reason, the selection of topics and the organization of the discussion are strongly colored by the personal journey of the authors in this nonlinear universe.},
  keywords = {Computational modeling,Data models,Nonlinear distortion,Nonlinear systems,Oscillators,Predictive models},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NHQA9AMH/Schoukens and Ljung - 2019 - Nonlinear System Identification A User-Oriented R.pdf;/Users/antoniohortaribeiro/Zotero/storage/YNSX5HJ7/8897147.html}
}

@article{schoukens_nonlinear_2019a,
  title = {Nonlinear {{System Identification}}: {{A User-Oriented Roadmap}}},
  shorttitle = {Nonlinear {{System Identification}}},
  author = {Schoukens, Johan and Ljung, Lennart},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.00683 [cs]},
  eprint = {1902.00683},
  primaryclass = {cs},
  urldate = {2019-04-08},
  abstract = {The goal of this article is twofold. Firstly, nonlinear system identification is introduced to a wide audience, guiding practicing engineers and newcomers in the field to a sound solution of their data driven modeling problems for nonlinear dynamic systems. In addition, the article also provides a broad perspective on the topic to researchers that are already familiar with the linear system identification theory, showing the similarities and differences between the linear and nonlinear problem. The reader will be referred to the existing literature for detailed mathematical explanations and formal proofs. Here the focus is on the basic philosophy, giving an intuitive understanding of the problems and the solutions, by making a guided tour along the wide range of user choices in nonlinear system identification. Guidelines will be given in addition to many examples, to reach that goal.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Systems and Control},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GEKZAQUK/schoukens_nonlinear_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/EIN29TF8/1902.html}
}

@article{schoukens_parametric_2015,
  title = {Parametric Identification of Parallel {{Wiener-Hammerstein}} Systems},
  author = {Schoukens, Maarten and Marconato, Anna and Pintelon, Rik and Vandersteen, Gerd and Rolain, Yves},
  year = {2015},
  month = jan,
  journal = {Automatica},
  volume = {51},
  eprint = {1708.06543},
  pages = {111--122},
  issn = {00051098},
  doi = {10/f6w4rj},
  urldate = {2018-11-20},
  abstract = {Block-oriented nonlinear models are popular in nonlinear modeling because of their advantages to be quite simple to understand and easy to use. To increase the flexibility of single branch block-oriented models, such as Hammerstein, Wiener, and Wiener-Hammerstein models, parallel block-oriented models can be considered. This paper presents a method to identify parallel Wiener-Hammerstein systems starting from input-output data only. In the first step, the best linear approximation is estimated for different input excitation levels. In the second step, the dynamics are decomposed over a number of parallel orthogonal branches. Next, the dynamics of each branch are partitioned into a linear time invariant subsystem at the input and a linear time invariant subsystem at the output. This is repeated for each branch of the model. The static nonlinear part of the model is also estimated during this step. The consistency of the proposed initialization procedure is proven. The method is validated on real-world measurements using a custom built parallel Wiener-Hammerstein test system.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Systems and Control},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6QMTNSZY/schoukens_parametric_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/CWQWRC2G/1708.html}
}

@article{schoukens_wienerhammerstein_,
  title = {Wiener-{{Hammerstein Benchmark}}},
  author = {Schoukens, Johan and Suykens, Johan and Ljung, Lennart},
  pages = {4},
  abstract = {This paper describes a benchmark for nonlinear system identification. A Wiener-Hammerstein system is selected as test object. In such a structure there is no direct access to the static nonlinearity starting from the measured input/output, because it is sandwiched between two unknown dynamic systems. The signal-to-noise ratio of the measurements is quite high, which puts the focus of the benchmark on the ability to identify the nonlinear behaviour, and not so much on the noise rejection properties. The benchmark is not intended as a competition, but as a tool to compare the possibilities of different methods to deal with this specific nonlinear structure.},
  langid = {english},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TUY8QV45/Schoukens et al. - Wiener-Hammerstein Benchmark.pdf}
}

@article{schoukens_wienerhammerstein_2016,
  title = {Wiener-{{Hammerstein}} Benchmark with Process Noise},
  author = {Schoukens, M and Noel, J P},
  year = {2016},
  pages = {5},
  langid = {english},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/LDM7CYL9/Schoukens and Noel - 2016 - Wiener-Hammerstein benchmark with process noise.pdf}
}

@inproceedings{schoukens_wienerhammerstein_2016a,
  title = {Wiener-{{Hammerstein}} Benchmark with Process Noise},
  booktitle = {Workshop on {{Nonlinear System Identification Benchmarks}}, {{Brussels}}},
  author = {Schoukens, M. and No{\"e}l, J. P.},
  year = {2016},
  pages = {15--19},
  urldate = {2017-08-28},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6UTMRVUP/schoukens_wiener-ham_2016.pdf}
}

@inproceedings{schroff_facenet_2015,
  title = {{{FaceNet}}: {{A Unified Embedding}} for {{Face Recognition}} and {{Clustering}}},
  shorttitle = {{{FaceNet}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year = {2015},
  pages = {815--823},
  urldate = {2018-01-26},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4PU522P4/schroff_facenet_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/8C5R8DRZ/schroff_facenet_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/H2DZAPT7/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html}
}

@article{schwarz_estimating_1978,
  title = {Estimating the Dimension of a Model},
  author = {Schwarz, Gideon},
  year = {1978},
  journal = {The annals of statistics},
  volume = {6},
  number = {2},
  pages = {461--464},
  issn = {0090-5364},
  keywords = {â“Multiple DOI},
  annotation = {00000}
}

@article{scott_limited_2021,
  title = {Limited Haplotype Diversity Underlies Polygenic Trait Architecture across 70\,Years of Wheat Breeding},
  author = {Scott, Michael F. and Fradgley, Nick and Bentley, Alison R. and Brabbs, Thomas and Corke, Fiona and Gardner, Keith A. and Horsnell, Richard and Howell, Phil and Ladejobi, Olufunmilayo and Mackay, Ian J. and Mott, Richard and Cockram, James},
  year = {2021},
  month = may,
  journal = {Genome Biology},
  volume = {22},
  number = {1},
  pages = {137},
  issn = {1474-760X},
  doi = {10.1186/s13059-021-02354-7},
  urldate = {2022-02-28},
  abstract = {Selection has dramatically shaped genetic and phenotypic variation in bread wheat. We can assess the genomic basis of historical phenotypic changes, and the potential for future improvement, using experimental populations that attempt to undo selection through the randomizing effects of recombination.},
  keywords = {Genomic prediction,GWAS,Imputation,Low-coverage whole-genome sequencing,MAGIC,Multi-parent population,Phenomics,Pleiotropy,Wheat},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VNQNYG6H/Scott et al_2021_Limited haplotype diversity underlies polygenic trait architecture across.pdf;/Users/antoniohortaribeiro/Zotero/storage/NMS6PHH4/s13059-021-02354-7.html}
}

@article{selvaraju_gradcam_2020,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2020},
  month = feb,
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  urldate = {2021-03-24},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/E87JSVH6/Selvaraju et al_2020_Grad-CAM.pdf;/Users/antoniohortaribeiro/Zotero/storage/S66CPA8U/1610.html}
}

@book{sen_principles_2007,
  title = {Principles of Electric Machines and Power Electronics},
  author = {Sen, P.C.},
  year = {2007},
  publisher = {Wiley India Pvt. Limited},
  isbn = {978-81-265-1101-3},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WQJZVKAC/sen_principles_2007.pdf}
}

@article{sermanet_overfeat_2013,
  title = {{{OverFeat}}: {{Integrated Recognition}}, {{Localization}} and {{Detection}} Using {{Convolutional Networks}}},
  shorttitle = {{{OverFeat}}},
  author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
  year = {2013},
  month = dec,
  journal = {arXiv:1312.6229 [cs]},
  eprint = {1312.6229},
  primaryclass = {cs},
  abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3DI2QCRT/sermanet_overfeat_2013.pdf;/Users/antoniohortaribeiro/Zotero/storage/7M2AIP3Z/1312.html}
}

@article{shah_errors_2007,
  title = {Errors in the Computerized Electrocardiogram Interpretation of Cardiac Rhythm.},
  author = {Shah, Atman P. and Rubin, Stanley A.},
  year = {2007 Sep-Oct},
  journal = {Journal of Electrocardiology},
  volume = {40},
  number = {5},
  pages = {385--390},
  issn = {1532-8430 0022-0736},
  doi = {10.1016/j.jelectrocard.2007.03.008},
  abstract = {BACKGROUND: More than 100 million computer-interpreted electrocardiograms (ECG-C) are obtained annually. However, there are few contemporary published data on the  accuracy of cardiac rhythm interpretation by this method. PURPOSE: The purpose of this study is to determine the accuracy of ECG-C rhythm interpretation in a typical patient population. METHODS: We compared the ECG-C rhythm interpretation  to that of 2 expert overreaders in 2112 randomly selected standard 12-lead ECGs.  RESULTS: The ECG-C correctly interpreted the rhythm in 1858 and incorrectly identified the rhythm in 254 (overall accuracy, 88.0\%). Sinus rhythm was correctly interpreted in 95.0\% of the ECGs (1666/1753) with this rhythm, whereas  nonsinus rhythms were correctly interpreted with an accuracy of only 53.5\% (192/359) (P {$<$} .0001). The ECG-C interpreted sinus rhythm with a sensitivity of 95\% (confidence interval, 93.8-96.7), specificity of 66.3\%, and positive predictive value of 93.2\%. The ECG-C interpreted nonsinus rhythms with a sensitivity of 72\%, (confidence interval, 68.7-73.7), a specificity of 93\%, and a positive predictive value of 59.3\%. Of the 254 ECGs that had incorrect rhythm interpretation, additional major errors were noted in 137 (54\%). CONCLUSIONS: The},
  langid = {english},
  pmid = {17531257},
  keywords = {{Arrhythmias, Cardiac/*diagnosis/epidemiology},{Diagnosis, Computer-Assisted/*methods},{Quality Assurance, Health Care/*methods},*Artifacts,California,Electrocardiography/*methods/*statistics \& numerical data,Humans,Observer Variation,Reproducibility of Results,Retrospective Studies,Sensitivity and Specificity}
}

@article{shaham_understanding_2018,
  title = {Understanding Adversarial Training: {{Increasing}} Local Stability of Supervised Models through Robust Optimization},
  author = {Shaham, Uri and Yamada, Yutaro and Negahban, Sahand},
  year = {2018},
  journal = {Neurocomputing},
  volume = {307},
  pages = {195--204},
  publisher = {Elsevier},
  issn = {0925-2312}
}

@book{shalev-shwartz_understanding_2014,
  title = {Understanding {{Machine Learning}}: {{From Theory}} to {{Algorithms}}},
  shorttitle = {Understanding {{Machine Learning}}},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9781107298019},
  urldate = {2020-05-06},
  isbn = {978-1-107-29801-9},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/F6IPWUWC/Shalev-Shwartz and Ben-David - 2014 - Understanding Machine Learning From Theory to Alg.pdf}
}

@article{shanmugam_multiple_2018,
  title = {Multiple {{Instance Learning}} for {{ECG Risk Stratification}}},
  author = {Shanmugam, Divya and Blalock, Davis and Gong, Jen G. and Guttag, John},
  year = {2018},
  month = dec,
  urldate = {2018-12-10},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6QDRLZ8I/shanmugam_multiple_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/5JD6RXGU/1812.html}
}

@article{shanmugam_multiple_2018a,
  title = {Multiple {{Instance Learning}} for {{ECG Risk Stratification}}},
  author = {Shanmugam, Divya and Blalock, Davis and Gong, Jen G. and Guttag, John},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.00475 [cs, stat]},
  eprint = {1812.00475},
  primaryclass = {cs, stat},
  urldate = {2018-12-10},
  abstract = {In this paper, we apply a multiple instance learning paradigm to signal-based risk stratification for cardiovascular outcomes. In contrast to methods that require hand-crafted features or domain knowledge, our method learns a representation with state-of-the-art predictive power from the raw ECG signal. We accomplish this by leveraging the multiple instance learning framework. This framework is particularly valuable to learning from biometric signals, where patient-level labels are available but signal segments are rarely annotated. We make two contributions in this paper: 1) reframing risk stratification for cardiovascular death (CVD) as a multiple instance learning problem, and 2) using this framework to design a new risk score, for which patients in the highest quartile are 15.9 times more likely to die of CVD within 90 days of hospital admission for an acute coronary syndrome.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {â›” No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{shanmugam_multiple_2018b,
  title = {Multiple {{Instance Learning}} for {{ECG Risk Stratification}}},
  author = {Shanmugam, Divya and Blalock, Davis and Gong, Jen G. and Guttag, John},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.00475 [cs, stat]},
  eprint = {1812.00475},
  primaryclass = {cs, stat},
  urldate = {2018-12-13},
  abstract = {In this paper, we apply a multiple instance learning paradigm to signal-based risk stratification for cardiovascular outcomes. In contrast to methods that require hand-crafted features or domain knowledge, our method learns a representation with state-of-the-art predictive power from the raw ECG signal. We accomplish this by leveraging the multiple instance learning framework. This framework is particularly valuable to learning from biometric signals, where patient-level labels are available but signal segments are rarely annotated. We make two contributions in this paper: 1) reframing risk stratification for cardiovascular death (CVD) as a multiple instance learning problem, and 2) using this framework to design a new risk score, for which patients in the highest quartile are 15.9 times more likely to die of CVD within 90 days of hospital admission for an acute coronary syndrome.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WW2JGM25/shanmugam_multiple_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/Q6JJ3KDK/1812.html}
}

@inproceedings{sharif_suitability_2018,
  title = {On the {{Suitability}} of {{Lp-Norms}} for {{Creating}} and {{Preventing Adversarial Examples}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Sharif, Mahmood and Bauer, Lujo and Reiter, Michael K.},
  year = {2018},
  pages = {1605--1613},
  urldate = {2021-05-15},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FLKCUL4N/Sharif et al. - 2018 - On the Suitability of Lp-Norms for Creating and Pr.pdf;/Users/antoniohortaribeiro/Zotero/storage/67ZEV5MM/Sharif_On_the_Suitability_CVPR_2018_paper.html}
}

@inproceedings{shashikumar_deep_2017,
  title = {A Deep Learning Approach to Monitoring and Detecting Atrial Fibrillation Using Wearable Technology},
  booktitle = {Proceedings of the {{IEEE EMBS International Conference}} on {{Biomedical}} \& {{Health Informatics}} ({{BHI}})},
  author = {Shashikumar, Supreeth Prajwal and Shah, Amit J. and Li, Qiao and Clifford, Gari D. and Nemati, Shamim},
  year = {2017},
  pages = {141--144},
  publisher = {IEEE},
  address = {Orland, FL, USA},
  doi = {10.1109/BHI.2017.7897225},
  urldate = {2018-10-21},
  abstract = {Atrial Fibrillation (AF) is the most common cardiac arrhythmia in clinical practice, with a prevalence of 2\% in the community. Not only it is associated with reduced quality of life, but also increased risk of stroke and myocardial infarction. Unfortunately, many cases of AF are clinically silent and undiagnosed, but long-term monitoring is difficult. Nonetheless, efforts at monitoring at-risk individuals and detecting clinically silent AF may yield significant public health benefit, as individuals with new-onset, asymptomatic AF would receive preventive therapies with anticoagulants and beta-blockers, for example.},
  isbn = {978-1-5090-4179-4},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CTKC9LN3/shashikumar_a deep_2017.pdf}
}

@inproceedings{shashikumar_detection_2018,
  title = {Detection of {{Paroxysmal Atrial Fibrillation Using Attention-based Bidirectional Recurrent Neural Networks}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Shashikumar, Supreeth P. and Shah, Amit J. and Clifford, Gari D. and Nemati, Shamim},
  year = {2018},
  series = {{{KDD}} '18},
  pages = {715--723},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/3219819.3219912},
  urldate = {2018-10-20},
  abstract = {Detection of atrial fibrillation (AF), a type of cardiac arrhythmia, is difficult since many cases of AF are usually clinically silent and undiagnosed. In particular paroxysmal AF is a form of AF that occurs occasionally, and has a higher probability of being undetected. In this work, we present an attention based deep learning framework for detection of paroxysmal AF episodes from a sequence of windows. Time-frequency representation of 30 seconds recording windows, over a 10 minute data segment, are fed sequentially into a deep convolutional neural network for image-based feature extraction, which are then presented to a bidirectional recurrent neural network with an attention layer for AF detection. To demonstrate the effectiveness of the proposed framework for transient AF detection, we use a database of 24 hour Holter Electrocardiogram (ECG) recordings acquired from 2850 patients at the University of Virginia heart station. The algorithm achieves an AUC of 0.94 on the testing set, which exceeds the performance of baseline models. We also demonstrate the cross-domain generalizablity of the approach by adapting the learned model parameters from one recording modality (ECG) to another (photoplethysmogram) with improved AF detection performance. The proposed high accuracy, low false alarm algorithm for detecting paroxysmal AF has potential applications in long-term monitoring using wearable sensors.},
  isbn = {978-1-4503-5552-0},
  keywords = {atrial fibrillation,convolutional neural network,deep learning,recurrent neural network,transfer learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HRJA3AP5/shashikumar_detection_2018.pdf}
}

@misc{shen_outofdistribution_2021,
  title = {Towards {{Out-Of-Distribution Generalization}}: {{A Survey}}},
  shorttitle = {Towards {{Out-Of-Distribution Generalization}}},
  author = {Shen, Zheyan and Liu, Jiashuo and He, Yue and Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Cui, Peng},
  year = {2021},
  month = aug,
  number = {arXiv:2108.13624},
  eprint = {2108.13624},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.13624},
  urldate = {2023-04-04},
  abstract = {Classic machine learning methods are built on the \$i.i.d.\$ assumption that training and testing data are independent and identically distributed. However, in real scenarios, the \$i.i.d.\$ assumption can hardly be satisfied, rendering the sharp drop of classic machine learning algorithms' performances under distributional shifts, which indicates the significance of investigating the Out-of-Distribution generalization problem. Out-of-Distribution (OOD) generalization problem addresses the challenging setting where the testing distribution is unknown and different from the training. This paper serves as the first effort to systematically and comprehensively discuss the OOD generalization problem, from the definition, methodology, evaluation to the implications and future directions. Firstly, we provide the formal definition of the OOD generalization problem. Secondly, existing methods are categorized into three parts based on their positions in the whole learning pipeline, namely unsupervised representation learning, supervised model learning and optimization, and typical methods for each category are discussed in detail. We then demonstrate the theoretical connections of different categories, and introduce the commonly used datasets and evaluation metrics. Finally, we summarize the whole literature and raise some future directions for OOD generalization problem. The summary of OOD generalization methods reviewed in this survey can be found at http://out-of-distribution-generalization.com.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SHWC7DJJ/Shen et al_2021_Towards Out-Of-Distribution Generalization.pdf;/Users/antoniohortaribeiro/Zotero/storage/5QC3465S/2108.html}
}

@article{sheng_trust_2016,
  title = {A Trust Region {{SQP}} Method for Coordinated Voltage Control in Smart Distribution Grid},
  author = {Sheng, Wanxing and Liu, Ke-Yan and Cheng, Sheng and Meng, Xiaoli and Dai, Wei},
  year = {2016},
  journal = {IEEE Transactions on Smart Grid},
  volume = {7},
  number = {1},
  pages = {381--391},
  doi = {10.1109/TSG.2014.2376197},
  annotation = {00000}
}

@article{shi_neural_2018,
  title = {Neural {{Lander}}: {{Stable Drone Landing Control}} Using {{Learned Dynamics}}},
  shorttitle = {Neural {{Lander}}},
  author = {Shi, Guanya and Shi, Xichen and O'Connell, Michael and Yu, Rose and Azizzadenesheli, Kamyar and Anandkumar, Animashree and Yue, Yisong and Chung, Soon-Jo},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.08027 [cs]},
  eprint = {1811.08027},
  primaryclass = {cs},
  urldate = {2019-06-01},
  abstract = {Precise near-ground trajectory control is difficult for multi-rotor drones, due to the complex aerodynamic effects caused by interactions between multi-rotor airflow and the environment. Conventional control methods often fail to properly account for these complex effects and fall short in accomplishing smooth landing. In this paper, we present a novel deep-learning-based robust nonlinear controller (Neural Lander) that improves control performance of a quadrotor during landing. Our approach combines a nominal dynamics model with a Deep Neural Network (DNN) that learns high-order interactions. We apply spectral normalization (SN) to constrain the Lipschitz constant of the DNN. Leveraging this Lipschitz property, we design a nonlinear feedback linearization controller using the learned model and prove system stability with disturbance rejection. To the best of our knowledge, this is the first DNN-based nonlinear feedback controller with stability guarantees that can utilize arbitrarily large neural nets. Experimental results demonstrate that the proposed controller significantly outperforms a Baseline Nonlinear Tracking Controller in both landing and cross-table trajectory tracking cases. We also empirically show that the DNN generalizes well to unseen data outside the training domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/H3PZW4HV/shi_neural_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/9TBVVJN2/1811.html}
}

@article{signoretto_tensor_2011,
  title = {Tensor {{Versus Matrix Completion}}: {{A Comparison With Application}} to {{Spectral Data}}},
  shorttitle = {Tensor {{Versus Matrix Completion}}},
  author = {Signoretto, M. and de Plas, R. Van and Moor, B. De and Suykens, J. A. K.},
  year = {2011},
  month = jul,
  journal = {IEEE Signal Processing Letters},
  volume = {18},
  number = {7},
  pages = {403--406},
  issn = {1070-9908},
  doi = {10.1109/LSP.2011.2151856},
  abstract = {Tensor completion recently emerged as a generalization of matrix completion for higher order arrays. This problem formulation allows one to exploit the structure of data that intrinsically have multiple dimensions. In this work, we recall a convex formulation for minimum (multilinear) ranks completion of arrays of arbitrary order. Successively we focus on completion of partially observed spectral images; the latter can be naturally represented as third order tensors and typically exhibit intraband correlations. We compare different convex formulations and assess them through case studies.},
  keywords = {arbitrary order,Arrays,convex formulation,convex programming,higher order arrays,Hyperspectral imaging,image processing,image reconstruction,Indexes,intraband correlations,Least squares approximation,matrix completion,matrix completion generalization,Matrix decomposition,minimum ranks completion,multilinear ranks completion,partially observed spectral images,spectral data,Tensile stress,tensor completion,tensors},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZGAP2RKB/signoretto_tensor_2011.pdf;/Users/antoniohortaribeiro/Zotero/storage/5TJR5CW8/5764499.html;/Users/antoniohortaribeiro/Zotero/storage/QSAHCMRT/5764499.html}
}

@article{sigurgeirsson_transport_2003,
  title = {Transport Coefficients of Hard Sphere Fluids},
  author = {Sigurgeirsson, H. and Heyes, D. M.},
  year = {2003},
  journal = {Molecular Physics},
  volume = {101},
  number = {3},
  pages = {469--482},
  doi = {10.1080/0026897021000037717},
  urldate = {2017-09-18},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/T8FZDKSJ/sigurgeirsson_transport_2003.pdf}
}

@book{silberschatz_operating_2014,
  title = {Operating System Concepts Essentials},
  author = {Silberschatz, Abraham and Galvin, Peter Baer and Gagne, Greg},
  year = {2014},
  publisher = {John Wiley \& Sons, Inc.},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9HETUAHQ/silberschatz_operating_2014.pdf}
}

@article{silverstein_empirical_1995,
  title = {On the {{Empirical Distribution}} of {{Eigenvalues}} of a {{Class}} of {{Large Dimensional Random Matrices}}},
  author = {Silverstein, J.W. and Bai, Z.D.},
  year = {1995},
  month = aug,
  journal = {Journal of Multivariate Analysis},
  volume = {54},
  number = {2},
  pages = {175--192},
  issn = {0047259X},
  doi = {10.1006/jmva.1995.1051},
  urldate = {2020-12-22},
  abstract = {A stronger result on the limiting distribution of the eigenvalues of random Hermitian matrices of the form A+XT X{$\ast$}, originally studied in Mar{\textasciicaron}cenko and Pastur [4], is presented. Here, X (N {\texttimes}n), T (n{\texttimes}n), and A (N {\texttimes}N ) are independent, with X containing i.i.d. entries having finite second moments, T is diagonal with real (diagonal) entries, A is Hermitian, and n/N {$\rightarrow$} c {$>$} 0 as N {$\rightarrow$} {$\infty$}. Under addtional assumptions on the eigenvalues of A and T , almost sure convergence of the empirical distribution function of the eigenvalues of A + XT X{$\ast$} is proven with the aid of Stieltjes transforms, taking a more direct approach than previous methods.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/C5AUESKG/Silverstein and Bai - 1995 - On the Empirical Distribution of Eigenvalues of a .pdf}
}

@incollection{simard_fixed_1989,
  title = {Fixed {{Point Analysis}} for {{Recurrent Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 1},
  author = {Simard, Patrice Y. and Ottaway, Mary B. and Ballard, Dana H.},
  editor = {Touretzky, D. S.},
  year = {1989},
  pages = {149--159},
  publisher = {Morgan-Kaufmann},
  urldate = {2019-10-14},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SCA58GSL/Simard et al. - 1989 - Fixed Point Analysis for Recurrent Networks.pdf;/Users/antoniohortaribeiro/Zotero/storage/EQAFRS2M/181-fixed-point-analysis-for-recurrent-networks.html}
}

@article{simonyan_deep_2013,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2013},
  month = dec,
  journal = {arXiv:1312.6034 [cs]},
  eprint = {1312.6034},
  primaryclass = {cs},
  urldate = {2018-11-15},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XV89WLW5/simonyan_deep_2013.pdf;/Users/antoniohortaribeiro/Zotero/storage/CEVTE6NZ/1312.html}
}

@article{simonyan_very_2014,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2014},
  month = sep,
  journal = {arXiv:1409.1556 [cs]},
  eprint = {1409.1556},
  primaryclass = {cs},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Z85GXW78/simonyan_very deep_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/2TNQ3BDZ/1409.html}
}

@inproceedings{sinai_notion_1959,
  title = {On the Notion of Entropy of a Dynamical System},
  booktitle = {Dokl. {{Akad}}. {{Nauk}}. {{SSSR}}},
  author = {Sinai, Yaha G},
  year = {1959},
  volume = {124},
  pages = {768},
  keywords = {â›” No DOI found}
}

@article{singh_identification_2013,
  title = {Identification on Non Linear Series-Parallel Model Using Neural Network},
  author = {Singh, Manu and Singh, Isha and Verma, A},
  year = {2013},
  journal = {MIT Int. J. Electr. Instrumen. Eng},
  volume = {3},
  number = {1},
  pages = {21--23},
  keywords = {ðŸ”No DOI found},
  annotation = {00000}
}

@article{singya_mitigating_2017,
  title = {Mitigating {{NLD}} for {{Wireless Networks}}: {{Effect}} of {{Nonlinear Power Amplifiers}} on {{Future Wireless Communication Networks}}},
  shorttitle = {Mitigating {{NLD}} for {{Wireless Networks}}},
  author = {Singya, Praveen Kumar and Kumar, Nagendra and Bhatia, Vimal},
  year = {2017},
  month = jul,
  journal = {IEEE Microwave Magazine},
  volume = {18},
  number = {5},
  pages = {73--90},
  issn = {1557-9581},
  doi = {10.1109/MMM.2017.2691423},
  abstract = {Efficient utilization of limited bandwidth with high-data-rate transmission while serving a large number of users is a prime requirement for present and future wireless communication systems. To meet this rising demand, orthogonal frequency-division multiplexing (OFDM) and cooperative communication have emerged as promising solutions due to their robustness in severely degraded channel conditions, link reliability, and spectral efficiency. Both OFDM and cooperative systems have challenged RF front-end specifications such as bandwidth and power-efficiency requirements for end users as well as for the base station due to the high peak-to-average power ratio (PAPR). In present and future communication systems, the power amplifier (PA) is a key component at the transmitter. To obtain maximum power efficiency, the PA is operated near its saturation point, which leads to nonlinear distortion (NLD), which is further exaggerated due to the high PAPR of the input signal. This NLD is expected to increase in future fifth-generation (5G) communication networks, due to the use of large bandwidth at millimeter-wave (mmW) frequencies.},
  keywords = {5G communication networks,5G mobile communication,channel conditions,cooperative communication,fifth-generation communication networks,high peak-to-average power ratio,high-data-rate transmission,input signal PAPR,link reliability,millimeter-wave frequencies,NLD,nonlinear distortion,OFDM modulation,OFDM system,orthogonal frequency-division multiplexing,PA,Peak to average power ratio,power amplifier,Power amplifiers,Power generation,radio links,radio transmitter,radio transmitters,radiofrequency power amplifiers,RF front-end specifications,spectral analysis,spectral efficiency,telecommunication network reliability,wireless channels,Wireless communication,wireless communication system,Wireless networks},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JA2BLVWH/Singya et al. - 2017 - Mitigating NLD for Wireless Networks Effect of No.pdf;/Users/antoniohortaribeiro/Zotero/storage/I45RJNX7/7942233.html}
}

@inproceedings{sinha_efficient_2014,
  title = {Efficient High-Resolution Stereo Matching Using Local Plane Sweeps},
  booktitle = {Computer {{Vision}} and {{Pattern Recognition}} ({{CVPR}}), 2014 {{IEEE Conference}} On},
  author = {Sinha, Sudipta N and Scharstein, Daniel and Szeliski, Richard},
  year = {2014},
  pages = {1582--1589},
  publisher = {IEEE},
  annotation = {00000}
}

@book{sipser_introduction_2006,
  title = {Introduction to the Theory of Computation},
  author = {Sipser, Michael},
  year = {2006},
  edition = {2nd ed},
  publisher = {Thomson Course Technology},
  address = {Boston},
  isbn = {978-0-534-95097-2},
  lccn = {QA267 .S56 2006},
  keywords = {Computational complexity,Machine theory},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KEU496KB/sipser_introducti_2006.pdf}
}

@article{sjoberg_nonlinear_1995,
  title = {Nonlinear Black-Box Modeling in System Identification: A Unified Overview},
  author = {Sj{\"o}berg, Jonas and Zhang, Qinghua and Ljung, Lennart and Benveniste, Albert and Delyon, Bernard and Glorennec, Pierre-Yves and akan Hjalmarsson, H{\textbackslash}a and Juditsky, Anatoli},
  year = {1995},
  journal = {Automatica},
  volume = {31},
  number = {12},
  pages = {1691--1724},
  doi = {10.1016/0005-1098(95)00120-8},
  keywords = {ðŸ”No DOI found},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RH72H9NT/ljung_nonlinear_1995.pdf}
}

@book{slotine_applied_1991,
  title = {Applied Nonlinear Control},
  author = {Slotine, J.-J. E. and Li, Weiping},
  year = {1991},
  publisher = {Prentice Hall},
  address = {Englewood Cliffs, N.J},
  isbn = {978-0-13-040890-7},
  langid = {english},
  lccn = {QA402.35 .S56 1990},
  keywords = {Nonlinear control theory},
  annotation = {19267},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5GHNZ6C2/Slotine and Li - 1991 - Applied nonlinear control.pdf}
}

@article{smith_deep_2019,
  title = {A Deep Neural Network Learning Algorithm Outperforms a Conventional Algorithm for Emergency Department Electrocardiogram Interpretation},
  author = {Smith, Stephen W. and Walsh, Brooks and Grauer, Ken and Wang, Kyuhyun and Rapin, Jeremy and Li, Jia and Fennell, William and Taboulet, Pierre},
  year = {2019},
  month = jan,
  journal = {Journal of Electrocardiology},
  volume = {52},
  pages = {88--95},
  issn = {0022-0736},
  doi = {10/gf286g},
  urldate = {2019-05-29},
  abstract = {Background Cardiologs{\textregistered} has developed the first electrocardiogram (ECG) algorithm that uses a deep neural network (DNN) for full 12-lead ECG analysis, including rhythm, QRS and ST-T-U waves. We compared the accuracy of the first version of Cardiologs{\textregistered} DNN algorithm to the Mortara/Veritas{\textregistered} conventional algorithm in emergency department (ED) ECGs. Methods Individual ECG diagnoses were prospectively mapped to one of 16 pre-specified groups of ECG diagnoses, which were further classified as ``major'' ECG abnormality or not. Automated interpretations were compared to blinded experts'. The primary outcome was the performance of the algorithms in finding at least one ``major'' abnormality. The secondary outcome was the proportion of all ECGs for which all groups were identified, with no false negative or false positive groups (``accurate ECG interpretation''). Additionally, we measured sensitivity and positive predictive value (PPV) for any abnormal group. Results Cardiologs{\textregistered} vs. Veritas{\textregistered} accuracy for finding a major abnormality was 92.2\% vs. 87.2\% (p\,{$<$}\,0.0001), with comparable sensitivity (88.7\% vs. 92.0\%, p\,=\,0.086), improved specificity (94.0\% vs. 84.7\%, p\,{$<$}\,0.0001) and improved positive predictive value (PPV 88.2\% vs. 75.4\%, p\,{$<$}\,0.0001). Cardiologs{\textregistered} had accurate ECG interpretation for 72.0\% (95\% CI: 69.6--74.2) of ECGs vs. 59.8\% (57.3--62.3) for Veritas{\textregistered} (P\,{$<$}\,0.0001). Sensitivity for any abnormal group for Cardiologs{\textregistered} and Veritas{\textregistered}, respectively, was 69.6\% (95CI 66.7--72.3) vs. 68.3\% (95CI 65.3--71.1) (NS). Positive Predictive Value was 74.0\% (71.1--76.7) for Cardiologs{\textregistered} vs. 56.5\% (53.7--59.3) for Veritas{\textregistered} (P\,{$<$}\,0.0001). Conclusion Cardiologs' DNN was more accurate and specific in identifying ECGs with at least one major abnormal group. It had a significantly higher rate of accurate ECG interpretation, with similar sensitivity and higher PPV.},
  keywords = {Artificial intelligence,Big data,Computer,Deep neural network,Electrocardiography},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VBQCRZB7/smith_a deep_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/TW6EYU9B/S0022073618302292.html}
}

@article{smith_disciplined_2018,
  title = {A Disciplined Approach to Neural Network Hyper-Parameters: {{Part}} 1 -- Learning Rate, Batch Size, Momentum, and Weight Decay},
  shorttitle = {A Disciplined Approach to Neural Network Hyper-Parameters},
  author = {Smith, Leslie N.},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.09820 [cs, stat]},
  eprint = {1803.09820},
  primaryclass = {cs, stat},
  abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/42SKW69F/smith_a_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/PICXAV3U/1803.html}
}

@article{smola_tutorial_2004,
  title = {A Tutorial on Support Vector Regression},
  author = {Smola, Alex J and Sch{\"o}lkopf, Bernhard},
  year = {2004},
  journal = {Statistics and computing},
  volume = {14},
  number = {3},
  pages = {199--222},
  issn = {0960-3174},
  doi = {10.1023/B:STCO.0000035301.49549.88},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/U9V7EI9R/smola_a_2004.pdf}
}

@article{soderstrom_comparison_1981,
  title = {Comparison of Some Instrumental Variable Methods Consistency and Accuracy Aspects},
  author = {S{\"o}derstr{\"o}m, T and Stoica, P},
  year = {1981},
  journal = {Automatica},
  volume = {17},
  number = {1},
  pages = {101--115},
  doi = {10.1016/0005-1098(81)90087-X},
  annotation = {00000}
}

@article{soderstrom_errorsinvariables_2007,
  title = {Errors-in-Variables Methods in System Identification},
  author = {S{\"o}derstr{\"o}m, Torsten},
  year = {2007},
  journal = {Automatica},
  volume = {43},
  number = {6},
  pages = {939--958},
  issn = {00051098},
  doi = {10.1016/j.automatica.2006.11.025},
  urldate = {2017-08-23},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/23Z77C8A/sÃ¶derstrÃ¶m_errors-in-_2007.pdf}
}

@book{soderstrom_system_1988,
  title = {System Identification},
  author = {S{\"o}derstr{\"o}m, Torsten and Stoica, Petre},
  year = {1988},
  publisher = {Prentice-Hall, Inc.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MHGF83I8/sÃ¶derstrÃ¶m_system_1988.pdf}
}

@book{sontag_mathematical_2013,
  title = {Mathematical Control Theory: Deterministic Finite Dimensional Systems},
  shorttitle = {Mathematical Control Theory},
  author = {Sontag, Eduardo D.},
  year = {2013},
  volume = {6},
  publisher = {Springer Science \& Business Media},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AJ8PWK5H/sontag_mathematic_2013.pdf}
}

@article{sontag_vc_,
  title = {{{VC Dimension}} of {{Neural Networks}}},
  author = {Sontag, Eduardo D},
  pages = {26},
  abstract = {This paper presents a brief introduction to Vapnik-Chervonenkis (VC) dimension, a quantity which characterizes the difficulty of distribution-independent learning. The paper establishes various elementary results, and discusses how to estimate the VC dimension in several examples of interest in neural network theory.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2X96QAI9/Sontag - VC Dimension of Neural Networks.pdf}
}

@article{soudry_implicit_2017,
  title = {The {{Implicit Bias}} of {{Gradient Descent}} on {{Separable Data}}},
  author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  year = {2017},
  month = oct,
  urldate = {2018-12-13},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WZSTVV37/soudry_the_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/DVPCXHFU/1710.html}
}

@book{spivak_calculus_1998,
  title = {Calculus on Manifolds: A Modern Approach to Classical Theorems of Advanced Calculus},
  shorttitle = {Calculus on Manifolds},
  author = {Spivak, Michael},
  year = {1998},
  series = {Mathematics Monograph Series},
  publisher = {Perseus Books},
  address = {Cambridge, Mass},
  isbn = {978-0-8053-9021-6},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/55T986XV/Spivak - 1998 - Calculus on manifolds a modern approach to classi.pdf;/Users/antoniohortaribeiro/Zotero/storage/MU7W34MK/Zeng - Solution of Exercise Problems.pdf;/Users/antoniohortaribeiro/Zotero/storage/VBD5KGC2/spivak.html}
}

@article{srivastava_dropout_2014,
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting.},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {1},
  pages = {1929--1958},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q69JD8RC/srivastava_dropout_2014.pdf}
}

@article{stead_clinical_2018,
  title = {Clinical Implications and Challenges of Artificial Intelligence and Deep Learning},
  author = {Stead, William W.},
  year = {2018},
  month = sep,
  journal = {JAMA},
  volume = {320},
  number = {11},
  pages = {1107--1108},
  issn = {0098-7484},
  doi = {10/gfkhr8},
  abstract = {Artificial intelligence (AI) and deep learning are entering the mainstream of clinical medicine. For example, in December 2016, Gulshan et al1 reported development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. An accompanying editorial by Wong and Bressler2 pointed out limits of the study, the need for further validation of the algorithm in different populations, and unresolved challenges (eg, incorporating the algorithm into clinical work flows and convincing clinicians and patients to ``trust a `black box'''). Sixteen months later, the Food and Drug Administration (FDA)3 permitted marketing of the first medical device to use AI to detect diabetic retinopathy. FDA reduced the risk of releasing the device by limiting the indication for use to screening adults who do not have visual symptoms for greater than mild retinopathy, to refer them to an eye care specialist.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SSY9HXT9/stead_clinical_2018.pdf}
}

@article{steihaug_conjugate_1983,
  title = {The Conjugate Gradient Method and Trust Regions in Large Scale Optimization},
  author = {Steihaug, Trond},
  year = {1983},
  journal = {SIAM Journal on Numerical Analysis},
  volume = {20},
  number = {3},
  pages = {626--637},
  doi = {10.1137/0720042},
  annotation = {00000}
}

@book{stein_real_2005,
  title = {Real Analysis: Measure Theory, Integration, and {{Hilbert}} Spaces},
  shorttitle = {Real Analysis},
  author = {Stein, Elias M. and Shakarchi, Rami},
  year = {2005},
  series = {Princeton Lectures in Analysis},
  number = {v. 3},
  publisher = {Princeton University Press},
  address = {Princeton, N.J},
  isbn = {978-0-691-11386-9},
  lccn = {QA320 .S84 2005},
  keywords = {{Integrals, Generalized},Functional analysis,Hilbertruimten,Maattheorie,Measure theory},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KVRA5GIH/stein_real_2005.pdf}
}

@article{stelzer_stereovisionbased_2012,
  title = {Stereo-Vision-Based Navigation of a Six-Legged Walking Robot in Unknown Rough Terrain},
  author = {Stelzer, Annett and Hirschm{\"u}ller, Heiko and G{\"o}rner, Martin},
  year = {2012},
  journal = {The International Journal of Robotics Research},
  volume = {31},
  number = {4},
  pages = {381--402},
  doi = {10.1177/0278364911435161},
  annotation = {00000}
}

@article{stensrud_why_2020,
  title = {Why {{Test}} for {{Proportional Hazards}}?},
  author = {Stensrud, Mats J. and Hern{\'a}n, Miguel A.},
  year = {2020},
  month = apr,
  journal = {JAMA},
  volume = {323},
  number = {14},
  pages = {1401--1402},
  issn = {1538-3598},
  doi = {10.1001/jama.2020.1267},
  langid = {english},
  pmid = {32167523},
  keywords = {Humans,Methods,Proportional Hazards Models,Research Design,Survival Analysis}
}

@article{stentoumis_accurate_2014,
  title = {On Accurate Dense Stereo-Matching Using a Local Adaptive Multi-Cost Approach},
  author = {Stentoumis, C and Grammatikopoulos, L and Kalisperakis, I and Karras, G},
  year = {2014},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {91},
  pages = {29--49},
  doi = {10.1016/j.isprsjprs.2014.02.006},
  annotation = {00000}
}

@book{stevens_advanced_2013,
  title = {Advanced {{Programming}} in the {{UNIX Environment}}},
  author = {Stevens, W.R. and Rago, S.A.},
  year = {2013},
  series = {Addison-{{Wesley Professional Computing Series}}},
  publisher = {Pearson Education},
  isbn = {978-0-321-63800-7},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/87E5GIA4/stevens_advanced_2013.pdf}
}

@book{stewart_calculus_2015,
  title = {Calculus: Early Transcendentals},
  author = {Stewart, James},
  year = {2015},
  publisher = {Cengage Learning},
  annotation = {00000}
}

@book{stoer_introduction_1980,
  title = {Introduction to {{Numerical Analysis}}},
  author = {Stoer, J and Bulirsch, R},
  year = {1980},
  publisher = {Springer-Verlag, New York},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/E87BI2V8/stoer_introducti_1980.pdf}
}

@book{stoer_introduction_1980a,
  title = {Introduction to {{Numerical Analysis}}},
  author = {Stoer, J and Bulirsch, R},
  year = {1980},
  publisher = {Springer-Verlag, New York}
}

@article{storn_differential_1997,
  title = {Differential Evolution--a Simple and Efficient Heuristic for Global Optimization over Continuous Spaces},
  author = {Storn, Rainer and Price, Kenneth},
  year = {1997},
  journal = {Journal of global optimization},
  volume = {11},
  number = {4},
  pages = {341--359},
  doi = {10.1023/A:1008202821328},
  annotation = {00000}
}

@book{strang_introduction_2009,
  title = {Introduction to {{Linear Algebra}}},
  author = {Strang, G.},
  year = {2009},
  publisher = {Wellesley-Cambridge Press},
  isbn = {978-0-9802327-1-4},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6H2XER86/strang_introducti_2009.pdf}
}

@book{strang_linear_2006,
  title = {Linear {{Algebra}} and {{Its Applications}}},
  author = {Strang, G.},
  year = {2006},
  publisher = {Thomson, Brooks/Cole},
  isbn = {978-0-03-010567-8},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4E5Q3BHZ/strang_linear_2006.pdf;/Users/antoniohortaribeiro/Zotero/storage/UER6IA8D/strang_linear_2006.pdf}
}

@book{strang_wavelets_1996,
  title = {Wavelets and Filter Banks},
  author = {Strang, Gilbert and Nguyen, Truong},
  year = {1996},
  publisher = {SIAM},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9C74SX6H/strang_wavelets_1996.pdf}
}

@inproceedings{street_nuclear_1993,
  title = {Nuclear Feature Extraction for Breast Tumor Diagnosis},
  booktitle = {Biomedical {{Image Processing}} and {{Biomedical Visualization}}},
  author = {Street, W. Nick and Wolberg, W. H. and Mangasarian, O. L.},
  year = {1993},
  month = jul,
  volume = {1905},
  pages = {861--870},
  publisher = {SPIE},
  doi = {10.1117/12.148698},
  urldate = {2024-05-16},
  abstract = {Interactive image processing techniques, along with a linear-programming-based inductive classifier, have been used to create a highly accurate system for diagnosis of breast tumors. A small fraction of a fine needle aspirate slide is selected and digitized. With an interactive interface, the user initializes active contour models, known as snakes, near the boundaries of a set of cell nuclei. The customized snakes are deformed to the exact shape of the nuclei. This allows for precise, automated analysis of nuclear size, shape and texture. Ten such features are computed for each nucleus, and the mean value, largest (or 'worst') value and standard error of each feature are found over the range of isolated cells. After 569 images were analyzed in this fashion, different combinations of features were tested to find those which best separate benign from malignant samples. Ten-fold cross-validation accuracy of 97\% was achieved using a single separating plane on three of the thirty features: mean texture, worst area and worst smoothness. This represents an improvement over the best diagnostic results in the medical literature. The system is currently in use at the University of Wisconsin Hospitals. The same feature set has also been utilized in the much more difficult task of predicting distant recurrence of malignancy in patients, resulting in an accuracy of 86\%.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9MU5K5JP/Street et al. - 1993 - Nuclear feature extraction for breast tumor diagno.pdf}
}

@article{strodthoff_deep_2020,
  title = {Deep {{Learning}} for {{ECG Analysis}}: {{Benchmarks}} and {{Insights}} from {{PTB-XL}}},
  shorttitle = {Deep {{Learning}} for {{ECG Analysis}}},
  author = {Strodthoff, Nils and Wagner, Patrick and Schaeffter, Tobias and Samek, Wojciech},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.13701 [cs, stat]},
  eprint = {2004.13701},
  primaryclass = {cs, stat},
  urldate = {2020-07-14},
  abstract = {Electrocardiography is a very common, non-invasive diagnostic procedure and its interpretation is increasingly supported by automatic interpretation algorithms. The progress in the field of automatic ECG interpretation has up to now been hampered by a lack of appropriate datasets for training as well as a lack of well-defined evaluation procedures to ensure comparability of different algorithms. To alleviate these issues, we put forward first benchmarking results for the recently published, freely accessible PTB-XL dataset, covering a variety of tasks from different ECG statement prediction tasks over age and gender prediction to signal quality assessment. We find that convolutional neural networks, in particular resnet- and inception-based architectures, show the strongest performance across all tasks outperforming feature-based algorithms by a large margin. These results are complemented by deeper insights into the classification algorithm in terms of hidden stratification, model uncertainty and an exploratory interpretability analysis. We also put forward benchmarking results for the ICBEB2018 challenge ECG dataset and discuss prospects of transfer learning using classifiers pretrained on PTB-XL. With this resource, we aim to establish the PTB-XL dataset as a resource for structured benchmarking of ECG analysis algorithms and encourage other researchers in the field to join these efforts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CDHIQ7U7/Strodthoff et al. - 2020 - Deep Learning for ECG Analysis Benchmarks and Ins.pdf;/Users/antoniohortaribeiro/Zotero/storage/Z85QNKGZ/2004.html}
}

@book{strogatz_nonlinear_2018,
  title = {Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry, and Engineering},
  author = {Strogatz, Steven H},
  year = {2018},
  publisher = {CRC Press},
  isbn = {0-429-96111-1},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/C6B8K9E2/strogatz_nonlinear_2018.pdf}
}

@book{stroock_concise_1994,
  title = {A Concise Introduction to the Theory of Integration},
  author = {Stroock, Daniel W.},
  year = {1994},
  edition = {2nd ed},
  publisher = {Birkh{\"a}user},
  address = {Boston},
  isbn = {978-0-8176-3759-0 978-3-7643-3759-9},
  lccn = {QA312 .S78 1994},
  keywords = {{Integrals, Generalized},Measure theory},
  file = {/Users/antoniohortaribeiro/Zotero/storage/H56NHDUB/stroock_a concise_1994.pdf}
}

@article{su_longterm_1992,
  title = {Long-Term Predictions of Chemical Processes Using Recurrent Neural Networks: {{A}} Parallel Training Approach},
  author = {Su, Hong Te and McAvoy, Thomas J and Werbos, Paul},
  year = {1992},
  journal = {Industrial \& Engineering Chemistry Research},
  volume = {31},
  number = {5},
  pages = {1338--1352},
  doi = {10.1021/ie00005a014}
}

@article{su_longterm_1992a,
  title = {Long-{{Term Predictions}} of {{Chemical Processes Using Recurrent Neural Networks}}: {{A Parallel Training Approach}}},
  author = {Su, Hong Te and McAvoy, Thomas J and Werbos, Paul},
  year = {1992},
  journal = {Industrial \& Engineering Chemistry Research},
  volume = {31},
  number = {5},
  pages = {1338--1352},
  doi = {10/cc9djd}
}

@inproceedings{su_neural_1993,
  title = {Neural Model Predictive Control of Nonlinear Chemical Processes},
  booktitle = {Intelligent {{Control}}, 1993., {{Proceedings}} of the 1993 {{IEEE International Symposium}} On},
  author = {Su, H-T and McAvoy, Thomas J},
  year = {1993},
  pages = {358--363},
  publisher = {IEEE},
  annotation = {00000}
}

@inproceedings{su_neural_1993a,
  title = {Neural {{Model Predictive Control}} of {{Nonlinear Chemical Processes}}},
  booktitle = {Intelligent {{Control}}, 1993., {{Proceedings}} of the 1993 {{IEEE International Symposium On}}},
  author = {Su, H-T and McAvoy, Thomas J},
  year = {1993},
  pages = {358--363},
  publisher = {IEEE}
}

@misc{sun_deep_2016,
  title = {Deep {{CORAL}}: {{Correlation Alignment}} for {{Deep Domain Adaptation}}},
  shorttitle = {Deep {{CORAL}}},
  author = {Sun, Baochen and Saenko, Kate},
  year = {2016},
  month = jul,
  number = {arXiv:1607.01719},
  eprint = {1607.01719},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-28},
  abstract = {Deep neural networks are able to learn powerful representations from large quantities of labeled input data, however they cannot always generalize well across changes in input distributions. Domain adaptation algorithms have been proposed to compensate for the degradation in performance due to domain shift. In this paper, we address the case when the target domain is unlabeled, requiring unsupervised adaptation. CORAL[1] is a ``frustratingly easy'' unsupervised domain adaptation method that aligns the second-order statistics of the source and target distributions with a linear transformation. Here, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (Deep CORAL). Experiments on standard benchmark datasets show state-of-the-art performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NTZGIUMT/Sun and Saenko - 2016 - Deep CORAL Correlation Alignment for Deep Domain Adaptation.pdf}
}

@article{surawicz_aha_2009,
  title = {{{AHA}}/{{ACCF}}/{{HRS Recommendations}} for the {{Standardization}} and {{Interpretation}} of the {{Electrocardiogram}}},
  author = {Surawicz, Borys and Childers, Rory and Deal, Barbara J. and Gettes, Leonard S.},
  year = {2009},
  month = mar,
  journal = {Journal of the American College of Cardiology},
  volume = {53},
  number = {11},
  pages = {976},
  doi = {10/bmv8kz}
}

@article{sussillo_opening_2013,
  title = {Opening the {{Black Box}}: {{Low-Dimensional Dynamics}} in {{High-Dimensional Recurrent Neural Networks}}},
  shorttitle = {Opening the {{Black Box}}},
  author = {Sussillo, David and Barak, Omri},
  year = {2013},
  month = mar,
  journal = {Neural Computation},
  volume = {25},
  number = {3},
  pages = {626--649},
  issn = {0899-7667, 1530-888X},
  doi = {10/f4kmg4},
  urldate = {2019-03-08},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GVHW5I4Y/Sussillo and Barak - 2013 - Opening the Black Box Low-Dimensional Dynamics in.pdf}
}

@article{sutarya_identification_2014,
  title = {Identification of Industrial Furnace Temperature for Sintering Process in Nuclear Fuel Fabrication Using {{NARX}} Neural Networks},
  author = {Sutarya, Dede and Kusumoputro, Benyamin},
  year = {2014},
  journal = {Science and Technology of Nuclear Installations},
  volume = {2014},
  doi = {10.1155/2014/854569},
  annotation = {00000}
}

@inproceedings{sutskever_sequence_2014,
  title = {Sequence to Sequence Learning with Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  pages = {3104--3112},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GDIP7JWF/sutskever_sequence_2014.pdf}
}

@article{sutton_reinforcement_2011,
  title = {Reinforcement Learning: {{An}} Introduction},
  author = {Sutton, Richard S and Barto, Andrew G},
  year = {2011},
  file = {/Users/antoniohortaribeiro/Zotero/storage/R7IRRQTJ/abdulhai_reinforcem_2003.pdf}
}

@book{suykens_artificial_2012,
  title = {Artificial Neural Networks for Modelling and Control of Non-Linear Systems},
  author = {Suykens, Johan AK and Vandewalle, Joos PL and {de Moor}, Bart L},
  year = {2012},
  publisher = {Springer Science \& Business Media},
  annotation = {00000}
}

@book{suykens_least_2002,
  title = {Least Squares Support Vector Machines},
  author = {Suykens, Johan AK and Van Gestel, Tony and De Brabanter, Jos},
  year = {2002},
  publisher = {World Scientific},
  annotation = {00000}
}

@article{svensson_flexible_2017,
  title = {A Flexible State--Space Model for Learning Nonlinear Dynamical Systems},
  author = {Svensson, Andreas and Sch{\"o}n, Thomas B.},
  year = {2017},
  month = jun,
  journal = {Automatica},
  volume = {80},
  pages = {189--199},
  issn = {00051098},
  doi = {10.1016/j.automatica.2017.02.030},
  urldate = {2018-04-25},
  langid = {english},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RW7QECUD/svensson_a_2017.pdf}
}

@inproceedings{szegedy_going_2015,
  title = {Going Deeper with Convolutions},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  pages = {1--9},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2PTTATCG/szegedy_going_2015.pdf}
}

@article{szeliski_stereo_1999,
  title = {Stereo Matching with Transparency and Matting},
  author = {Szeliski, Richard and Golland, Polina},
  year = {1999},
  journal = {International Journal of Computer Vision},
  volume = {32},
  number = {1},
  pages = {45--61},
  keywords = {â“Multiple DOI},
  annotation = {00000}
}

@article{szumilas_explaining_2010,
  title = {Explaining {{Odds Ratios}}},
  author = {Szumilas, Magdalena},
  year = {2010},
  month = aug,
  journal = {Journal of the Canadian Academy of Child and Adolescent Psychiatry},
  volume = {19},
  number = {3},
  pages = {227--229},
  issn = {1719-8429},
  urldate = {2023-08-30},
  pmcid = {PMC2938757},
  pmid = {20842279},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TXSNY99S/Szumilas - 2010 - Explaining Odds Ratios.pdf}
}

@article{taheri_asymptotic_2022,
  title = {Asymptotic {{Behavior}} of {{Adversarial Training}} in {{Binary Classification}}},
  author = {Taheri, Hossein and Pedarsani, Ramtin and Thrampoulidis, Christos},
  year = {2022},
  journal = {IEEE International Symposium on Information Theory (ISIT)},
  volume = {127--132},
  eprint = {2010.13275},
  doi = {10.1109/ISIT50566.2022.9834717},
  urldate = {2021-05-16},
  abstract = {It has been consistently reported that many machine learning models are susceptible to adversarial attacks i.e., small additive adversarial perturbations applied to data points can cause misclassification. Adversarial training using empirical risk minimization is considered to be the state-of-the-art method for defense against adversarial attacks. Despite being successful in practice, several problems in understanding generalization performance of adversarial training remain open. In this paper, we derive precise theoretical predictions for the performance of adversarial training in binary classification. We consider the high-dimensional regime where the dimension of data grows with the size of the training data-set at a constant ratio. Our results provide exact asymptotics for standard and adversarial errors of the estimators obtained by adversarial training with \${\textbackslash}ell\_q\$-norm bounded perturbations (\$q {\textbackslash}ge 1\$) for both discriminative binary models and generative Gaussian mixture models. Furthermore, we use these sharp predictions to uncover several intriguing observations on the role of various parameters including the over-parameterization ratio, the data model, and the attack budget on the adversarial and standard errors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JS55WQXM/Taheri et al. - 2021 - Asymptotic Behavior of Adversarial Training in Bin.pdf;/Users/antoniohortaribeiro/Zotero/storage/92BEEZCI/2010.html}
}

@inproceedings{taigman_deepface_2014,
  title = {{{DeepFace}}: {{Closing}} the {{Gap}} to {{Human-Level Performance}} in {{Face Verification}}},
  shorttitle = {{{DeepFace}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
  year = {2014},
  pages = {1701--1708},
  urldate = {2018-01-26},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RCPKGNP9/taigman_deepface_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/T6ZNUPFI/taigman_deepface_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/7PKZACD4/Taigman_DeepFace_Closing_the_2014_CVPR_paper.html}
}

@article{tan_efficientnet_2019,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = {2019},
  journal = {Proceedings of the 36th International Conference on Machine Learning,},
  series = {{{PMLR}}},
  volume = {97},
  eprint = {1905.11946},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7GV7R6T9/tan_efficientn_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/48UX5I7U/1905.html}
}

@article{tan_neuralnetworksbased_2000,
  title = {Neural-Networks-Based Nonlinear Dynamic Modeling for Automotive Engines},
  author = {Tan, Yonghong and Saif, Mehrdad},
  year = {2000},
  journal = {Neurocomputing},
  volume = {30},
  number = {1},
  pages = {129--142},
  keywords = {ðŸ”No DOI found},
  annotation = {00000}
}

@article{tan_simulating_2019,
  title = {Simulating Extrapolated Dynamics with Parameterization Networks},
  author = {Tan, James P. L.},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.03440 [nlin]},
  eprint = {1902.03440},
  primaryclass = {nlin},
  urldate = {2021-03-30},
  abstract = {An artificial neural network architecture, parameterization networks, is proposed for simulating extrapolated dynamics beyond observed data in dynamical systems. Parameterization networks are used to ensure the long term integrity of extrapolated dynamics, while careful tuning of model hyperparameters against validation errors controls overfitting. A parameterization network is demonstrated on the logistic map, where chaos and other nonlinear phenomena consistent with the underlying model can be extrapolated from non-chaotic training time series with good fidelity. The stated results are a lot less fantastical than they appear to be because the neural network is only extrapolating between quadratic return maps. Nonetheless, the results do suggest that successful extrapolation of qualitatively different behaviors requires learning to occur on a level of abstraction where the corresponding behaviors are more similar in nature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Nonlinear Sciences - Chaotic Dynamics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/F2PU4CXC/Tan - 2019 - Simulating extrapolated dynamics with parameteriza.pdf;/Users/antoniohortaribeiro/Zotero/storage/X4FSLC9E/1902.html}
}

@article{tanaka_pruning_2020,
  title = {Pruning Neural Networks without Any Data by Iteratively Conserving Synaptic Flow},
  author = {Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L. K. and Ganguli, Surya},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.05467 [cond-mat, q-bio, stat]},
  eprint = {2006.05467},
  primaryclass = {cond-mat, q-bio, stat},
  urldate = {2020-06-15},
  abstract = {Pruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.9 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that data must be used to quantify which synapses are important.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NBSNT85P/Tanaka et al. - 2020 - Pruning neural networks without any data by iterat.pdf;/Users/antoniohortaribeiro/Zotero/storage/EZLARHJM/2006.html}
}

@article{tanaka_wavecyclegan2_2019,
  title = {{{WaveCycleGAN2}}: {{Time-domain Neural Post-filter}} for {{Speech Waveform Generation}}},
  shorttitle = {{{WaveCycleGAN2}}},
  author = {Tanaka, Kou and Kameoka, Hirokazu and Kaneko, Takuhiro and Hojo, Nobukatsu},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.02892},
  eprint = {1904.02892},
  urldate = {2020-03-23},
  abstract = {WaveCycleGAN has recently been proposed to bridge the gap between natural and synthesized speech waveforms in statistical parametric speech synthesis and provides fast inference with a moving average model rather than an autoregressive model and high-quality speech synthesis with the adversarial training. However, the human ear can still distinguish the processed speech waveforms from natural ones. One possible cause of this distinguishability is the aliasing observed in the processed speech waveform via down/up-sampling modules. To solve the aliasing and provide higher quality speech synthesis, we propose WaveCycleGAN2, which 1) uses generators without down/up-sampling modules and 2) combines discriminators of the waveform domain and acoustic parameter domain. The results show that the proposed method 1) alleviates the aliasing well, 2) is useful for both speech waveforms generated by analysis-and-synthesis and statistical parametric speech synthesis, and 3) achieves a mean opinion score comparable to those of natural speech and speech synthesized by WaveNet (open WaveNet) and WaveGlow while processing speech samples at a rate of more than 150 kHz on an NVIDIA Tesla P100.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YG57WDWW/Tanaka et al. - 2019 - WaveCycleGAN2 Time-domain Neural Post-filter for .pdf;/Users/antoniohortaribeiro/Zotero/storage/DRJ4HYJM/1904.html}
}

@book{tanenbaum_computer_2011,
  title = {Computer Networks},
  author = {Tanenbaum, Andrew S. and Wetherall, D.},
  year = {2011},
  edition = {5th ed},
  publisher = {Pearson Prentice Hall},
  address = {Boston},
  isbn = {978-0-13-212695-3},
  lccn = {TK5105.5 .T36 2011},
  keywords = {Computer networks},
  annotation = {OCLC: ocn660087726},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GNCS3FKU/tanenbaum_computer_2011.pdf}
}

@book{tanenbaum_modern_2009,
  title = {Modern Operating System},
  author = {Tanenbaum, Andrew S.},
  year = {2009},
  publisher = {Pearson Education, Inc},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IVAT32MT/tanenbaum_modern_2009.pdf;/Users/antoniohortaribeiro/Zotero/storage/S8Z8VB4E/Sistemas Operacionais Modernos - 3Âª EdicÌ§aÌƒo - Tanenbaum.pdf}
}

@book{tanenbaum_structured_2006,
  title = {Structured Computer Organization},
  author = {Tanenbaum, Andrew S.},
  year = {2006},
  edition = {5th ed},
  publisher = {Pearson Prentice Hall},
  address = {Upper Saddle River, N.J},
  isbn = {978-0-13-148521-1},
  lccn = {QA76.6 .T38 2006},
  keywords = {Computer organization,Computer programming},
  annotation = {OCLC: ocm57506907},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3MRRGR8P/tanenbaum_structured_2006.pdf}
}

@inproceedings{tao_system_2017,
  title = {System Identification of Fractional-Order for Non-Minimum-Phase and Non-Self-Balancing System},
  booktitle = {2017 {{IEEE International Conference}} on {{Mechatronics}} and {{Automation}} ({{ICMA}})},
  author = {Tao, M. and Ke, Z. and Yu, Y.},
  year = {2017},
  month = aug,
  pages = {758--763},
  doi = {10.1109/ICMA.2017.8015911},
  abstract = {The steam generator is one of the most essential equipment of the nuclear power device which takes part in heat exchanging. The water level system of a steam generator has the peculiarities of apparent nonlinearity, large inertia and time-delaying. Furthermore, the water level of a steam generator has a direct impact on the quality of the outlet steam and the security of the system. Thus, it is significant and necessary to maintain the water level within a safety limit. the mathematical model of the steam generator, which is difficult to be described with integer order, can be established relatively clearly and precisely by using the fractional order calculus. With the progress of the modern technology, the merits of using a fractional order model become increasingly obvious due to its good performance in describing the actual plants and the dynamic process. This paper proposes a fractional order system identification method which can identify the model parameters concisely. This method is simple to be realized and also has a good adaptability to the initial value. The proposed fractional order identification scheme provides a new method of the important equipment modeling in nuclear power plant. The model that established by this method can provide research basis and technical support of the high precision and performance index control system.},
  keywords = {fractional order,non-minimum-phase,Non-Self-Balancing,Steam generator,system identification},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CW8N7WZ5/tao_system_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/2KI3P7WW/8015911.html;/Users/antoniohortaribeiro/Zotero/storage/QHUUTJAL/8015911.html}
}

@book{tao_topics_2012,
  title = {Topics in Random Matrix Theory},
  author = {Tao, Terence},
  year = {2012},
  series = {Graduate {{Studies}} in {{Mathematics}}},
  volume = {132},
  publisher = {American Mathematical Society},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9SMVV3JC/Tao - Topics in random matrix theory.pdf}
}

@inproceedings{tassa_controllimited_2014,
  title = {Control-Limited Differential Dynamic Programming},
  booktitle = {Robotics and {{Automation}} ({{ICRA}}), 2014 {{IEEE International Conference}} On},
  author = {Tassa, Yuval and Mansard, Nicolas and Todorov, Emo},
  year = {2014},
  pages = {1168--1175},
  publisher = {IEEE},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3WDTWERH/tassa_control-li_2014.pdf}
}

@article{tecnologia_vocabulario_,
  title = {Vocabul{\'a}rio {{Internacional}} de {{Metrologia}}--{{Conceitos}} Fundamentais e Gerais e Termos Associados ({{VIM}} 2012)},
  author = {Tecnologia, Inmetro and Filipe, Eduarda and Pellegrino, Olivier and Baratto, Antonio Carlos and {de Oliveira}, S{\'e}rgio Pinheiro and Mendoza, Victor Manuel Loayza},
  keywords = {ðŸ”No DOI found},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/P3PEJ73W/tecnologia_vocabulÃ¡ri_.pdf}
}

@inproceedings{teijeiro_arrhythmia_2017,
  title = {Arrhythmia {{Classification}} from the {{Abductive Interpretation}} of {{Short Single-Lead ECG Records}}},
  booktitle = {Computing in {{Cardiology}}},
  author = {Teijeiro, Tomas and Garcia, Constantino A. and Castro, Daniel and F{\'e}lix, Paulo},
  year = {2017},
  month = sep,
  doi = {10.22489/CinC.2017.166-054},
  urldate = {2018-10-21},
  abstract = {In this work we propose a new method for the rhythm classification of short single-lead ECG records, using a set of high-level and clinically meaningful features provided by the abductive interpretation of the records. These features include morphological and rhythm-related features that are used to build two classifiers: one that evaluates the record globally, using aggregated values for each feature; and another one that evaluates the record as a sequence, using a Recurrent Neural Network fed with the individual features for each detected heartbeat. The two classifiers are finally combined using the stacking technique, providing an answer by means of four target classes: Normal sinus rhythm (N), Atrial fibrillation (A), Other anomaly (O) and Noisy ({\textasciitilde}). The approach has been validated against the 2017 Physionet/CinC Challenge dataset, obtaining a final score of 0.83 and ranking first in the competition.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EGIZTDYH/teijeiro_arrhythmia_2017.pdf}
}

@article{teixeira_datadriven_2014,
  title = {Data-{{Driven Soft Sensor}} of {{Downhole Pressure}} for a {{Gas-Lift Oil Well}}},
  author = {Teixeira, Bruno O. S. and Castro, Walace S and Teixeira, Alex F and Aguirre, Luis A},
  year = {2014},
  journal = {Control Engineering Practice},
  volume = {22},
  pages = {34--43},
  doi = {10/f5nhgb}
}

@article{teixeira_datadriven_2014a,
  title = {Data-{{Driven Soft Sensor}} of {{Downhole Pressure}} for a {{Gas-Lift Oil Well}}},
  author = {Teixeira, Bruno O. S. and Castro, Walace S and Teixeira, Alex F and Aguirre, Luis A},
  year = {2014},
  journal = {Control Engineering Practice},
  volume = {22},
  pages = {34--43},
  doi = {10/f5nhgb}
}

@article{ternes_identification_2017,
  title = {Identification of Biomarker-by-Treatment Interactions in Randomized Clinical Trials with Survival Outcomes and High-Dimensional Spaces},
  author = {Tern{\`e}s, Nils and Rotolo, Federico and Heinze, Georg and Michiels, Stefan},
  year = {2017},
  month = jul,
  journal = {Biometrical Journal},
  volume = {59},
  number = {4},
  pages = {685--701},
  issn = {1521-4036},
  doi = {10.1002/bimj.201500234},
  abstract = {Stratified medicine seeks to identify biomarkers or parsimonious gene signatures distinguishing patients that will benefit most from a targeted treatment. We evaluated 12 approaches in high-dimensional Cox models in randomized clinical trials: penalization of the biomarker main effects and biomarker-by-treatment interactions (full-lasso, three kinds of adaptive lasso, ridge+lasso and group-lasso); dimensionality reduction of the main effect matrix via linear combinations (PCA+lasso (where PCA is principal components analysis) or PLS+lasso (where PLS is partial least squares)); penalization of modified covariates or of the arm-specific biomarker effects (two-I model); gradient boosting; and univariate approach with control of multiple testing. We compared these methods via simulations, evaluating their selection abilities in null and alternative scenarios. We varied the number of biomarkers, of nonnull main effects and true biomarker-by-treatment interactions. We also proposed a novel measure evaluating the interaction strength of the developed gene signatures. In the null scenarios, the group-lasso, two-I model, and gradient boosting performed poorly in the presence of nonnull main effects, and performed well in alternative scenarios with also high interaction strength. The adaptive lasso with grouped weights was too conservative. The modified covariates, PCA+lasso, PLS+lasso, and ridge+lasso performed moderately. The full-lasso and adaptive lassos performed well, with the exception of the full-lasso in the presence of only nonnull main effects. The univariate approach performed poorly in alternative scenarios. We also illustrate the methods using gene expression data from 614 breast cancer patients treated with adjuvant chemotherapy.},
  langid = {english},
  keywords = {Biomarker-by-treatment interactions,High-dimensional,Precision medicine,Stratified medicine,Survival,variable selection},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Z7VDBBTX/ternÃ¨s_identifica_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/5X9I3CWP/abstract.html}
}

@inproceedings{terzi_learning_2018,
  title = {Learning Multi-Step Prediction Models for Receding Horizon Control},
  booktitle = {2018 {{European Control Conference}} ({{ECC}})},
  author = {Terzi, E. and Fagiano, L. and Farina, M. and Scattolini, R.},
  year = {2018},
  month = jun,
  pages = {1335--1340},
  doi = {10/gfxx69},
  abstract = {In this paper, the derivation of multi-step-ahead prediction models from sampled input-output data of a linear system is considered. Specifically, a dedicated prediction model is built for each future time step of interest. Each model is linearly parametrized in a suitable regressor vector, composed of past output values and past and future input values. In addition to a nominal model, the set of all models consistent with data and prior information is derived as well, making the approach suitable for robust control design within a Model Predictive Control framework. The resulting parameter identification problem is solved through a sequence of convex programs. Convergence of the identified error bounds to their theoretical minimum is demonstrated, under suitable assumptions on the measured data, and features like worst-case accuracy computation are illustrated in a numerical example.},
  keywords = {Computational modeling,control system synthesis,convex programming,Data models,dedicated prediction model,future input values,horizon control,learning (artificial intelligence),learning multistep prediction models,linear system,linear systems,Linear systems,model predictive control framework,multistep-ahead prediction models,Noise measurement,nominal model,Numerical models,output values,parameter estimation,predictive control,Predictive models,robust control,robust control design,sampled input-output data,suitable assumptions,suitable regressor vector,Uncertainty},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KG7DB4H3/terzi_learning_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/AU66RLDM/8550494.html}
}

@article{teschl_ordinary_,
  title = {Ordinary {{Differential Equations}} and {{Dynamical Systems}}},
  author = {Teschl, Gerald},
  pages = {364},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7W6GIHTM/Teschl - Ordinary Differential Equations and Dynamical Syst.pdf}
}

@incollection{thekumparampil_robustness_2018,
  title = {Robustness of Conditional {{GANs}} to Noisy Labels},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Thekumparampil, Kiran K and Khetan, Ashish and Lin, Zinan and Oh, Sewoong},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {10291--10302},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-04},
  file = {/Users/antoniohortaribeiro/Zotero/storage/V2HQPKQB/Thekumparampil et al. - Robustness of Conditional GANs to Noisy Labels - S.pdf;/Users/antoniohortaribeiro/Zotero/storage/Y2ABZRA4/thekumparampil_robustness_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/S9V22NPC/8229-robustness-of-conditional-gans-to-noisy-labels.html}
}

@article{thomas_maximum_2014,
  title = {Maximum {{Likelihood Estimation}} of {{GEVD}}: {{Applications}} in {{Bioinformatics}}},
  shorttitle = {Maximum {{Likelihood Estimation}} of {{GEVD}}},
  author = {Thomas, M. and Daemen, A. and Moor, B. De},
  year = {2014},
  month = jul,
  journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  volume = {11},
  number = {4},
  pages = {673--680},
  issn = {1545-5963},
  doi = {10.1109/TCBB.2014.2304292},
  abstract = {We propose a method, maximum likelihood estimation of generalized eigenvalue decomposition (MLGEVD) that employs a well known technique relying on the generalization of singular value decomposition (SVD). The main aim of the work is to show the tight equivalence between MLGEVD and generalized ridge regression. This relationship reveals an important mathematical property of GEVD in which the second argument act as prior information in the model. Thus we show that MLGEVD allows the incorporation of external knowledge about the quantities of interest into the estimation problem. We illustrate the importance of prior knowledge in clinical decision making/identifying differentially expressed genes with case studies for which microarray data sets with corresponding clinical/literature information are available. On all of these three case studies, MLGEVD outperformed GEVD on prediction in terms of test area under the ROC curve (test AUC). MLGEVD results in significantly improved diagnosis, prognosis and prediction of therapy response.},
  keywords = {bioinformatics,Breast cancer,clinical decision making-identification,clinical-literature information,Eigenvalue decomposition,Eigenvalues and eigenfunctions,expressed genes,external knowledge incorporation,generalized eigenvalue decomposition,generalized ridge regression,generalized singular value decomposition,genetics,mathematical property,Matrix decomposition,maximum likelihood estimation,maximum likelihood generalized eigenvalue decomposition,microarray data sets,MLGEVD,Principal component analysis,quantities-of-interest,regression analysis,ROC curve,sensitivity analysis,singular value decomposition,SVD,therapy response diagnosis,therapy response prediction,therapy response prognosis},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JW3RHDPW/thomas_maximum_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/UZGGAZSW/6730900.html;/Users/antoniohortaribeiro/Zotero/storage/ZWCM25X6/6730900.html}
}

@inproceedings{thrampoulidis_regularized_2015,
  title = {Regularized {{Linear Regression}}: {{A Precise Analysis}} of the {{Estimation Error}}},
  shorttitle = {Regularized {{Linear Regression}}},
  booktitle = {Proceedings of {{The}} 28th {{Conference}} on {{Learning Theory}}},
  author = {Thrampoulidis, Christos and Oymak, Samet and Hassibi, Babak},
  year = {2015},
  month = jun,
  pages = {1683--1709},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2022-08-04},
  abstract = {Non-smooth regularized convex optimization procedures have emerged as a powerful tool to recover structured signals (sparse, low-rank, etc.) from (possibly compressed) noisy linear measurements. We focus on the problem of linear regression and consider a general class of optimization methods that minimize a loss function measuring the misfit of the model to the observations with an added structured-inducing regularization term. Celebrated instances include the LASSO, Group-LASSO, Least-Absolute Deviations method, etc.. We develop a quite general framework for how to determine precise prediction performance guaranties (e.g. mean-square-error) of such methods for the case of Gaussian measurement ensemble. The  machinery builds upon  Gordon's Gaussian min-max theorem under additional convexity assumptions that arise in many practical applications. This theorem associates with a primary optimization (PO) problem a simplified auxiliary optimization  (AO) problem from which we can tightly infer properties of the original (PO), such as the optimal cost, the norm of the optimal solution, etc. Our theory applies to general loss functions and regularization and provides guidelines on how to optimally tune the regularizer coefficient when certain structural properties (such as sparsity level, rank, etc.) are known.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UYRPJD4I/Thrampoulidis et al_2015_Regularized Linear Regression.pdf}
}

@article{tibshirani_lasso_2013,
  title = {The {{Lasso Problem}} and {{Uniqueness}}},
  author = {Tibshirani, Ryan J},
  year = {2013},
  journal = {Electronic Journal of Statistics},
  volume = {7},
  pages = {1456--1490},
  abstract = {The lasso is a popular tool for sparse linear regression, especially for problems in which the number of variables p exceeds the number of observations n. But when p {$>$} n, the lasso criterion is not strictly convex, and hence it may not have a unique minimizer. An important question is: when is the lasso solution well-defined (unique)? We review results from the literature, which show that if the predictor variables are drawn from a continuous probability distribution, then there is a unique lasso solution with probability one, regardless of the sizes of n and p. We also show that this result extends easily to 1 penalized minimization problems over a wide range of loss functions.},
  langid = {english},
  keywords = {\_tablet},
  file = {/Users/antoniohortaribeiro/Zotero/storage/22G8J5TA/Tibshirani_2013_The Lasso Problem and Uniqueness.pdf}
}

@article{tibshirani_regression_1996,
  title = {Regression Shrinkage and Selection via the {{LASSO}}},
  author = {Tibshirani, Robert},
  year = {1996},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  pages = {267--288},
  keywords = {ðŸ”No DOI found},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/V2RVK5JF/tibshirani_regression_1996.pdf}
}

@article{tibshirani_regression_2011,
  title = {Regression Shrinkage and Selection via the Lasso: A Retrospective},
  shorttitle = {Regression Shrinkage and Selection via the Lasso},
  author = {Tibshirani, Robert},
  year = {2011},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {3},
  pages = {273--282},
  doi = {10.1111/j.1467-9868.2011.00771.x},
  urldate = {2017-09-14},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/T465FMH2/tibshirani_regression_2011.pdf}
}

@article{tierney_markov_1994,
  title = {Markov {{Chains}} for {{Exploring Posterior Distributions}}},
  author = {Tierney, Luke},
  year = {1994},
  journal = {The Annals of Statistics},
  volume = {22},
  number = {4},
  eprint = {2242477},
  eprinttype = {jstor},
  pages = {1701--1728},
  issn = {0090-5364},
  urldate = {2018-12-13},
  abstract = {[Several Markov chain methods are available for sampling from a posterior distribution. Two important examples are the Gibbs sampler and the Metropolis algorithm. In addition, several strategies are available for constructing hybrid algorithms. This paper outlines some of the basic methods and strategies and discusses some related theoretical and practical issues. On the theoretical side, results from the theory of general state space Markov chains can be used to obtain convergence rates, laws of large numbers and central limit theorems for estimates obtained from Markov chain methods. These theoretical results can be used to guide the construction of more efficient algorithms. For the practical use of Markov chain methods, standard simulation methodology provides several variance reduction techniques and also give guidance on the choice of sample size and allocation.]},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KRSP9A4J/Tierney - 1994 - Markov Chains for Exploring Posterior Distribution.pdf}
}

@misc{tihonenko_st_2007,
  title = {St.-{{Petersburg Institute}} of {{Cardiological Technics}} 12-Lead {{Arrhythmia Database}}},
  author = {Tihonenko, Viktor and Khaustov, Alexander and Ivanov, Sergey and Rivin, Alexei},
  year = {2007},
  publisher = {physionet.org},
  doi = {10.13026/C2V88N},
  urldate = {2020-11-03},
  abstract = {This database consists of 75 annotated recordings extracted from 32 Holter records. Each record is 30 minutes long and contains 12 standard leads, each sampled at 257 Hz, with gains varying from 250 to 1100 analog-to-digital converter units per millivolt. Gains for each record are specified in its .hea file. The reference annotation files contain over 175,000 beat annotations in all.}
}

@article{tijani_nonlinear_2014,
  title = {Nonlinear Identification of a Small Scale Unmanned Helicopter Using Optimized {{NARX}} Network with Multiobjective Differential Evolution},
  author = {Tijani, Ismaila B and Akmeliawati, Rini and Legowo, Ari and Budiyono, Agus},
  year = {2014},
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {33},
  pages = {99--115},
  doi = {10.1016/j.engappai.2014.04.003},
  annotation = {00000}
}

@article{tijani_nonlinear_2014a,
  title = {Nonlinear {{Identification}} of a {{Small Scale Unmanned Helicopter Using Optimized NARX Network}} with {{Multiobjective Differential Evolution}}},
  author = {Tijani, Ismaila B and Akmeliawati, Rini and Legowo, Ari and Budiyono, Agus},
  year = {2014},
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {33},
  pages = {99--115},
  doi = {10/f58zk9}
}

@article{tikk_survey_2003,
  title = {A Survey on Universal Approximation and Its Limits in Soft Computing Techniques},
  author = {Tikk, Domonkos and K{\'o}czy, L{\'a}szl{\'o} T and Gedeon, Tam{\'a}s D},
  year = {2003},
  journal = {International Journal of Approximate Reasoning},
  volume = {33},
  number = {2},
  pages = {185--202},
  doi = {10.1016/S0888-613X(03)00021-5},
  annotation = {00000}
}

@article{timmer_parametric_2000,
  title = {Parametric, Nonparametric and Parametric Modelling of a Chaotic Circuit Time Series},
  author = {Timmer, J and Rust, H and Horbelt, W and Voss, {\relax HU}},
  year = {2000},
  journal = {Physics Letters A},
  volume = {274},
  number = {3},
  pages = {123--134},
  keywords = {ðŸ”No DOI found,Nonlinear Sciences - Chaotic Dynamics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CF4FNX4S/timmer_parametric_2000.pdf;/Users/antoniohortaribeiro/Zotero/storage/G75CL9C4/0009040.html;/Users/antoniohortaribeiro/Zotero/storage/LJAR9LGL/0009040.html}
}

@article{tiso_new_2017,
  title = {A New, Challenging Benchmark for Nonlinear System Identification},
  author = {Tiso, Paolo and No{\"e}l, Jean-Philippe},
  year = {2017},
  month = feb,
  journal = {Mechanical Systems and Signal Processing},
  series = {Recent Advances in Nonlinear System Identification},
  volume = {84},
  pages = {185--193},
  issn = {0888-3270},
  doi = {10.1016/j.ymssp.2016.08.008},
  abstract = {The progress accomplished during the past decade in nonlinear system identification in structural dynamics is considerable. The objective of the present paper is to consolidate this progress by challenging the community through a new benchmark structure exhibiting complex nonlinear dynamics. The proposed structure consists of two offset cantilevered beams connected by a highly flexible element. For increasing forcing amplitudes, the system sequentially features linear behaviour, localised nonlinearity associated with the buckling of the connecting element, and distributed nonlinearity resulting from large elastic deformations across the structure. A finite element-based code with time integration capabilities is made available at https://sem.org/nonlinear-systems-imac-focus-group/. This code permits the numerical simulation of the benchmark dynamics in response to arbitrary excitation signals.},
  keywords = {Benchmark,Buckling,Distributed nonlinearity,Large displacements,Localised nonlinearity,Nonlinear system identification,Structural dynamics},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/UFGNBB7C/tiso_a new,_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/NBGWKGPG/S0888327016302837.html;/Users/antoniohortaribeiro/Zotero/storage/PG9DDTSN/S0888327016302837.html}
}

@book{topol_deep_2019,
  title = {Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again},
  author = {Topol, Eric},
  year = {2019},
  publisher = {Hachette UK},
  isbn = {1-5416-4464-6}
}

@article{topol_medical_2024,
  title = {Medical Forecasting},
  author = {Topol, Eric J.},
  year = {2024},
  month = may,
  journal = {Science},
  volume = {384},
  number = {6698},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.adp7977},
  urldate = {2024-06-08}
}

@article{torkamani_personal_2018,
  title = {The Personal and Clinical Utility of Polygenic Risk Scores},
  author = {Torkamani, Ali and Wineinger, Nathan E and Topol, Eric J},
  year = {2018},
  journal = {Nature Reviews Genetics},
  volume = {19},
  number = {9},
  pages = {581--590},
  publisher = {Nature Publishing Group UK London},
  issn = {1471-0056}
}

@book{trefethen_numerical_1997,
  title = {Numerical Linear Algebra},
  author = {Trefethen, Lloyd N. and Bau, David},
  year = {1997},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia},
  isbn = {978-0-89871-361-9},
  lccn = {QA184 .T74 1997},
  keywords = {{Algebras, Linear},Numerical calculations},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2XHEWI6I/trefethen_numerical_1997.pdf}
}

@article{tripathy_novel_2019,
  title = {A {{Novel Approach}} for {{Detection}} of {{Myocardial Infarction From ECG Signals}} of {{Multiple Electrodes}}},
  author = {Tripathy, R. K. and Bhattacharyya, A. and Pachori, R. B.},
  year = {2019},
  month = jun,
  journal = {IEEE Sensors Journal},
  volume = {19},
  number = {12},
  pages = {4509--4517},
  issn = {1530-437X},
  doi = {10/gf286v},
  abstract = {Myocardial infarction (MI) is also called the heart attack, and it results in the death of heart muscle cells due to the lacking in the supply of oxygen and other nutrients. The early and accurate detection of MI using the 12-lead electrocardiogram (ECG) is helpful in the clinical standard for saving the lives of the patients suffering from this pathology. This paper proposes a novel approach for the detection of MI pathology using the multiresolution analysis of 12-lead ECG signals. The approach is based on the use of Fourier--Bessel series expansion-based empirical wavelet transform (FBSE-EWT) for the time-scale decomposition of 12-lead ECG signals. For each lead ECG signal, nine subband signals are evaluated using FBSE-EWT. The statistical features such as the kurtosis, the skewness, and the entropy are evaluated from the subband signals of each ECG lead. The deep neural network such as the deep layer least-square support-vector machine (DL-LSSVM) which is formulated using the hidden layers of sparse auto-encoders and the LSSVM is used for the detection of MI from the feature vector of 12-lead ECG. The experimental results demonstrate that the combination of FBSE-EWT-based entropy features and DL-LSSVM has the mean accuracy, the mean sensitivity, and the mean specificity values of 99.74\%, 99.87\%, and 99.60\%, respectively, for the detection of MI. The accuracy value of the proposed method is improved by more than 3\% as compared to the wavelet-based features for the detection of MI.},
  keywords = {12-lead ECG,clinical information,DL-LSSVM,Electrocardiography,Entropy,FBSE-EWT,Feature extraction,Heart,Myocardial infarction,Pathology,Signal resolution,Support vector machines},
  file = {/Users/antoniohortaribeiro/Zotero/storage/KJYBMEDK/tripathy_a novel_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/QFH2N5DX/8630000.html}
}

@book{trucco_introductory_1998,
  title = {Introductory Techniques for 3-{{D}} Computer Vision},
  author = {Trucco, Emanuele and Verri, Alessandro},
  year = {1998},
  volume = {201},
  publisher = {Prentice Hall Englewood Cliffs},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NBBQXNFW/trucco_introducto_1998.pdf}
}

@article{tsai_versatile_1987,
  title = {A Versatile Camera Calibration Technique for High-Accuracy {{3D}} Machine Vision Metrology Using off-the-Shelf {{TV}} Cameras and Lenses},
  author = {Tsai, Roger Y},
  year = {1987},
  journal = {Robotics and Automation, IEEE Journal of},
  volume = {3},
  number = {4},
  pages = {323--344},
  doi = {10.1109/JRA.1987.1087109},
  annotation = {00000}
}

@article{tseng_convergence_2001,
  title = {Convergence of a Block Coordinate Descent Method for Nondifferentiable Minimization},
  author = {Tseng, Paul},
  year = {2001},
  journal = {Journal of Optimization Theory and Applications},
  volume = {109},
  number = {3},
  pages = {475--494},
  doi = {10.1023/A:1017501703105},
  urldate = {2017-09-13},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/S8AUJPFC/tseng_convergenc_2001.pdf;/Users/antoniohortaribeiro/Zotero/storage/UWKPPRN4/tseng_convergenc_2001.pdf}
}

@article{tsipras_robustness_2019,
  title = {Robustness {{May Be At Odds}} with {{Accuracy}}},
  author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Ma, Aleksander},
  year = {2019},
  journal = {International Conference for Learning Representations},
  abstract = {We show that there exists an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed in practice. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the features learned by robust models tend to align better with salient data characteristics and human perception.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YWRXHWPR/Tsipras et al. - 2019 - ROBUSTNESS MAY BE AT ODDS WITH ACCURACY.pdf}
}

@article{tulleken_greybox_1993,
  title = {Grey-Box Modelling and Identification Using Physical Knowledge and {{Bayesian}} Techniques},
  author = {Tulleken, Herbert JAF},
  year = {1993},
  journal = {Automatica},
  volume = {29},
  number = {2},
  pages = {285--308},
  issn = {0005-1098},
  doi = {10.1016/0005-1098(93)90124-C},
  annotation = {00000}
}

@article{turner_residential_2017,
  title = {Residential {{HVAC Fault Detection Using}} a {{System Identification Approach}}},
  author = {Turner, W. J. N. and Staino, A and Basu, B},
  year = {2017},
  journal = {Energy and Buildings},
  doi = {10/gbxqqf},
  annotation = {00013}
}

@article{ugray_scatter_2007,
  title = {Scatter Search and Local {{NLP}} Solvers: {{A}} Multistart Framework for Global Optimization},
  author = {Ugray, Zsolt and Lasdon, Leon and Plummer, John and Glover, Fred and Kelly, James and Mart{\'i}, Rafael},
  year = {2007},
  journal = {INFORMS Journal on Computing},
  volume = {19},
  number = {3},
  pages = {328--340},
  doi = {10.1287/ijoc.1060.0175},
  annotation = {00000}
}

@article{ulicny_harmonic_2018,
  title = {Harmonic {{Networks}}: {{Integrating Spectral Information}} into {{CNNs}}},
  shorttitle = {Harmonic {{Networks}}},
  author = {Ulicny, Matej and Krylov, Vladimir A. and Dahyot, Rozenn},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.03205 [cs]},
  eprint = {1812.03205},
  primaryclass = {cs},
  urldate = {2020-07-07},
  abstract = {Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. In contrast, in this paper we propose harmonic blocks that produce features by learning optimal combinations of spectral filters defined by the Discrete Cosine Transform. The harmonic blocks are used to replace conventional convolutional layers to construct partial or fully harmonic CNNs. We extensively validate our approach and show that the introduction of harmonic blocks into state-of-the-art CNN baseline architectures results in comparable or better performance in classification tasks on small NORB, CIFAR10 and CIFAR100 datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/P6HKXX58/Ulicny et al. - 2018 - Harmonic Networks Integrating Spectral Informatio.pdf;/Users/antoniohortaribeiro/Zotero/storage/MYUCJBIQ/1812.html}
}

@phdthesis{umenberger_convex_2017,
  title = {Convex {{Identifcation}} of {{Stable Dynamical Systems}}},
  author = {Umenberger, Jack},
  year = {2017},
  address = {Sydney},
  abstract = {This thesis concerns the scalable application of convex optimization to data-driven mod- eling of dynamical systems, termed system identification in the control community. Two problems commonly arising in system identification are model instability (e.g. unreliability of long-term, open-loop predictions), and nonconvexity of quality-of-fit criteria, such as sim- ulation error (a.k.a. output error). To address these problems, this thesis presents convex parametrizations of stable dynamical systems, convex quality-of-fit criteria, and e cient algorithms to optimize the latter over the former. In particular, this thesis makes extensive use of Lagrangian relaxation, a technique for gen- erating convex approximations to nonconvex optimization problems. Recently, Lagrangian relaxation has been used to approximate simulation error and guarantee nonlinear model stability via semidefinite programming (SDP), however, the resulting SDPs have large di- mension, limiting their practical utility. The first contribution of this thesis is a custom interior point algorithm that exploits structure in the problem to significantly reduce com- putational complexity. The new algorithm enables empirical comparisons to established methods including Nonlinear ARX, in which superior generalization to new data is demon- strated. Equipped with this algorithmic machinery, the second contribution of this thesis is the in- corporation of model stability constraints into the maximum likelihood framework. Specifi- cally, Lagrangian relaxation is combined with the expectation maximization (EM) algorithm to derive tight bounds on the likelihood function, that can be optimized over a convex parametrization of all stable linear dynamical systems. Two diâ†µerent formulations are pre- sented, one of which gives higher fidelity bounds when disturbances (a.k.a. process noise) dominate measurement noise, and vice versa. Finally, identification of positive systems is considered. Such systems enjoy substantially simpler stability and performance analysis compared to the general linear time-invariant(LTI) case, and appear frequently in applications where physical constraints imply nonneg- ativity of the quantities of interest. Lagrangian relaxation is used to derive new convex parametrizations of stable positive systems and quality-of-fit criteria, and substantial im- provements in accuracy of the identified models, compared to existing approaches based on weighted equation error, are demonstrated. Furthermore, the convex parametrizations of stable systems based on linear Lyapunov functions are shown to be amenable to distributed optimization, which is useful for identification of large-scale networked dynamical systems.},
  langid = {english},
  school = {The University of Sydney},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/K7WHJDT8/umenberger_convex_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/SBNNJ6NF/umenberger_convex_2017.pdf}
}

@article{umenberger_identification_2015,
  title = {On {{Identification}} via {{EM}} with {{Latent Disturbances}} and {{Lagrangian Relaxation}}},
  shorttitle = {On {{Identification}} via {{EM}} with {{Latent Disturbances}} and {{Lagrangian Relaxation}}},
  author = {Umenberger, Jack and W{\aa}gberg, Johan and Manchester, Ian R. and Sch{\"o}n, Thomas B.},
  year = {2015},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {17th {{IFAC Symposium}} on {{System Identification SYSID}} 2015},
  volume = {48},
  number = {28},
  pages = {69--74},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2015.12.102},
  abstract = {In the application of the Expectation Maximization (EM) algorithm to identification of dynamical systems, latent variables are typically taken as system states, for simplicity. In this work, we propose a different choice of latent variables, namely, system disturbances. Such a formulation is shown, under certain circumstances, to improve the fidelity of bounds on the likelihood, and circumvent difficulties related to intractable model transition densities. To access these benefits, we propose a Lagrangian relaxation of the challenging optimization problem that arises when formulating over latent disturbances, and fully develop the method for linear models.},
  keywords = {convex relaxation,expectation maximization,system identification},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/H98CZXNF/umenberger_on_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/8XSR3JK3/S2405896315027263.html}
}

@article{umenberger_learning_2018,
  title = {Learning Convex Bounds for Linear Quadratic Control Policy Synthesis},
  author = {Umenberger, Jack and Sch{\"o}n, Thomas B.},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.00319 [cs, math, stat]},
  eprint = {1806.00319},
  primaryclass = {cs, math, stat},
  abstract = {Learning to make decisions from observed data in dynamic environments remains a problem of fundamental importance in a number of fields, from artificial intelligence and robotics, to medicine and finance. This paper concerns the problem of learning control policies for unknown linear dynamical systems so as to maximize a quadratic reward function. We present a method to optimize the expected value of the reward over the posterior distribution of the unknown system parameters, given data. The algorithm involves sequential convex programing, and enjoys reliable local convergence and robust stability guarantees. Numerical simulations and stabilization of a real-world inverted pendulum are used to demonstrate the approach, with strong performance and robustness properties observed in both.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VF7P9AJH/umenberger_learning_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/2Q2J5PPB/1806.html}
}

@article{umenberger_linear_2016,
  title = {Linear {{System Identification}} via {{EM}} with {{Latent Disturbances}} and {{Lagrangian Relaxation}}},
  author = {Umenberger, Jack and W{\aa}gberg, Johan and Manchester, Ian R. and Sch{\"o}n, Thomas B.},
  year = {2016},
  month = mar,
  journal = {arXiv:1603.09157 [cs, stat]},
  eprint = {1603.09157},
  primaryclass = {cs, stat},
  abstract = {In the application of the Expectation Maximization algorithm to identification of dynamical systems, internal states are typically chosen as latent variables, for simplicity. In this work, we propose a different choice of latent variables, namely, system disturbances. Such a formulation elegantly handles the problematic case of singular state space models, and is shown, under certain circumstances, to improve the fidelity of bounds on the likelihood, leading to convergence in fewer iterations. To access these benefits we develop a Lagrangian relaxation of the nonconvex optimization problems that arise in the latent disturbances formulation, and proceed via semidefinite programming.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Systems and Control,Statistics - Computation},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4XACR2ZK/umenberger_linear_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/59KBSPQN/umenberger_linear_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/2ZTRB8KN/1603.html;/Users/antoniohortaribeiro/Zotero/storage/WPVWXAK2/1603.html}
}

@book{umenberger_maximum_2017,
  title = {Maximum Likelihood Identification of Stable Linear Dynamical Systems},
  author = {Umenberger, Jack and W{\aa}gberg, Johan and Manchester, Ian and Sch{\"o}n, Thomas},
  year = {2017},
  month = jun,
  annotation = {00000}
}

@inproceedings{umenberger_scalable_2016,
  title = {Scalable Identification of Stable Positive Systems},
  booktitle = {2016 {{IEEE}} 55th {{Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Umenberger, Jack and Manchester, Ian R},
  year = {2016},
  month = dec,
  pages = {4630--4635},
  doi = {10.1109/CDC.2016.7798974},
  keywords = {analogous methods,control system analysis,convex programming,Cost function,decomposability,general LTI,large-scale networked systems,large-scale systems,linear program,linear programming,Linear systems,Lyapunov methods,Mathematical model,Minimization,optimization,polytopic parameterization,positive LTI,scalable identification,semidefinite programs,simulation error,stability,Stability analysis,stable positive systems},
  annotation = {00000}
}

@inproceedings{umenberger_specialized_2016,
  title = {Specialized Algorithm for Identification of Stable Linear Systems Using {{Lagrangian}} Relaxation},
  booktitle = {2016 {{American Control Conference}} ({{ACC}})},
  author = {Umenberger, Jack and Manchester, Ian R},
  year = {2016},
  month = jul,
  pages = {930--935},
  doi = {10.1109/ACC.2016.7525034},
  keywords = {Approximation algorithms,convex approximations,convex bound optimization,convex programming,general-purpose semidefinite programming solvers,Lagrangian relaxation,linear state-space models,Linear systems,minimisation,Minimization,model stability,optimization,simulation error minimization problem,stability,Stability analysis,stable linear system identification,state-space methods,Symmetric matrices,system identification},
  annotation = {00000}
}

@article{unwin_iris_2021,
  title = {The {{Iris Data Set}}: {{In Search}} of the {{Source}} of {{Virginica}}},
  shorttitle = {The {{Iris Data Set}}},
  author = {Unwin, Antony and Kleinman, Kim},
  year = {2021},
  month = dec,
  journal = {Significance},
  volume = {18},
  number = {6},
  pages = {26--29},
  issn = {1740-9705},
  doi = {10.1111/1740-9713.01589},
  urldate = {2024-05-21},
  abstract = {The iris data set is one of the best-known and most widely used data sets in statistics and data science. But the origins of at least part of the data have been something of a mystery for decades. Antony Unwin and Kim Kleinman believe they have traced the source},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4TBAEVU9/Unwin and Kleinman - 2021 - The Iris Data Set In Search of the Source of Virg.pdf;/Users/antoniohortaribeiro/Zotero/storage/M8XYW62A/7038520.html}
}

@article{vaicenavicius_evaluating_2019,
  title = {Evaluating Model Calibration in Classification},
  author = {Vaicenavicius, Juozas and Widmann, David and Andersson, Carl and Lindsten, Fredrik and Roll, Jacob and Sch{\"o}n, Thomas B.},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.06977 [cs, stat]},
  eprint = {1902.06977},
  primaryclass = {cs, stat},
  urldate = {2019-03-20},
  abstract = {Probabilistic classifiers output a probability distribution on target classes rather than just a class prediction. Besides providing a clear separation of prediction and decision making, the main advantage of probabilistic models is their ability to represent uncertainty about predictions. In safety-critical applications, it is pivotal for a model to possess an adequate sense of uncertainty, which for probabilistic classifiers translates into outputting probability distributions that are consistent with the empirical frequencies observed from realized outcomes. A classifier with such a property is called calibrated. In this work, we develop a general theoretical calibration evaluation framework grounded in probability theory, and point out subtleties present in model calibration evaluation that lead to refined interpretations of existing evaluation techniques. Lastly, we propose new ways to quantify and visualize miscalibration in probabilistic classification, including novel multidimensional reliability diagrams.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GSSG7CKN/vaicenavicius_evaluating_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/8R3555B2/1902.html}
}

@article{valko_lectures_,
  title = {Lectures 6 -- 7 : {{Marchenko-Pastur Law}}},
  author = {Valko, B},
  pages = {7},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7HAUZXFS/Valko - Lectures 6 â€“ 7  Marchenko-Pastur Law.pdf}
}

@article{vandegeer_asymptotically_2014,
  title = {On Asymptotically Optimal Confidence Regions and Tests for High-Dimensional Models},
  author = {{van de Geer}, Sara and B{\"u}hlmann, Peter and Ritov, Ya'acov and Dezeure, Ruben},
  year = {2014},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {42},
  number = {3},
  eprint = {1303.0518},
  primaryclass = {math, stat},
  issn = {0090-5364},
  doi = {10.1214/14-AOS1221},
  urldate = {2024-07-10},
  abstract = {We propose a general method for constructing confidence intervals and statistical tests for single or low-dimensional components of a large parameter vector in a high-dimensional model. It can be easily adjusted for multiplicity taking dependence among tests into account. For linear models, our method is essentially the same as in Zhang and Zhang [J. R. Stat. Soc. Ser. B Stat. Methodol. 76 (2014) 217-242]: we analyze its asymptotic properties and establish its asymptotic optimality in terms of semiparametric efficiency. Our method naturally extends to generalized linear models with convex loss functions. We develop the corresponding theory which includes a careful analysis for Gaussian, sub-Gaussian and bounded correlated designs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory},
  file = {/Users/antoniohortaribeiro/Zotero/storage/JM4BXJI3/van de Geer et al. - 2014 - On asymptotically optimal confidence regions and t.pdf}
}

@article{vandeleur_automatic_2020,
  title = {Automatic {{Triage}} of 12-{{Lead ECGs Using Deep Convolutional Neural Networks}}},
  author = {{van de Leur}, Rutger R. and Blom, Lennart J. and Gavves, Efstratios and Hof, Irene E. and {van der Heijden}, Jeroen F. and Clappers, Nick C. and Doevendans, Pieter A. and Hassink, Rutger J. and {van Es}, Ren{\'e}},
  year = {2020},
  month = may,
  journal = {Journal of the American Heart Association},
  volume = {9},
  number = {10},
  issn = {2047-9980},
  doi = {10.1161/JAHA.119.015138},
  urldate = {2020-07-28},
  abstract = {BACKGROUND: The correct interpretation of the ECG is pivotal for the accurate diagnosis of many cardiac abnormalities, and conventional computerized interpretation has not been able to reach physician-\-level accuracy in detecting (acute) cardiac abnormalities. This study aims to develop and validate a deep neural network for comprehensive automated ECG triage in daily practice. METHODS AND RESULTS: We developed a 37-\-layer convolutional residual deep neural network on a data set of free-\-text physician-a\-nnotated 12-l\-ead ECGs. The deep neural network was trained on a data set with 336.835 recordings from 142.040 patients and validated on an independent validation data set (n=984), annotated by a panel of 5 cardiologists electrophysiologists. The 12-\-lead ECGs were acquired in all noncardiology departments of the University Medical Center Utrecht. The algorithm learned to classify these ECGs into the following 4 triage categories: normal, abnormal not acute, subacute, and acute. Discriminative performance is presented with overall and category-s\- pecific concordance statistics, polytomous discrimination indexes, sensitivities, specificities, and positive and negative predictive values. The patients in the validation data set had a mean age of 60.4 years and 54.3\% were men. The deep neural network showed excellent overall discrimination with an overall concordance statistic of 0.93 (95\% CI, 0.92--0.95) and a polytomous discriminatory index of 0.83 (95\% CI, 0.79--0.87). CONCLUSIONS: This study demonstrates that an end-\-to-\-end deep neural network can be accurately trained on unstructured free-t\-ext physician annotations and used to consistently triage 12-l\-ead ECGs. When further fine-\-tuned with other clinical outcomes and externally validated in clinical practice, the demonstrated deep learning--based ECG interpretation can potentially improve time to treatment and decrease healthcare burden.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TN72KPQC/van de Leur et al. - 2020 - Automatic Triage of 12â€Lead ECGs Using Deep Convol.pdf}
}

@book{vandervaart_asymptotic_2000,
  title = {Asymptotic Statistics},
  author = {{van der Vaart}, A. W.},
  year = {2000},
  series = {Cambridge Series in Statistical and Probabilistic Mathematics},
  publisher = {Cambridge University Press},
  isbn = {0-521-78450-6 978-0-521-78450-4 0-521-49603-9 978-0-521-49603-2},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7KZQAYK3/Vaart - 1998 - Asymptotic statistics.pdf}
}

@techreport{vandomselaar_nonlinear_1975,
  title = {Nonlinear {{Parameter Estimation}} in {{Initial Value Problems}}},
  author = {Van Domselaar, B and Hemker, Piet W},
  year = {1975},
  institution = {SIS-76-1121},
  annotation = {00002}
}

@techreport{vandomselaar_nonlinear_1975a,
  title = {Nonlinear {{Parameter Estimation}} in {{Initial Value Problems}}},
  author = {Van Domselaar, B and Hemker, Piet W},
  year = {1975},
  institution = {SIS-76-1121}
}

@article{vanmulders_two_2009,
  title = {Two {{Nonlinear Optimization Methods}} for {{Black Box Identification Compared}}},
  author = {Van Mulders, Anne and Schoukens, Johan and Volckaert, Marnix and Diehl, Moritz},
  year = {2009},
  month = jan,
  journal = {15th IFAC Symposium on System Identification},
  volume = {42},
  number = {10},
  pages = {1086--1091},
  issn = {1474-6670},
  doi = {10/dt72p8},
  abstract = {In this paper, two nonlinear optimization methods for the identification of nonlinear systems are compared. Both methods estimate all the parameters of a polynomial nonlinear state-space model by means of a nonlinear least-squares optimization. While the first method does not estimate the states explicitly, the second estimates both states and parameters adding an extra constraint equation. Both methods are introduced and their similarities and differences are discussed utilizing simulation and experimental data. The unconstrained method appears to be faster and more memory efficient, while the constrained method is robust towards instabilities.},
  keywords = {constraints,Identification algorithms,nonlinear models,nonlinear systems,parameter estimation,state-space models},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3IB2YHP5/automatica_multiple_shooting.pdf}
}

@article{vanmulders_two_2010,
  title = {Two Nonlinear Optimization Methods for Black Box Identification Compared},
  author = {Van Mulders, Anne and Schoukens, Johan and Volckaert, Marnix and Diehl, Moritz},
  year = {2010},
  month = oct,
  journal = {Automatica},
  volume = {46},
  number = {10},
  pages = {1675--1681},
  issn = {00051098},
  doi = {10/dzv9r8},
  urldate = {2019-03-19},
  abstract = {In this paper, two nonlinear optimization methods for the identification of nonlinear systems are compared. Both methods estimate the parameters of e.g. a polynomial nonlinear state-space model by means of a nonlinear least-squares optimization of the same cost function. While the first method does not estimate the states explicitly, the second method estimates both states and parameters adding an extra constraint equation. Both methods are introduced and their similarities and differences are discussed utilizing simulation data. The unconstrained method appears to be faster and more memory efficient, but the constrained method has a significant advantage as well: it is robust for unstable systems of which bounded input--output data can be measured (e.g. a system captured in a stabilizing feedback loop). Both methods have successfully been applied on real-life measurement data.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q4ETHCBA/Van Mulders et al. - 2010 - Two nonlinear optimization methods for black box i.pdf}
}

@article{vanoverschee_n4sid_1994,
  title = {{{N4SID}}: {{Subspace}} Algorithms for the Identification of Combined Deterministic-Stochastic Systems},
  shorttitle = {{{N4SID}}},
  author = {Van Overschee, Peter and De Moor, Bart},
  year = {1994},
  month = jan,
  journal = {Automatica},
  series = {Special Issue on Statistical Signal Processing and Control},
  volume = {30},
  number = {1},
  pages = {75--93},
  issn = {0005-1098},
  doi = {10.1016/0005-1098(94)90230-5},
  abstract = {Recently a great deal of attention has been given to numerical algorithms for subspace state space system identification (N4SID). In this paper, we derive two new N4SID algorithms to identify mixed deterministic-stochastic systems. Both algorithms determine state sequences through the projection of input and output data. These state sequences are shown to be outputs of non-steady state Kalman filter banks. From these it is easy to determine the state space system matrices. The N4SID algorithms are always convergent (non-iterative) and numerically stable since they only make use of QR and Singular Value Decompositions. Both N4SID algorithms are similar, but the second one trades off accuracy for simplicity. These new algorithms are compared with existing subspace algorithms in theory and in practice.},
  keywords = {difference equations,Kalman filters,Multivariable systems,QR and singular value decomposition,state space methods,system identification},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4EN9QU9F/0005109894902305.html;/Users/antoniohortaribeiro/Zotero/storage/TXQB2NXA/0005109894902305.html}
}

@book{vanoverschee_subspace_2012,
  title = {Subspace Identification for Linear Systems: {{Theory}}---{{Implementation}}---{{Applications}}},
  shorttitle = {Subspace Identification for Linear Systems},
  author = {Van Overschee, Peter and De Moor, B. L.},
  year = {2012},
  publisher = {Springer Science \& Business Media},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VGIJAP9M/van overschee_subspace_2012.pdf}
}

@article{vanoverschee_unifying_1995,
  title = {A Unifying Theorem for Three Subspace System Identification Algorithms},
  author = {Van Overschee, Peter and De Moor, Bart},
  year = {1995},
  month = dec,
  journal = {Automatica},
  series = {Trends in {{System Identification}}},
  volume = {31},
  number = {12},
  pages = {1853--1864},
  issn = {0005-1098},
  doi = {10.1016/0005-1098(95)00072-0},
  abstract = {The aim of this paper is to indicate and explore the similarities between three different subspace algorithms for the identification of combined deterministic-stochastic systems. The similarities between these algorithms have been obscured, due to different notations and backgrounds. It is shown that all three algorithms are special cases of one unifying theorem. The comparison also reveals that the three algorithms use exactly the same subspace to determine the order and the extended observability matrix, but that the weighting matrix, used to calculate a basis for the column space of the observability matrix is different in the three cases.},
  keywords = {difference equations,Kalman filters,linear algebra,Multivariable systems,state-space methods,stochastic systems,subspace methods,system identification},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/446GFH43/0005109895000720.html;/Users/antoniohortaribeiro/Zotero/storage/E8B2PXVW/0005109895000720.html}
}

@article{vanschoren_openml_2014,
  title = {{{OpenML}}: Networked Science in Machine Learning},
  shorttitle = {{{OpenML}}},
  author = {Vanschoren, Joaquin and {van Rijn}, Jan N. and Bischl, Bernd and Torgo, Luis},
  year = {2014},
  month = jun,
  journal = {ACM SIGKDD Explorations Newsletter},
  volume = {15},
  number = {2},
  pages = {49--60},
  issn = {1931-0145},
  doi = {10.1145/2641190.2641198},
  urldate = {2024-05-16},
  abstract = {Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YTVCM9RJ/Vanschoren et al. - 2014 - OpenML networked science in machine learning.pdf}
}

@book{vapnik_nature_2013,
  title = {The {{Nature}} of {{Statistical Learning Theory}}},
  author = {Vapnik, V.},
  year = {2013},
  series = {Information {{Science}} and {{Statistics}}},
  publisher = {Springer New York},
  isbn = {978-1-4757-3264-1},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/APSNV5GT/vapnik_the_2013.pdf}
}

@article{vargas_improved_2019,
  title = {Improved Learning Algorithm for Two-Layer Neural Networks for Identification of Nonlinear Systems},
  author = {Vargas, Jos{\'e} A.R. and Pedrycz, Witold and Hemerly, Elder M.},
  year = {2019},
  month = feb,
  journal = {Neurocomputing},
  volume = {329},
  pages = {86--96},
  issn = {0925-2312},
  doi = {10/gfwt99},
  abstract = {This study is concerned with the asymptotic identification of nonlinear systems based on Lyapunov theory and two-layer neural networks. An improved identification model enhanced with a feedback term and a novel adaptation law for the threshold offset, associated with the output weight matrix, is introduced to assure the convergence of the online prediction error, even in the presence of approximation error and bounded disturbances and when upper bounds for these perturbations are not known in advance. The effectiveness of the proposed method and its application to the identification of a hyperchaotic system and control of a welding system is investigated.},
  keywords = {Dynamical systems,Identification,Lyapunov methods,Neural networks}
}

@article{vasconcelos_effective_2020,
  title = {An {{Effective Anti-Aliasing Approach}} for {{Residual Networks}}},
  author = {Vasconcelos, Cristina and Larochelle, Hugo and Dumoulin, Vincent and Roux, Nicolas Le and Goroshin, Ross},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.10675},
  eprint = {2011.10675},
  abstract = {Image pre-processing in the frequency domain has traditionally played a vital role in computer vision and was even part of the standard pipeline in the early days of deep learning. However, with the advent of large datasets, many practitioners concluded that this was unnecessary due to the belief that these priors can be learned from the data itself. Frequency aliasing is a phenomenon that may occur when sub-sampling any signal, such as an image or feature map, causing distortion in the sub-sampled output. We show that we can mitigate this effect by placing non-trainable blur filters and using smooth activation functions at key locations, particularly where networks lack the capacity to learn them. These simple architectural changes lead to substantial improvements in out-of-distribution generalization on both image classification under natural corruptions on ImageNet-C [10] and few-shot learning on Meta-Dataset [17], without introducing additional trainable parameters and using the default hyper-parameters of open source codebases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q29Z6XPU/Vasconcelos et al. - 2020 - An Effective Anti-Aliasing Approach for Residual N.pdf;/Users/antoniohortaribeiro/Zotero/storage/GSCS23FJ/2011.html}
}

@inproceedings{vaswani_attention_2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  pages = {5998--6008},
  urldate = {2019-04-24},
  keywords = {ðŸ”No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WTDCFPKJ/vaswani_attention_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/XHJMR88I/vaswani_attention_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/W5KUJF6F/1706.html;/Users/antoniohortaribeiro/Zotero/storage/ZFL3N7ZA/7181-attention-is-all-you-need.html}
}

@inproceedings{veloso_lazy_2006,
  title = {Lazy {{Associative Classification}}},
  booktitle = {Proceedingsof the 6th {{International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Veloso, Adriano and Meira Jr, Wagner and Zaki, M. J.},
  year = {2006},
  pages = {645--654},
  doi = {10/bhq7p6},
  abstract = {Decision tree classifiers perform a greedy search for rules by heuristically selecting the most promising features. Such greedy (local) search may discard important rules. Associative classifiers, on the other hand, perform a global search for rules satisfying some quality constraints (i.e., minimum support). This global search, however, may generate a large number of rules. Further, many of these rules may be useless during classification, and worst, important rules may never be mined. Lazy (non-eager) associative classification overcomes this problem by focusing on the features of the given test instance, increasing the chance of generating more rules that are useful for classifying the test instance. In this paper we assess the performance of lazy associative classification. First we demonstrate that an associative classifier performs no worse than the corresponding decision tree classifier. Also we demonstrate that lazy classifiers outperform the corresponding eager ones. Our claims are empirically confirmed by an extensive set of experimental results. We show that our proposed lazy associative classifier is responsible for an error rate reduction of approximately 10\% when compared against its eager counterpart, and for a reduction of 20\% when compared against a decision tree classifier. A simple caching mechanism makes lazy associative classification fast, and thus improvements in the execution time are also observed.},
  keywords = {associative rule mining,caching mechanism,Classification tree analysis,Computer science,data mining,Data mining,decision tree classifier,decision trees,Decision trees,Error analysis,Genetic algorithms,greedy algorithms,greedy search,lazy associative classification,lazy learning,learning (artificial intelligence),Neural networks,pattern classification,Predictive models,Testing,Training data,tree searching},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QGWZYXI5/veloso_lazy_2006.pdf;/Users/antoniohortaribeiro/Zotero/storage/X4J9CP92/4053090.html}
}

@inproceedings{verdie_tilde_2015,
  title = {{{TILDE}}: {{A Temporally Invariant Learned DEtector}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Verdie, Yannick and Yi, Kwang and Fua, Pascal and Lepetit, Vincent},
  year = {2015},
  pages = {5279--5288},
  annotation = {00000}
}

@article{veronese_emergency_2016,
  title = {Emergency Physician Accuracy in Interpreting Electrocardiograms with Potential {{ST-segment}} Elevation Myocardial Infarction: {{Is}} It Enough?},
  shorttitle = {Emergency Physician Accuracy in Interpreting Electrocardiograms with Potential {{ST-segment}} Elevation Myocardial Infarction},
  author = {Veronese, Giacomo and Germini, Federico and Ingrassia, Stella and Cutuli, Ombretta and Donati, Valeria and Bonacchini, Luca and Marcucci, Maura and Fabbri, Andrea and {Italian Society of Emergency Medicine (SIMEU)}},
  year = {2016},
  month = mar,
  journal = {Acute Cardiac Care},
  volume = {18},
  number = {1},
  pages = {7--10},
  issn = {1748-295X},
  doi = {10.1080/17482941.2016.1234058},
  abstract = {BACKGROUND: Electrocardiogram (ECG) interpretation is widely performed by emergency physicians. We aimed to determine the accuracy of interpretation of potential ST-segment elevation myocardial infarction (STEMI) ECGs by emergency physicians. METHODS: Thirty-six ECGs resulted in putative STEMI diagnoses were selected. Participants were asked to focus on whether or not the ECG in question met the diagnostic criteria for an acutely blocked coronary artery causing a STEMI. Based on the coronary angiogram, a binary outcome of accurate versus inaccurate ECG interpretation was defined. We computed the overall sensitivity, specificity, accuracy and 95\% confidence intervals (95\%CIs) for ECG interpretation. Data on participant training level, working experience and place were collected. RESULTS: 135 participants interpreted 4603 ECGs. Overall sensitivity to identify 'true' STEMI ECGs was 64.5\% (95\%CI: 62.8-66.3); specificity in determining 'false' ECGs was 78\% (95\%CI: 76-80.1). Overall accuracy was modest (69.1, 95\%CI: 67.8-70.4). Higher accuracy in ECG interpretation was observed for attending physicians, participants working in tertiary care hospitals and those more experienced. CONCLUSION: The accuracy of interpretation of potential STEMI ECGs was modest among emergency physicians. The study supports the notion that ECG interpretation for establishing a STEMI diagnosis lacks the necessary sensitivity and specificity to be considered a reliable 'stand-alone' diagnostic test.},
  langid = {english},
  pmid = {27759433},
  keywords = {Accuracy,Clinical Competence,Coronary Angiography,Diagnostic Errors,Dimensional Measurement Accuracy,electrocardiogram,Electrocardiography,Emergency Medical Services,emergency medicine,Health Care Surveys,Humans,Italy,myocardial infarction,Physicians,Reproducibility of Results,Sensitivity and Specificity,ST Elevation Myocardial Infarction}
}

@book{vershynin_highdimensional_2018,
  title = {High-{{Dimensional Probability}}},
  author = {Vershynin, Roman},
  year = {2018},
  series = {Cambridge Series in Statistical and Probabilistic Mathematics},
  publisher = {Cambridge University Press},
  isbn = {978-1-108-41519-4 1-108-41519-9 978-1-108-23159-6},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DRJVEJB2/High-Dimensional Probability.pdf}
}

@article{verstraete_lorentz_2002,
  title = {Lorentz Singular-Value Decomposition and Its Applications to Pure States of Three Qubits},
  author = {Verstraete, Frank and Dehaene, Jeroen and De Moor, Bart},
  year = {2002},
  month = feb,
  journal = {Physical Review A},
  volume = {65},
  number = {3},
  pages = {032308},
  doi = {10.1103/PhysRevA.65.032308},
  abstract = {All mixed states of two qubits can be brought into normal form by the action of local operations and classical communication operations of the kind {$\rho\prime$}=(A{$\otimes$}B){$\rho$}(A{$\otimes$}B){\dag}. These normal forms can be obtained by considering a Lorentz singular-value decomposition on a real parametrization of the density matrix. We show that the Lorentz singular values are variationally defined and give rise to entanglement monotones, with as a special case the concurrence. Next a necessary and sufficient criterion is conjectured for a mixed state to be convertible into another specific one with a nonzero probability. Finally the formalism of the Lorentz singular-value decomposition is applied to tripartite pure states of qubits. New proofs are given for the existence of the Greenberger-Horne-Zeilinger (GHZ) class and W class of states, and a rigorous proof for the optimal distillation of a GHZ state is derived.},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HSWAAVK9/PhysRevA.65.html;/Users/antoniohortaribeiro/Zotero/storage/MGJ2EQQZ/PhysRevA.65.html}
}

@book{vidyasagar_nonlinear_1993,
  title = {Nonlinear Systems Analysis},
  author = {Vidyasagar, M.},
  year = {1993},
  edition = {2nd ed},
  publisher = {Prentice Hall},
  address = {Englewood Cliffs, N.J},
  isbn = {978-0-13-623463-0},
  langid = {english},
  lccn = {QA402 .V53 1993},
  keywords = {{Differential equations, Nonlinear},System analysis},
  annotation = {01559},
  file = {/Users/antoniohortaribeiro/Zotero/storage/37P75EKC/vidyasagar_nonlinear_1993.pdf;/Users/antoniohortaribeiro/Zotero/storage/67K84KEG/Vidyasagar - 1993 - Nonlinear systems analysis.pdf;/Users/antoniohortaribeiro/Zotero/storage/LQUHJLDS/Vidyasagar - 1993 - Nonlinear systems analysis.pdf}
}

@article{vilhjalmsson_modeling_2015,
  title = {Modeling {{Linkage Disequilibrium Increases Accuracy}} of {{Polygenic Risk Scores}}},
  author = {Vilhj{\'a}lmsson, Bjarni J. and Yang, Jian and Finucane, Hilary K. and Gusev, Alexander and Lindstr{\"o}m, Sara and Ripke, Stephan and Genovese, Giulio and Loh, Po-Ru and Bhatia, Gaurav and Do, Ron and Hayeck, Tristan and Won, Hong-Hee and Consortium, Schizophrenia Working Group of the Psychiatric Genomics and Discovery, Biology and Kathiresan, Sekar and Pato, Michele and Pato, Carlos and Tamimi, Rulla and Stahl, Eli and Zaitlen, Noah and Pasaniuc, Bogdan and Belbin, Gillian and Kenny, Eimear E. and Schierup, Mikkel H. and Jager, Philip De and Patsopoulos, Nikolaos A. and McCarroll, Steve and Daly, Mark and Purcell, Shaun and Chasman, Daniel and Neale, Benjamin and Goddard, Michael and Visscher, Peter M. and Kraft, Peter and Patterson, Nick and Price, Alkes L.},
  year = {2015},
  month = oct,
  journal = {American Journal of Human Genetics},
  volume = {97},
  number = {4},
  pages = {576},
  publisher = {Elsevier},
  doi = {10.1016/j.ajhg.2015.09.001},
  urldate = {2024-01-24},
  abstract = {Polygenic risk scores have shown great promise in predicting complex disease risk and will become more accurate as training sample sizes increase. The standard approach for calculating risk scores involves linkage disequilibrium (LD)-based marker pruning ...},
  langid = {english},
  pmid = {26430803},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FFNL45YV/VilhjÃ¡lmsson et al. - 2015 - Modeling Linkage Disequilibrium Increases Accuracy.pdf}
}

@incollection{virmaux_lipschitz_2018,
  title = {Lipschitz Regularity of Deep Neural Networks: Analysis and Efficient Estimation},
  shorttitle = {Lipschitz Regularity of Deep Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Virmaux, Aladin and Scaman, Kevin},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {3835--3844},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-06-05},
  file = {/Users/antoniohortaribeiro/Zotero/storage/769K53JE/Virmaux and Scaman - 2018 - Lipschitz regularity of deep neural networks anal.pdf;/Users/antoniohortaribeiro/Zotero/storage/IGINN325/7640-lipschitz-regularity-of-deep-neural-networks-analysis-and-efficient-estimation.html}
}

@article{virtanen_scipy_2020,
  title = {{{SciPy}} 1.0--{{Fundamental Algorithms}} for {{Scientific Computing}} in {{Python}}},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and Contributors, SciPy 1.0},
  year = {2020},
  journal = {Nature Methods},
  volume = {17},
  number = {3},
  eprint = {1907.10121},
  pages = {261--272},
  doi = {10.1038/s41592-019-0686-2},
  abstract = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Mathematical Software,Computer Science - Software Engineering,Physics - Computational Physics},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9TI3V9ZK/1907.html}
}

@article{vlachas_datadriven_2018,
  title = {Data-Driven Forecasting of High-Dimensional Chaotic Systems with Long Short-Term Memory Networks},
  author = {Vlachas, Pantelis R. and Byeon, Wonmin and Wan, Zhong Y. and Sapsis, Themistoklis P. and Koumoutsakos, Petros},
  year = {2018},
  month = may,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {474},
  number = {2213},
  pages = {20170844},
  publisher = {Royal Society},
  doi = {10.1098/rspa.2017.0844},
  urldate = {2022-06-15},
  abstract = {We introduce a data-driven forecasting method for high-dimensional chaotic systems using long short-term memory (LSTM) recurrent neural networks. The proposed LSTM neural networks perform inference of high-dimensional dynamical systems in their reduced order space and are shown to be an effective set of nonlinear approximators of their attractor. We demonstrate the forecasting performance of the LSTM and compare it with Gaussian processes (GPs) in time series obtained from the Lorenz 96 system, the Kuramoto--Sivashinsky equation and a prototype climate model. The LSTM networks outperform the GPs in short-term forecasting accuracy in all applications considered. A hybrid architecture, extending the LSTM with a mean stochastic model (MSM--LSTM), is proposed to ensure convergence to the invariant measure. This novel hybrid method is fully data-driven and extends the forecasting capabilities of LSTM networks.},
  keywords = {data-driven forecasting,Gaussian processes,long short-term memory,Lorenz 96,T21 barotropic climate model},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VI9M6686/Vlachas et al. - 2018 - Data-driven forecasting of high-dimensional chaoti.pdf}
}

@inproceedings{voglis_rectangular_2004,
  title = {A {{Rectangular Trust Region Dogleg Approach}} for {{Unconstrained}} and {{Bound Constrained Nonlinear Optimization}}},
  booktitle = {{{WSEAS Conference}}},
  author = {Voglis, C and Lagaris, {\relax IE}},
  year = {2004},
  pages = {17--19},
  annotation = {00000}
}

@article{vonbachmann_evaluating_2022,
  title = {Evaluating Regression and Probabilistic Methods for {{ECG-based}} Electrolyte Prediction},
  author = {Von Bachmann, Philipp and Gedon, Daniel and Gustafsson, Fredrik K. and Ribeiro, Ant{\^o}nio H. and Lampa, Erik and Gustafsson, Stefan and Sundstr{\"o}m, Johan and Sch{\"o}n, Thomas B.},
  year = {2022},
  month = dec,
  journal = {Scientific Reports},
  volume = {14},
  number = {15273},
  eprint = {2212.13890},
  doi = {10.1038/s41598-024-65223-w},
  abstract = {Imbalances in electrolyte concentrations can have severe consequences, but accurate and accessible measurements could improve patient outcomes. The current measurement method based on blood tests is accurate but invasive and time-consuming and is often unavailable for example in remote locations or an ambulance setting. In this paper, we explore the use of deep neural networks (DNNs) for regression tasks to accurately predict continuous electrolyte concentrations from electrocardiograms (ECGs), a quick and widely adopted tool. We analyze our DNN models on a novel dataset of over 290,000 ECGs across four major electrolytes and compare their performance with traditional machine learning models. For improved understanding, we also study the full spectrum from continuous predictions to a binary classification of extreme concentration levels. Finally, we investigate probabilistic regression approaches and explore uncertainty estimates for enhanced clinical usefulness. Our results show that DNNs outperform traditional models but model performance varies significantly across different electrolytes. While discretization leads to good classification performance, it does not address the original problem of continuous concentration level prediction. Probabilistic regression has practical potential, but our uncertainty estimates are not perfectly calibrated. Our study is therefore a first step towards developing an accurate and reliable ECG-based method for electrolyte concentration level prediction---a method with high potential impact within multiple clinical scenarios.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/45AMFVN2/Von Bachmann et al. - 2022 - ECG-Based Electrolyte Prediction Evaluating Regre.pdf;/Users/antoniohortaribeiro/Zotero/storage/FP9NRPHE/2212.html}
}

@article{vonbachmann_evaluating_2024,
  title = {Evaluating Regression and Probabilistic Methods for {{ECG-based}} Electrolyte Prediction},
  author = {{von Bachmann}, Philipp and Gedon, Daniel and Gustafsson, Fredrik K. and Ribeiro, Ant{\^o}nio H. and Lampa, Erik and Gustafsson, Stefan and Sundstr{\"o}m, Johan and Sch{\"o}n, Thomas B.},
  year = {2024},
  month = jul,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {15273},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-65223-w},
  urldate = {2024-08-19},
  abstract = {Imbalances in electrolyte concentrations can have severe consequences, but accurate and accessible measurements could improve patient outcomes. The current measurement method based on blood tests is accurate but invasive and time-consuming and is often unavailable for example in remote locations or an ambulance setting. In this paper, we explore the use of deep neural networks (DNNs) for regression tasks to accurately predict continuous electrolyte concentrations from electrocardiograms (ECGs), a quick and widely adopted tool. We analyze our DNN models on a novel dataset of over 290,000 ECGs across four major electrolytes and compare their performance with traditional machine learning models. For improved understanding, we also study the full spectrum from continuous predictions to a binary classification of extreme concentration levels. Finally, we investigate probabilistic regression approaches and explore uncertainty estimates for enhanced clinical usefulness. Our results show that DNNs outperform traditional models but model performance varies significantly across different electrolytes. While discretization leads to good classification performance, it does not address the original problem of continuous concentration level prediction. Probabilistic regression has practical potential, but our uncertainty estimates are not perfectly calibrated. Our study is therefore a first step towards developing an accurate and reliable ECG-based method for electrolyte concentration level prediction---a method with high potential impact within multiple clinical scenarios.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Biomedical engineering,Cardiology,Computer science},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SS7HUL4I/von Bachmann et al. - 2024 - Evaluating regression and probabilistic methods fo.pdf}
}

@article{vorontsov_orthogonality_2017,
  title = {On Orthogonality and Learning Recurrent Networks with Long Term Dependencies},
  author = {Vorontsov, Eugene and Trabelsi, Chiheb and Kadoury, Samuel and Pal, Chris},
  year = {2017},
  month = jan,
  journal = {arXiv:1702.00071 [cs]},
  eprint = {1702.00071},
  primaryclass = {cs},
  urldate = {2019-07-27},
  abstract = {It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and may therefore be a desirable property. This paper explores issues with optimization convergence, speed and gradient stability when encouraging or enforcing orthogonality. To perform this analysis, we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation. We find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/YP25YNFC/vorontsov_on_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/VPC3FNNI/1702.html}
}

@article{voros_iterative_2015,
  title = {Iterative Identification of Nonlinear Dynamic Systems with Output Backlash Using Three-Block Cascade Models},
  author = {V{\"o}r{\"o}s, Jozef},
  year = {2015},
  journal = {Nonlinear Dynamics},
  volume = {79},
  number = {3},
  pages = {2187--2195},
  issn = {0924-090X},
  doi = {10.1007/s11071-014-1804-4},
  annotation = {00000}
}

@article{voss_nonlinear_2004,
  title = {Nonlinear Dynamical System Identification from Uncertain and Indirect Measurements},
  author = {Voss, Henning U and Timmer, Jens and Kurths, J{\"u}rgen},
  year = {2004},
  journal = {International Journal of Bifurcation and Chaos},
  volume = {14},
  number = {06},
  pages = {1905--1933},
  doi = {10.1142/S0218127404010345},
  annotation = {00000}
}

@book{vukosavic_digital_2007,
  title = {Digital {{Control}} of {{Electrical Drives}}},
  author = {Vukosavic, S.N.},
  year = {2007},
  series = {Power {{Electronics}} and {{Power Systems}}},
  publisher = {Springer US},
  isbn = {978-0-387-48598-0},
  annotation = {00093},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9XP5IBT6/vukosavic_digital_2007.pdf;/Users/antoniohortaribeiro/Zotero/storage/AVPNC7H3/vukosavic_digital_2007.pdf;/Users/antoniohortaribeiro/Zotero/storage/DZ7EFST3/vukosavic_digital_2007.pdf;/Users/antoniohortaribeiro/Zotero/storage/KJN6UFP2/vukosavic_digital_2007.pdf;/Users/antoniohortaribeiro/Zotero/storage/QQ65K9D2/vukosavic_digital_2007.pdf;/Users/antoniohortaribeiro/Zotero/storage/R8JNBZME/vukosavic_digital_2007.pdf;/Users/antoniohortaribeiro/Zotero/storage/RUT4WNXZ/vukosavic_digital_2007.pdf;/Users/antoniohortaribeiro/Zotero/storage/TISIXZ28/vukosavic_digital_2007.pdf;/Users/antoniohortaribeiro/Zotero/storage/UGEMBEE2/vukosavic_digital_2007.pdf;/Users/antoniohortaribeiro/Zotero/storage/XNK59ID7/vukosavic_digital_2007.pdf}
}

@incollection{vuorio_multimodal_2019,
  title = {Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Vuorio, Risto and Sun, Shao-Hua and Hu, Hexiang and Lim, Joseph J},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {1--12},
  publisher = {Curran Associates, Inc.}
}

@article{wachter_implementation_2006,
  title = {On the Implementation of an Interior-Point Filter Line-Search Algorithm for Large-Scale Nonlinear Programming},
  author = {W{\"a}chter, Andreas and Biegler, Lorenz T.},
  year = {2006},
  month = mar,
  journal = {Mathematical Programming},
  volume = {106},
  number = {1},
  pages = {25--57},
  issn = {0025-5610, 1436-4646},
  doi = {10.1007/s10107-004-0559-y},
  urldate = {2017-08-20},
  langid = {english},
  annotation = {04288},
  file = {/Users/antoniohortaribeiro/Zotero/storage/EI2P6ZCV/wÃ¤chter_on the_2006.pdf}
}

@article{wagberg_regularized_2017,
  title = {Regularized Parametric System Identification: A Decision-Theoretic Formulation},
  shorttitle = {Regularized Parametric System Identification},
  author = {W{\aa}gberg, Johan and Zachariah, Dave and Sch{\"o}n, Thomas B.},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.04009 [cs]},
  eprint = {1710.04009},
  primaryclass = {cs},
  abstract = {Parametric prediction error methods constitute a classical approach to the identification of linear dynamic systems with excellent large-sample properties. A more recent regularized approach, inspired by machine learning and Bayesian methods, has also gained attention. Methods based on this approach estimate the system impulse response with excellent small-sample properties. In several applications, however, it is desirable to obtain a compact representation of the system in the form of a parametric model. By viewing the identification of such models as a decision, we develop a decision-theoretic formulation of the parametric system identification problem that bridges the gap between the classical and regularized approaches above. Using the output-error model class as an illustration, we show that this decision-theoretic approach leads to a regularized method that is robust to small sample-sizes as well as overparameterization.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Systems and Control},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IC94CDJN/wÃ¥gberg_regularize_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/QCUEC9TN/wÃ¥gberg_regularize_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/4M2QIHUJ/1710.html}
}

@article{wagner_ptbxl_2020,
  title = {{{PTB-XL}}, a Large Publicly Available Electrocardiography Dataset},
  author = {Wagner, Patrick and Strodthoff, Nils and Bousseljot, Ralf-Dieter and Kreiseler, Dieter and Lunze, Fatima I. and Samek, Wojciech and Schaeffter, Tobias},
  year = {2020},
  month = may,
  journal = {Scientific Data},
  volume = {7},
  number = {1},
  pages = {154},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-020-0495-6},
  urldate = {2020-07-14},
  abstract = {Electrocardiography (ECG) is a key non-invasive diagnostic tool for cardiovascular diseases which is increasingly supported by algorithms based on machine learning. Major obstacles for the development of automatic ECG interpretation algorithms are both the lack of public datasets and well-defined benchmarking procedures to allow comparison s of different algorithms. To address these issues, we put forward PTB-XL, the to-date largest freely accessible clinical 12-lead ECG-waveform dataset comprising 21837 records from 18885 patients of 10\,seconds length. The ECG-waveform data was annotated by up to two cardiologists as a multi-label dataset, where diagnostic labels were further aggregated into super and subclasses. The dataset covers a broad range of diagnostic classes including, in particular, a large fraction of healthy records. The combination with additional metadata on demographics, additional diagnostic statements, diagnosis likelihoods, manually annotated signal properties as well as suggested folds for splitting training and test sets turns the dataset into a rich resource for the development and the evaluation of automatic ECG interpretation algorithms.},
  copyright = {2020 The Author(s)},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7G5Q39GN/Wagner et al. - 2020 - PTB-XL, a large publicly available electrocardiogr.pdf;/Users/antoniohortaribeiro/Zotero/storage/FKMVJSHP/s41597-020-0495-6.html}
}

@book{wainwright_highdimensional_2019,
  title = {High-{{Simensional Statistics}}: A Non-{{Asymptotic Viewpoint}}},
  author = {Wainwright, Martin J},
  year = {2019},
  series = {Cambridge Series on Statistical and Probabilistic Mathematics 48},
  publisher = {Cambridge University Press},
  isbn = {978-1-108-62777-1 1-108-62777-3 978-1-108-49802-9},
  file = {/Users/antoniohortaribeiro/Zotero/storage/8QA2UCX8/Wainwright - 2019 - High-dimensional statistics a non-asymptotic view.pdf}
}

@article{waltz_interior_2006,
  title = {An Interior Algorithm for Nonlinear Optimization That Combines Line Search and Trust Region Steps},
  author = {Waltz, Richard A. and Morales, Jos{\'e} Luis and Nocedal, Jorge and Orban, Dominique},
  year = {2006},
  journal = {Mathematical programming},
  volume = {107},
  number = {3},
  pages = {391--408},
  doi = {10.1007/s10107-004-0560-5},
  urldate = {2017-08-20},
  annotation = {00643},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6EPFCUCJ/waltz_an_2006.pdf}
}

@article{waltz_interior_2006a,
  title = {An {{Interior Algorithm}} for {{Nonlinear Optimization That Combines Line Search}} and {{Trust Region Steps}}},
  author = {Waltz, Richard A. and Morales, Jos{\'e} Luis and Nocedal, Jorge and Orban, Dominique},
  year = {2006},
  journal = {Mathematical programming},
  volume = {107},
  number = {3},
  pages = {391--408},
  doi = {10/dp848r},
  urldate = {2017-08-20}
}

@article{wang_generalized_2015,
  title = {Generalized {{Single-Hidden Layer Feedforward Networks}} for {{Regression Problems}}},
  author = {Wang, N. and Er, M. J. and Han, M.},
  year = {2015},
  month = jun,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {26},
  number = {6},
  pages = {1161--1176},
  issn = {2162-237X},
  doi = {10.1109/TNNLS.2014.2334366},
  abstract = {In this paper, traditional single-hidden layer feedforward network (SLFN) is extended to novel generalized SLFN (GSLFN) by employing polynomial functions of inputs as output weights connecting randomly generated hidden units with corresponding output nodes. The significant contributions of this paper are as follows: 1) a primal GSLFN (P-GSLFN) is implemented using randomly generated hidden nodes and polynomial output weights whereby the regression matrix is augmented by full or partial input variables and only polynomial coefficients are to be estimated; 2) a simplified GSLFN (S-GSLFN) is realized by decomposing the polynomial output weights of the P-GSLFN into randomly generated polynomial nodes and tunable output weights; 3) both P- and S-GSLFN are able to achieve universal approximation if the output weights are tuned by ridge regression estimators; and 4) by virtue of the developed batch and online sequential ridge ELM (BR-ELM and OSR-ELM) learning algorithms, high performance of the proposed GSLFNs in terms of generalization and learning speed is guaranteed. Comprehensive simulation studies and comparisons with standard SLFNs are carried out on real-world regression benchmark data sets. Simulation results demonstrate that the innovative GSLFNs using BR-ELM and OSR-ELM are superior to standard SLFNs in terms of accuracy, training speed, and structure compactness.},
  keywords = {Approximation capability,Approximation methods,batch ridge ELM learning algorithms,BR-ELM,Educational institutions,extreme learning machine (ELM),feedforward neural nets,feedforward neural networks,generalized single-hidden layer feedforward networks,generalized single-hidden layer feedforward networks (GSLFN),GSLFN,Joining processes,learning (artificial intelligence),matrix algebra,novel generalized SLFN,online sequential ridge ELM learning algorithms,OSR-ELM,polynomial functions,polynomial output weights,polynomials,randomly generated hidden nodes,regression analysis,regression problems,ridge regression,ridge regression.,Standards,universal approximation},
  annotation = {00073},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RGDVBJHR/wang_generalize_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/J4ZZGQ42/6856201.html}
}

@article{wang_hybrid_2016,
  title = {Hybrid Recursive Least Squares Algorithm for Online Sequential Identification Using Data Chunks},
  author = {Wang, Ning and Sun, Jing-Chao and Er, Meng Joo and Liu, Yan-Cheng},
  year = {2016},
  month = jan,
  journal = {Neurocomputing},
  volume = {174},
  pages = {651--660},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2015.09.090},
  abstract = {In this paper, a hybrid recursive least squares (HRLS) algorithm for online identification using sequential chunk-by-chunk observations is proposed. By employing the optimization-based least squares (O-LS), the HRLS can be initialized with any chunk of data samples and works successively in two recursive procedures for updating the inverse matrix with minimal dimension and least rank-deficiency, and thereby contributing to fast and stable online identification. Since norms of the output weight and training errors are minimized simultaneously, the HRLS achieves high accuracy in terms of both generalization and approximation. Simulation studies and comprehensive comparisons demonstrate that the HRLS is numerically more stable and superior to other algorithms in terms of accuracy and speed.},
  keywords = {Hybrid recursive least squares (HRLS),Online sequential identification,Recursive orthogonal least squares (ROLS),Single-hidden-layer feedforward network (SLFN)},
  annotation = {00014},
  file = {/Users/antoniohortaribeiro/Zotero/storage/R84HFM5V/wang_hybrid_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/H7VSIVFQ/S092523121501423X.html}
}

@inproceedings{wang_new_2002,
  title = {A New Method for Evaluating {{ECG}} Signal Quality for Multi-Lead Arrhythmia Analysis},
  booktitle = {Computers in {{Cardiology}}, 2002},
  author = {Wang, J. Y.},
  year = {2002},
  pages = {85--88},
  publisher = {IEEE},
  annotation = {00031},
  file = {/Users/antoniohortaribeiro/Zotero/storage/RM4I54WC/wang_a new_2002.pdf}
}

@inproceedings{wang_new_2017,
  title = {A New Concept Using {{LSTM Neural Networks}} for Dynamic System Identification},
  booktitle = {2017 {{American Control Conference}} ({{ACC}})},
  author = {Wang, Yu},
  year = {2017},
  month = may,
  pages = {5324--5329},
  doi = {10.23919/ACC.2017.7963782},
  abstract = {Recently, Recurrent Neural Network becomes a very popular research topic in machine learning field. Many new ideas and RNN structures have been generated by different authors, including long short term memory (LSTM) RNN and Gated Recurrent United (GRU) RNN ([1],[2]), a number of applications have also been developed among various research labs or industrial companies ([3]-[5]). Most of these schemes, however, are only applicable to machine learning problems, or static systems in control field. In this paper, a new concept of applying one of the most popular RNN approach - LSTM to identify and control dynamic system is to be investigated. Both identification (or learning) dynamic system and design of controller based on identification are going to be discussed. Also, a new concept of using a convex-based LSTM networks for fast learning purpose will be explained in detail. Simulation studies will be presented to demonstrated the new LSTM structure performs much better than conventional RNN and even single LSTM network.},
  keywords = {Adaptation models,Control systems,convex-based LSTM networks,dynamic system identification,gated recurrent united RNN,learning (artificial intelligence),Logic gates,long short term memory RNN,LSTM neural networks,machine learning,Periodic structures,recurrent neural nets,recurrent neural network,Recurrent neural networks,Training},
  annotation = {00011},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BNUWBVG5/wang_a new_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/DZ5XA5M4/7963782.html}
}

@article{wang_parsimonious_2014,
  title = {Parsimonious {{Extreme Learning Machine Using Recursive Orthogonal Least Squares}}},
  author = {Wang, N. and Er, M. J. and Han, M.},
  year = {2014},
  month = oct,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {25},
  number = {10},
  pages = {1828--1841},
  issn = {2162-237X},
  doi = {10.1109/TNNLS.2013.2296048},
  abstract = {Novel constructive and destructive parsimonious extreme learning machines (CP- and DP-ELM) are proposed in this paper. By virtue of the proposed ELMs, parsimonious structure and excellent generalization of multiinput-multioutput single hidden-layer feedforward networks (SLFNs) are obtained. The proposed ELMs are developed by innovative decomposition of the recursive orthogonal least squares procedure into sequential partial orthogonalization (SPO). The salient features of the proposed approaches are as follows: 1) Initial hidden nodes are randomly generated by the ELM methodology and recursively orthogonalized into an upper triangular matrix with dramatic reduction in matrix size; 2) the constructive SPO in the CP-ELM focuses on the partial matrix with the subcolumn of the selected regressor including nonzeros as the first column while the destructive SPO in the DP-ELM operates on the partial matrix including elements determined by the removed regressor; 3) termination criteria for CP- and DP-ELM are simplified by the additional residual error reduction method; and 4) the output weights of the SLFN need not be solved in the model selection procedure and is derived from the final upper triangular equation by backward substitution. Both single- and multi-output real-world regression data sets are used to verify the effectiveness and superiority of the CP- and DP-ELM in terms of parsimonious architecture and generalization accuracy. Innovative applications to nonlinear time-series modeling demonstrate superior identification results.},
  keywords = {backward substitution,constructive parsimonious extreme learning machine,Context,CP-ELM,data analysis,destructive parsimonious extreme learning machine,destructive SPO,DP-ELM,Educational institutions,ELM methodology,extreme learning machine (ELM),feedforward neural nets,generalisation (artificial intelligence),generalization accuracy,hidden node random generation,learning (artificial intelligence),least squares approximations,Mathematical model,matrix algebra,Matrix decomposition,matrix size reduction,multiinput-multioutput single hidden-layer feedforward networks,nonlinear time-series modeling,parsimonious architecture,parsimonious model selection,parsimonious structure,partial matrix,recursive functions,Recursive orthogonal least squares (ROLS),recursive orthogonal least squares decomposition,recursive orthogonalization,regression analysis,regression data set,regressor,residual error reduction method,sequential partial orthogonalization,sequential partial orthogonalization (SPO),single hidden-layer feedforward network (SLFN),single hidden-layer feedforward network (SLFN).,SLFN,termination criteria,time series,Training,Training data,upper triangular equation,upper triangular matrix,Vectors},
  annotation = {00083},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NT6N93CP/wang_parsimonio_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/ZJMG54DS/6704311.html}
}

@article{wang_recovery_2011,
  title = {Recovery of Seismic Wavefields Based on Compressive Sensing by an L1-Norm Constrained Trust Region Method and the Piecewise Random Subsampling},
  author = {Wang, Yanfei and Cao, Jingjie and Yang, Changchun},
  year = {2011},
  journal = {Geophysical Journal International},
  volume = {187},
  number = {1},
  pages = {199--213},
  doi = {10.1111/j.1365-246X.2011.05130.x},
  keywords = {â“Multiple DOI},
  annotation = {00000}
}

@article{wang_recovery_2011a,
  title = {Recovery of {{Seismic Wavefields Based}} on {{Compressive Sensing}} by an {{L1-Norm Constrained Trust Region Method}} and the {{Piecewise Random Subsampling}}},
  author = {Wang, Yanfei and Cao, Jingjie and Yang, Changchun},
  year = {2011},
  journal = {Geophysical Journal International},
  volume = {187},
  number = {1},
  pages = {199--213},
  keywords = {â“Multiple DOI},
  annotation = {00000}
}

@article{wang_regression_2007,
  title = {Regression {{Coefficient}} and {{Autoregressive Order Shrinkage}} and {{Selection Via}} the {{Lasso}}},
  author = {Wang, Hansheng and Li, Guodong and Tsai, Chih-Ling},
  year = {2007},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {69},
  number = {1},
  eprint = {4623254},
  eprinttype = {jstor},
  pages = {63--78},
  issn = {1369-7412},
  doi = {10.1111/j.1467-9868.2007.00577.x},
  abstract = {The least absolute shrinkage and selection operator ('lasso') has been widely used in regression shrinkage and selection. We extend its application to the regression model with autoregressive errors. Two types of lasso estimators are carefully studied. The first is similar to the traditional lasso estimator with only two tuning parameters (one for regression coefficients and the other for autoregression coeffi). These tuning parameters can be easily calculated via a data-driven method, but the resulting lasso estimator may not be fully efficient. To overcome this limitation, we propose a second lasso estimator which uses different tuning parameters for each coefficient. We show that this modified lasso can produce the estimator as efficiently as the oracle. Moreover, we propose an algorithm for tuning parameter estimates to obtain the modified lasso estimator. Simulation studies demonstrate that the modified estimator is superior to the traditional estimator. One empirical example is also presented to illustrate the usefulness of lasso estimators. The extension of the lasso to the autoregression with exogenous variables model is briefly discussed.},
  annotation = {00225},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ADZ4UD9T/wang_regression_2007.pdf;/Users/antoniohortaribeiro/Zotero/storage/B2HBUZI7/wang_regression_2007.pdf}
}

@article{wang_tight_2022,
  title = {Tight Bounds for Minimum {{L1-norm}} Interpolation of Noisy Data},
  author = {Wang, Guillaume and Donhauser, Konstantin and Yang, Fanny},
  year = {2022},
  month = mar,
  journal = {arXiv:2111.05987},
  eprint = {2111.05987},
  urldate = {2022-05-06},
  abstract = {We provide matching upper and lower bounds of order \${\textbackslash}sigma{\textasciicircum}2/{\textbackslash}log(d/n)\$ for the prediction error of the minimum \${\textbackslash}ell\_1\$-norm interpolator, a.k.a. basis pursuit. Our result is tight up to negligible terms when \$d {\textbackslash}gg n\$, and is the first to imply asymptotic consistency of noisy minimum-norm interpolation for isotropic features and sparse ground truths. Our work complements the literature on "benign overfitting" for minimum \${\textbackslash}ell\_2\$-norm interpolation, where asymptotic consistency can be achieved only when the features are effectively low-dimensional.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IB27WARJ/Wang et al_2022_Tight bounds for minimum l1-norm interpolation of noisy data.pdf;/Users/antoniohortaribeiro/Zotero/storage/RNLVRR48/2111.html}
}

@article{wang_understanding_2018,
  title = {Towards {{Understanding Learning Representations}}: {{To What Extent Do Different Neural Networks Learn}} the {{Same Representation}}},
  shorttitle = {Towards {{Understanding Learning Representations}}},
  author = {Wang, Liwei and Hu, Lunjia and Gu, Jiayuan and Wu, Yue and Hu, Zhiqiang and He, Kun and Hopcroft, John},
  year = {2018},
  month = oct,
  urldate = {2018-12-06},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/29RA5REY/wang_towards_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/ZY2FCWVC/1810.html}
}

@inproceedings{warwick_introduction_1996,
  title = {An Introduction to Radial Basis Functions for System Identification. {{A}} Comparison with Other Neural Network Methods},
  booktitle = {Decision and {{Control}}, 1996., {{Proceedings}} of the 35th {{IEEE Conference}} On},
  author = {Warwick, K and Craddock, R},
  year = {1996},
  volume = {1},
  pages = {464--469},
  publisher = {IEEE},
  annotation = {00038}
}

@book{wasserman_all_2013,
  title = {All of Statistics: A Concise Course in Statistical Inference},
  shorttitle = {All of Statistics},
  author = {Wasserman, Larry},
  year = {2013},
  publisher = {Springer Science \& Business Media},
  annotation = {03141},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6N7KAXB9/wasserman_all of_2013.pdf}
}

@article{wehrmann_hierarchical_,
  title = {Hierarchical {{Multi-Label Classification Networks}}},
  author = {Wehrmann, J{\^o}natas and Cerri, Ricardo and Barros, Rodrigo C},
  pages = {10},
  abstract = {One of the most challenging machine learning problems is a particular case of data classification in which classes are hierarchically structured and objects can be assigned to multiple paths of the class hierarchy at the same time. This task is known as hierarchical multi-label classification (HMC), with applications in text classification, image annotation, and in bioinformatics problems such as protein function prediction. In this paper, we propose novel neural network architectures for HMC called HMCN, capable of simultaneously optimizing local and global loss functions for discovering local hierarchical class-relationships and global information from the entire class hierarchy while penalizing hierarchical violations. We evaluate its performance in 21 datasets from four distinct domains, and we compare it against the current HMC state-of-the-art approaches. Results show that HMCN substantially outperforms all baselines with statistical significance, arising as the novel state-of-the-art for HMC.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/K5GRKEU5/Wehrmann et al. - Hierarchical Multi-Label Classification Networks.pdf}
}

@article{wen_method_1976,
  title = {Method for Random Vibration of Hysteretic Systems},
  author = {Wen, Yi-Kwei},
  year = {1976},
  journal = {Journal of the engineering mechanics division},
  volume = {102},
  number = {2},
  pages = {249--263},
  publisher = {American Society of Civil Engineers},
  issn = {0044-7951}
}

@inproceedings{wen_sharpness_2023,
  title = {Sharpness {{Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization}}},
  booktitle = {{{NeurIPS}}},
  author = {Wen, Kaiyue and Li, Zhiyuan and Ma, Tengyu},
  year = {2023},
  month = nov,
  urldate = {2023-12-12},
  abstract = {Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization. This work critically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize poorly, and (3) perhaps most strikingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize. Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve better generalization. This calls for the search for other explanations for the generalization of over-parameterized neural networks},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VGSJEIAB/Wen et al_2023_Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve.pdf}
}

@techreport{wesely_dry_1979,
  title = {Dry Deposition and Emission of Small Particles at the Surface of the Earth},
  author = {Wesely, M. L. and Hicks, B. B.},
  year = {1979},
  institution = {Argonne National Lab., IL (USA)},
  urldate = {2017-09-11}
}

@book{weste_cmos_2011,
  title = {{{CMOS VLSI}} Design: A Circuits and Systems Perspective},
  shorttitle = {{{CMOS VLSI}} Design},
  author = {Weste, Neil H. E. and Harris, David Money},
  year = {2011},
  edition = {4th ed},
  publisher = {Addison Wesley},
  address = {Boston},
  isbn = {978-0-321-54774-3},
  lccn = {TK7874 .W45 2011},
  keywords = {{Metal oxide semiconductors, Complementary},Integrated circuits,Very large scale integration Design and construction},
  annotation = {OCLC: ocn473447233},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4NDAA7VN/weste_cmos vlsi_2011.pdf}
}

@article{weston_learning_2003,
  title = {Learning to {{Find Pre-Images}}},
  author = {Weston, Jason and Sch{\"o}lkopf, Bernhard and Bakir, G{\"o}khan H},
  year = {2003},
  journal = {Advances in Neural Information Processing Systems 16 (NIPS)},
  abstract = {We consider the problem of reconstructing patterns from a feature map. Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS. We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images. The introduced technique avoids difficult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/H3MK4REL/Weston et al. - Learning to Find Pre-Images.pdf}
}

@techreport{widrow_adaptive_1960,
  title = {Adaptive Switching Circuits},
  author = {Widrow, Bernard and Hoff, Marcian E},
  year = {1960},
  institution = {Stanford Univ Ca Stanford Electronics Labs}
}

@article{wigren_coupled_,
  title = {Coupled {{Electric Drives Data Set}} and {{Reference Models}}},
  author = {Wigren, Torbj{\"o}rn and Schoukens, Maarten},
  pages = {11},
  abstract = {The following report provides a description of the CE8 coupled electric drives laboratory process. A first set of continuous time model structures that are suitable to describe the nonlinear dynamics are presented. The data sets, which are available in .mat and .csv file formats, are then described in detail. The available data sets are short, which constitute a challenge when performing identification. In support of future work, Wiener models are identified with a recursive algorithm that is parameterized in continuous time. This approach reduces the number of parameters to four for identification of third order dynamics.},
  langid = {english},
  keywords = {ðŸ”No DOI found},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WGGUI7CJ/Wigren and Schoukens - Coupled Electric Drives Data Set and Reference Mod.pdf}
}

@inproceedings{wigren_three_2013,
  title = {Three Free Data Sets for Development and Benchmarking in Nonlinear System Identification},
  booktitle = {2013 {{European Control Conference}} ({{ECC}})},
  author = {Wigren, T. and Schoukens, J.},
  year = {2013},
  month = jul,
  pages = {2933--2938},
  doi = {10.23919/ECC.2013.6669201},
  abstract = {System identification is a fundamentally experimental field of science in that it deals with modeling of system dynamics using measured data. Despite this fact many algorithms and theoretical results are only tested with simulations at the time of publication. One reason for this may be a lack of easily available live data. This paper therefore presents three sets of data, suitable for development, testing and benchmarking of system identification algorithms for nonlinear systems. The data sets are collected from laboratory processes that can be described by block - oriented dynamic models, and by more general nonlinear difference and differential equation models. All data sets are available for free download.},
  keywords = {block-oriented dynamic models,Clocks,differential equation models,differential equations,Heuristic algorithms,identification,Laboratories,Mathematical model,nonlinear control systems,nonlinear difference models,Nonlinear dynamical systems,nonlinear system identification,Silver,system dynamics,system identification benchmarking,system identification development},
  file = {/Users/antoniohortaribeiro/Zotero/storage/R9YJXM89/wigren_three_2013.pdf;/Users/antoniohortaribeiro/Zotero/storage/2UAYXCT9/6669201.html}
}

@inproceedings{wiklicky_nonexistence_1994,
  title = {On the {{Non-Existence}} of a {{Universal Learning Algorithm}} for {{Recurrent Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wiklicky, Herbert},
  editor = {Cowan, J. and Tesauro, G. and Alspector, J.},
  year = {1994},
  volume = {6},
  publisher = {Morgan-Kaufmann},
  urldate = {2021-05-28},
  file = {/Users/antoniohortaribeiro/Zotero/storage/F4XCJDU7/Wiklicky - 1994 - On the Non-Existence of a Universal Learning Algor.pdf;/Users/antoniohortaribeiro/Zotero/storage/S46N2J5Z/a1afc58c6ca9540d057299ec3016d726-Paper.html}
}

@article{willems_diagnostic_1991,
  title = {The Diagnostic Performance of Computer Programs for the Interpretation of Electrocardiograms},
  author = {Willems, J. L. and {Abreu-Lima}, C. and Arnaud, P. and {van Bemmel}, J. H. and Brohet, C. and Degani, R. and Denis, B. and Gehring, J. and Graham, I. and {van Herpen}, G.},
  year = {1991},
  month = dec,
  journal = {The New England Journal of Medicine},
  volume = {325},
  number = {25},
  pages = {1767--1773},
  issn = {0028-4793},
  doi = {10.1056/NEJM199112193252503},
  abstract = {BACKGROUND: Computer programs for the interpretation of electrocardiograms (ECGs) are now widely used. However, a systematic assessment of various computer programs for the interpretation of ECGs has not been performed. METHODS: We undertook a large international study to compare the performance of nine electrocardiographic computer programs with that of eight cardiologists in interpreting ECGs in 1220 clinically validated cases of various cardiac disorders. ECGs from the following groups were included in the sample: control patients (n = 382); patients with left ventricular hypertrophy (n = 183), right ventricular hypertrophy (n = 55), or biventricular hypertrophy (n = 53); patients with anterior myocardial infarction (n = 170), inferior myocardial infarction (n = 273), or combined myocardial infarction (n = 73); and patients with combined infarction and hypertrophy (n = 31). The interpretations of the computer programs and the cardiologists were compared with the clinical diagnoses made independently of the ECGs, and the computer interpretations were compared with those of the cardiologists. RESULTS: The percentage of ECGs correctly classified by the computer programs (median, 91.3 percent) was lower than that of the cardiologists (median, 96.0 percent; P less than 0.01). The median sensitivity of the computer programs was also significantly lower than that of the cardiologists in diagnosing left ventricular hypertrophy (56.6 percent vs. 63.9 percent, P less than 0.02), right ventricular hypertrophy (31.8 percent vs. 46.6 percent, P less than 0.01), anterior myocardial infarction (77.1 percent vs. 84.9 percent, P less than 0.001), and inferior myocardial infarction (58.8 percent vs. 71.7 percent, P less than 0.0001). The median total accuracy level (the percentage of correct classifications) was 6.6 percent lower for the computer programs (69.7 percent) than for the cardiologists (76.3 percent; P less than 0.001). However, the performance of the best programs nearly matched that of the most accurate cardiologists. CONCLUSIONS: Our study shows that some but not all computer programs for the interpretation of ECGs perform almost as well as cardiologists in identifying seven major cardiac disorders.},
  langid = {english},
  pmid = {1834940},
  keywords = {{Diagnosis, Computer-Assisted},Cardiology,Cardiomegaly,Electrocardiography,Evaluation Studies as Topic,Humans,Myocardial Infarction,Sensitivity and Specificity,Software}
}

@article{willems_testing_1987,
  title = {Testing the Performance of {{ECG}} Computer Programs: The {{CSE}} Diagnostic Pilot Study},
  shorttitle = {Testing the Performance of {{ECG}} Computer Programs},
  author = {Willems, J. L. and {Abreu-Lima}, C. and Arnaud, P. and {van Bemmel}, J. H. and Brohet, C. and Degani, R. and Denis, B. and Graham, I. and {van Herpen}, G. and Macfarlane, P. W.},
  year = {1987},
  month = oct,
  journal = {Journal of Electrocardiology},
  volume = {20 Suppl},
  pages = {73--77},
  issn = {0022-0736},
  abstract = {In an international project investigators from 21 institutes are trying to establish a common reference library and evaluation methods for testing the diagnostic performance of various ECG computer programs using ECG independent clinical information. Preliminary results indicate that the classification accuracy of different programs varies widely.},
  langid = {english},
  keywords = {{Signal Processing, Computer-Assisted},Electrocardiography,Humans,Information Systems,Pilot Projects,Software,Software Validation}
}

@article{williams_experimental_1989,
  title = {Experimental Analysis of the Real-Time Recurrent Learning Algorithm},
  author = {Williams, Ronald J. and Zipser, David},
  year = {1989},
  journal = {Connection Science},
  volume = {1},
  number = {1},
  pages = {87--111},
  doi = {10.1080/09540098908915631},
  urldate = {2017-09-10},
  annotation = {00369},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GDIDQF8H/williams_experiment_1989.pdf}
}

@inproceedings{williams_using_2001,
  title = {Using the Nystr{\"o}m Method to Speed up Kernel Machines},
  booktitle = {Advances in Neural Information Processing Systems 13 ({{NIPS}} 2000)},
  author = {Williams, Christopher K. I. and Seeger, Matthias},
  editor = {Leen, T.K. and Dietterich, T.G. and Tresp, V.},
  year = {2001},
  pages = {682--688},
  publisher = {MIT Press},
  abstract = {A major problem for kernel-based predictors (such as Support Vector Machines and Gaussian processes) is that the amount of computation required to find the solution scales as O(n ), where n is the number of training examples. We show that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystr{\"o}m method (which is used for the numerical solution of eigenproblems). This is achieved by carrying out an eigendecomposition on a smaller system of size m {\textexclamdown} n, and then expanding the results back up to n dimensions. The computational complexity of a predictor using this approximation is O(m n). We report experiments on the USPS and abalone data sets and show that we can set m n without any significant decrease in the accuracy of the solution.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VLETSJQ2/nystroem.pdf}
}

@article{williams_wavelet_2018,
  title = {{{WAVELET POOLING FOR CONVOLUTIONAL NEURAL NETWORKS}}},
  author = {Williams, Travis and Li, Robert},
  year = {2018},
  pages = {12},
  abstract = {Convolutional Neural Networks continuously advance the progress of 2D and 3D image and object classification. The steadfast usage of this algorithm requires constant evaluation and upgrading of foundational concepts to maintain progress. Network regularization techniques typically focus on convolutional layer operations, while leaving pooling layer operations without suitable options. We introduce Wavelet Pooling as another alternative to traditional neighborhood pooling. This method decomposes features into a second level decomposition, and discards the first-level subbands to reduce feature dimensions. This method addresses the overfitting problem encountered by max pooling, while reducing features in a more structurally compact manner than pooling via neighborhood regions. Experimental results on four benchmark classification datasets demonstrate our proposed method outperforms or performs comparatively with methods like max, mean, mixed, and stochastic pooling.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Z46H5RPP/Williams and Li - 2018 - WAVELET POOLING FOR CONVOLUTIONAL NEURAL NETWORKS.pdf}
}

@article{wills_stochastic_2018,
  title = {Stochastic Quasi-{{Newton}} with Adaptive Step Lengths for Large-Scale Problems},
  author = {Wills, Adrian and Sch{\"o}n, Thomas},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.04310 [cs, stat]},
  eprint = {1802.04310},
  primaryclass = {cs, stat},
  abstract = {We provide a numerically robust and fast method capable of exploiting the local geometry when solving large-scale stochastic optimisation problems. Our key innovation is an auxiliary variable construction coupled with an inverse Hessian approximation computed using a receding history of iterates and gradients. It is the Markov chain nature of the classic stochastic gradient algorithm that enables this development. The construction offers a mechanism for stochastic line search adapting the step length. We numerically evaluate and compare against current state-of-the-art with encouraging performance on real-world benchmark problems where the number of observations and unknowns is in the order of millions.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Learning,Statistics - Machine Learning},
  annotation = {00001},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q3JG6XGR/wills_stochastic_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/VKP8PENJ/1802.html}
}

@article{wilson_multidecadal_2017,
  title = {Multi-Decadal Time Series of Remotely Sensed Vegetation Improves Prediction of Soil Carbon in a Subtropical Grassland},
  author = {Wilson, Chris H. and Caughlin, T. Trevor and Rifai, Sami W. and Boughton, Elizabeth H. and Mack, Michelle C. and Flory, S. Luke},
  year = {2017},
  month = jul,
  journal = {Ecological Applications},
  volume = {27},
  number = {5},
  pages = {1646--1656},
  issn = {1939-5582},
  doi = {10.1002/eap.1557},
  abstract = {Soil carbon sequestration in agroecosystems could play a key role in climate change mitigation but will require accurate predictions of soil organic carbon (SOC) stocks over spatial scales relevant to land management. Spatial variation in underlying drivers of SOC, such as plant productivity and soil mineralogy, complicates these predictions. Recent advances in the availability of remotely sensed data make it practical to generate multidecadal time series of vegetation indices with high spatial resolution and coverage. However, the utility of such data largely is unknown, only having been tested with shorter (e.g., 1--2~yr) data summaries. Across a 2,000~ha subtropical grassland, we found that a long time series (28~yr) of a vegetation index (Enhanced Vegetation Index; EVI) derived from the Landsat 5 satellite significantly enhanced prediction of spatially varying SOC pools, while a short summary (2~yr) was an ineffective predictor. EVI was the best predictor for surface SOC (0--5~cm depth) and total measured SOC stocks (0--15~cm). The optimum models for SOC in the upper soil layer combined EVI records with elevation and calcium concentration, while deeper SOC was more strongly associated with calcium availability. We demonstrate how data from the open access Landsat archive can predict SOC stocks, a key ecosystem metric, and illustrate the rich variety of analytical approaches that can be applied to long time series of remotely sensed greenness. Overall, our results showed that SOC pools were closely coupled to EVI in this ecosystem, demonstrating that maintenance of higher average green leaf area is correlated with higher SOC. The strong associations of vegetation greenness and calcium concentration with SOC suggest that the ability to sequester additional SOC likely will rely on strategic management of pasture vegetation and soil fertility.},
  langid = {english},
  keywords = {enhanced vegetation index,Google Earth Engine,Landsat time series,remote sensing,soil carbon sequestration,soil organic carbon,subtropical grasslands},
  annotation = {00007},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QHZJRCUD/wilson_multi-deca_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/DQ2I95NH/abstract.html}
}

@article{winkler_association_2019,
  title = {Association {{Between Surgical Skin Markings}} in {{Dermoscopic Images}} and {{Diagnostic Performance}} of a {{Deep Learning Convolutional Neural Network}} for {{Melanoma Recognition}}},
  author = {Winkler, Julia K. and Fink, Christine and Toberer, Ferdinand and Enk, Alexander and Deinlein, Teresa and {Hofmann-Wellenhof}, Rainer and Thomas, Luc and Lallas, Aimilios and Blum, Andreas and Stolz, Wilhelm and Haenssle, Holger A.},
  year = {2019},
  month = aug,
  journal = {JAMA Dermatology},
  doi = {10/gf6894},
  urldate = {2019-08-30},
  abstract = {{$<$}h3{$>$}Importance{$<$}/h3{$><$}p{$>$}Deep learning convolutional neural networks (CNNs) have shown a performance at the level of dermatologists in the diagnosis of melanoma. Accordingly, further exploring the potential limitations of CNN technology before broadly applying it is of special interest.{$<$}/p{$><$}h3{$>$}Objective{$<$}/h3{$><$}p{$>$}To investigate the association between gentian violet surgical skin markings in dermoscopic images and the diagnostic performance of a CNN approved for use as a medical device in the European market.{$<$}/p{$><$}h3{$>$}Design and Setting{$<$}/h3{$><$}p{$>$}A cross-sectional analysis was conducted from August 1, 2018, to November 30, 2018, using a CNN architecture trained with more than 120 000 dermoscopic images of skin neoplasms and corresponding diagnoses. The association of gentian violet skin markings in dermoscopic images with the performance of the CNN was investigated in 3 image sets of 130 melanocytic lesions each (107 benign nevi, 23 melanomas).{$<$}/p{$><$}h3{$>$}Exposures{$<$}/h3{$><$}p{$>$}The same lesions were sequentially imaged with and without the application of a gentian violet surgical skin marker and then evaluated by the CNN for their probability of being a melanoma. In addition, the markings were removed by manually cropping the dermoscopic images to focus on the melanocytic lesion.{$<$}/p{$><$}h3{$>$}Main Outcomes and Measures{$<$}/h3{$><$}p{$>$}Sensitivity, specificity, and area under the curve (AUC) of the receiver operating characteristic (ROC) curve for the CNN's diagnostic classification in unmarked, marked, and cropped images.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}In all, 130 melanocytic lesions (107 benign nevi and 23 melanomas) were imaged. In unmarked lesions, the CNN achieved a sensitivity of 95.7\% (95\% CI, 79\%-99.2\%) and a specificity of 84.1\% (95\% CI, 76.0\%-89.8\%). The ROC AUC was 0.969. In marked lesions, an increase in melanoma probability scores was observed that resulted in a sensitivity of 100\% (95\% CI, 85.7\%-100\%) and a significantly reduced specificity of 45.8\% (95\% CI, 36.7\%-55.2\%,\emph{P} \&lt; .001). The ROC AUC was 0.922. Cropping images led to the highest sensitivity of 100\% (95\% CI, 85.7\%-100\%), specificity of 97.2\% (95\% CI, 92.1\%-99.0\%), and ROC AUC of 0.993. Heat maps created by vanilla gradient descent backpropagation indicated that the blue markings were associated with the increased false-positive rate.{$<$}/p{$><$}h3{$>$}Conclusions and Relevance{$<$}/h3{$><$}p{$>$}This study's findings suggest that skin markings significantly interfered with the CNN's correct diagnosis of nevi by increasing the melanoma probability scores and consequently the false-positive rate. A predominance of skin markings in melanoma training images may have induced the CNN's association of markings with a melanoma diagnosis. Accordingly, these findings suggest that skin markings should be avoided in dermoscopic images intended for analysis by a CNN.{$<$}/p{$><$}h3{$>$}Trial Registration{$<$}/h3{$><$}p{$>$}German Clinical Trial Register (DRKS) Identifier:DRKS00013570{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7XX8333X/winkler_associatio_2019.pdf;/Users/antoniohortaribeiro/Zotero/storage/7SXKEU57/2740808.html}
}

@incollection{wisdom_fullcapacity_2016,
  title = {Full-{{Capacity Unitary Recurrent Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Wisdom, Scott and Powers, Thomas and Hershey, John and Le Roux, Jonathan and Atlas, Les},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {4880--4888},
  publisher = {Curran Associates, Inc.},
  urldate = {2019-09-19},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9M6X98FX/wisdom_full-capac_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/SGJSKL3F/6327-full-capacity-unitary-recurrent-neural-networks.html}
}

@article{wolfe_convergence_1969,
  title = {Convergence Conditions for Ascent Methods},
  author = {Wolfe, Philip},
  year = {1969},
  journal = {SIAM review},
  volume = {11},
  number = {2},
  pages = {226--235},
  keywords = {â“Multiple DOI},
  annotation = {00926}
}

@article{wong_lasso_2016,
  title = {Lasso {{Guarantees}} for {{Time Series Estimation Under Subgaussian Tails}} and \$ {\textbackslash}beta \$-{{Mixing}}},
  author = {Wong, Kam Chung and Li, Zifan and Tewari, Ambuj},
  year = {2016},
  month = feb,
  journal = {arXiv:1602.04265 [cs, stat]},
  eprint = {1602.04265},
  primaryclass = {cs, stat},
  abstract = {Many theoretical results on estimation of high dimensional time series require specifying an underlying data generating model (DGM). Instead, along the footsteps of{\textasciitilde}{\textbackslash}cite\{wong2017lasso\}, this paper relies only on (strict) stationarity and \$ {\textbackslash}beta \$-mixing condition to establish consistency of lasso when data comes from a \${\textbackslash}beta\$-mixing process with marginals having subgaussian tails. Because of the general assumptions, the data can come from DGMs different than standard time series models such as VAR or ARCH. When the true DGM is not VAR, the lasso estimates correspond to those of the best linear predictors using the past observations. We establish non-asymptotic inequalities for estimation and prediction errors of the lasso estimates. Together with{\textasciitilde}{\textbackslash}cite\{wong2017lasso\}, we provide lasso guarantees that cover full spectrum of the parameters in specifications of \$ {\textbackslash}beta \$-mixing subgaussian time series. Applications of these results potentially extend to non-Gaussian, non-Markovian and non-linear times series models as the examples we provide demonstrate. In order to prove our results, we derive a novel Hanson-Wright type concentration inequality for \${\textbackslash}beta\$-mixing subgaussian random vectors that may be of independent interest.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/M4GJQEFQ/wong_lasso_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/BIGJ7IUM/1602.html}
}

@article{wong_learning_2020,
  title = {Learning Perturbation Sets for Robust Machine Learning},
  author = {Wong, Eric and Kolter, J. Zico},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.08450 [cs, stat]},
  eprint = {2007.08450},
  primaryclass = {cs, stat},
  urldate = {2020-07-27},
  abstract = {Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline digit transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to learn models which have improved generalization performance and are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at https://github.com/locuslab/perturbation\_learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IZGSMRUB/Wong and Kolter - 2020 - Learning perturbation sets for robust machine lear.pdf;/Users/antoniohortaribeiro/Zotero/storage/K4XUBBAY/2007.html}
}

@article{worldhealthorganization_chagas_2015,
  title = {Chagas Disease in {{Latin America}}: An Epidemiological Update Based on 2010 Estimates},
  author = {{World Health Organization}},
  year = {2015},
  journal = {Weekly Epidemiological Record= Relev{\'e} {\'e}pid{\'e}miologique hebdomadaire},
  volume = {90},
  number = {06},
  pages = {33--44}
}

@book{worldhealthorganization_global_2014,
  title = {Global Status Report on Noncommunicable Diseases 2014: Attaining the Nine Global Noncommunicable Diseases Targets; a Shared Responsibility.},
  shorttitle = {Global Status Report on Noncommunicable Diseases 2014},
  author = {{World Health Organization}},
  year = {2014},
  publisher = {World Health Organization},
  address = {Geneva},
  isbn = {978-92-4-156485-4},
  langid = {english},
  annotation = {OCLC: 907517003},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DV64A8RF/Organisation mondiale de la santÃ© - 2014 - Global status report on noncommunicable diseases 2.pdf}
}

@article{wray_basic_2021,
  title = {From Basic Science to Clinical Application of Polygenic Risk Scores: A Primer},
  author = {Wray, Naomi R and Lin, Tian and Austin, Jehannine and McGrath, John J and Hickie, Ian B and Murray, Graham K and Visscher, Peter M},
  year = {2021},
  journal = {JAMA psychiatry},
  volume = {78},
  number = {1},
  pages = {101--109},
  publisher = {American Medical Association},
  issn = {2168-622X}
}

@article{wright_direct_1996,
  title = {Direct Search Methods: {{Once}} Scorned, Now Respectable},
  author = {Wright, Margaret H},
  year = {1996},
  journal = {Pitman Research Notes in Mathematics Series},
  pages = {191--208},
  issn = {0269-3674},
  keywords = {ðŸ”No DOI found},
  annotation = {00398}
}

@article{wu_coordinate_2008,
  title = {Coordinate {{Descent Algorithms}} for {{Lasso Penalized Regression}}},
  author = {Wu, Tong Tong and Lange, Kenneth},
  year = {2008},
  journal = {The Annals of Applied Statistics},
  volume = {2},
  number = {1},
  eprint = {30244184},
  eprinttype = {jstor},
  pages = {224--244},
  issn = {1932-6157},
  doi = {10.1214/07-AOAS147},
  abstract = {Imposition of a lasso penalty shrinks parameter estimates toward zero and performs continuous model selection. Lasso penalized regression is capable of handling linear regression problems where the number of predictors far exceeds the number of cases. This paper tests two exceptionally fast algorithms for estimating regression coefficients with a lasso penalty. The previously known {$\ell_2$} algorithm is based on cyclic coordinate descent. Our new {$\ell_1$} algorithm is based on greedy coordinate descent and Edgeworth's algorithm for ordinary {$\ell_1$} regression. Each algorithm relies on a tuning constant that can be chosen by cross-validation. In some regression problems it is natural to group parameters and penalize parameters group by group rather than separately. If the group penalty is proportional to the Euclidean norm of the parameters of the group, then it is possible to majorize the norm and reduce parameter estimation to {$\ell_2$} regression with a lasso penalty. Thus, the existing algorithm can be extended to novel settings. Each of the algorithms discussed is tested via either simulated or real data or both. The Appendix proves that a greedy form of the {$\ell_2$} algorithm converges to the minimum value of the objective function.},
  annotation = {00622},
  file = {/Users/antoniohortaribeiro/Zotero/storage/HQG69ISR/wu_coordinate_2008.pdf}
}

@incollection{wu_how_2018,
  title = {How {{SGD Selects}} the {{Global Minima}} in {{Over-parameterized Learning}}: {{A Dynamical Stability Perspective}}},
  shorttitle = {How {{SGD Selects}} the {{Global Minima}} in {{Over-parameterized Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Wu, Lei and Ma, Chao and E, Weinan},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {8289--8298},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-13},
  file = {/Users/antoniohortaribeiro/Zotero/storage/H7YESUF7/wu_how sgd_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/SR9K8IFZ/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-per.html}
}

@article{wu_learning_2019,
  title = {Learning a {{Compressed Sensing Measurement Matrix}} via {{Gradient Unrolling}}},
  author = {Wu, Shanshan and Dimakis, Alexandros G. and Sanghavi, Sujay and Yu, Felix X. and {Holtmann-Rice}, Daniel and Storcheus, Dmitry and Rostamizadeh, Afshin and Kumar, Sanjiv},
  year = {2019},
  month = jul,
  journal = {arXiv:1806.10175 [cs, math, stat]},
  eprint = {1806.10175},
  primaryclass = {cs, math, stat},
  urldate = {2020-07-20},
  abstract = {Linear encoding of sparse vectors is widely popular, but is commonly data-independent -- missing any possible extra (but a priori unknown) structure beyond sparsity. In this paper we present a new method to learn linear encoders that adapt to data, while still performing well with the widely used \${\textbackslash}ell\_1\$ decoder. The convex \${\textbackslash}ell\_1\$ decoder prevents gradient propagation as needed in standard gradient-based training. Our method is based on the insight that unrolling the convex decoder into \$T\$ projected subgradient steps can address this issue. Our method can be seen as a data-driven way to learn a compressed sensing measurement matrix. We compare the empirical performance of 10 algorithms over 6 sparse datasets (3 synthetic and 3 real). Our experiments show that there is indeed additional structure beyond sparsity in the real datasets; our method is able to discover it and exploit it to create excellent reconstructions with fewer measurements (by a factor of 1.1-3x) compared to the previous state-of-the-art methods. We illustrate an application of our method in learning label embeddings for extreme multi-label classification, and empirically show that our method is able to match or outperform the precision scores of SLEEC, which is one of the state-of-the-art embedding-based approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GF75FTTR/Wu et al. - 2019 - Learning a Compressed Sensing Measurement Matrix v.pdf;/Users/antoniohortaribeiro/Zotero/storage/4DDKNGVW/1806.html}
}

@article{wu_optical_2007,
  title = {Optical Imaging for Medical Diagnosis Based on Active Stereo Vision and Motion Tracking},
  author = {Wu, Tao T and Qu, Jianan Y},
  year = {2007},
  journal = {Optics express},
  volume = {15},
  number = {16},
  pages = {10421--10426},
  doi = {10.1364/OE.15.010421},
  annotation = {00025}
}

@article{xia_internet_2012,
  title = {Internet of Things},
  author = {Xia, Feng and Yang, Laurence T and Wang, Lizhe and Vinel, Alexey},
  year = {2012},
  journal = {International journal of communication systems},
  volume = {25},
  number = {9},
  pages = {1101},
  issn = {1074-5351}
}

@inproceedings{xie_adversarial_2020,
  title = {Adversarial Examples Improve Image Recognition},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan L. and Le, Quoc V.},
  year = {2020},
  month = jun,
  file = {/Users/antoniohortaribeiro/Zotero/storage/VUPW4TPX/Xie et al. - Adversarial Examples Improve Image Recognition.pdf}
}

@article{xie_highdimensional_2024,
  title = {High-Dimensional ({{Group}}) {{Adversarial Training}} in {{Linear Regression}}},
  author = {Xie, Yiling and Huo, Xiaoming},
  year = {2024},
  month = may,
  eprint = {2405.13940},
  urldate = {2024-07-15},
  abstract = {Adversarial training can achieve robustness against adversarial perturbations and has been widely used in machine learning models. This paper delivers a non-asymptotic consistency analysis of the adversarial training procedure under {$\ell\infty$}-perturbation in high-dimensional linear regression. It will be shown that the associated convergence rate of prediction error can achieve the minimax rate up to a logarithmic factor in the high-dimensional linear regression on the class of sparse parameters. Additionally, the group adversarial training procedure is analyzed. Compared with classic adversarial training, it will be proved that the group adversarial training procedure enjoys a better prediction error upper bound under certain group-sparsity patterns.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory},
  file = {/Users/antoniohortaribeiro/Zotero/storage/I7QGRCZK/Xie and Huo - 2024 - High-dimensional (Group) Adversarial Training in L.pdf}
}

@article{xie_identification_2013,
  title = {Identification of Nonlinear Hysteretic Systems by Artificial Neural Network},
  author = {Xie, S.L. and Zhang, Y.H. and Chen, C.H. and Zhang, X.N.},
  year = {2013},
  month = jan,
  journal = {Mechanical Systems and Signal Processing},
  volume = {34},
  number = {1},
  pages = {76--87},
  issn = {0888-3270},
  doi = {10/f4kjsq},
  abstract = {An identification method is developed for nonlinear hysteretic systems by use of artificial neural network in the paper. Employing the Bouc--Wen differential model widely used for memory-type nonlinear hysteretic systems, the approach sets up a Bouc--Wen model-based neural network. The weights of the designed specifically network correspond to the Bouc--Wen model parameters and are thus physical ones. Taking advantage of powerful function approximation capability of neural network, the nonlinear hysteretic systems can be identified with the proposed approach by network training. The identification scheme is validated by a simulated case and thereafter applied to modeling of a wire cable vibration isolation experimental system. The results show that the presented identification method can identify the nonlinear hysteretic systems with high accuracy.},
  keywords = {Bouc--Wen model,Identification,Neural network,Nonlinear hysteretic system,Wire cable isolator}
}

@inproceedings{xie_selftraining_2020,
  title = {Self-{{Training With Noisy Student Improves ImageNet Classification}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
  year = {2020},
  month = jun,
  pages = {10684--10695},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.01070},
  urldate = {2020-10-16},
  abstract = {We present a simple self-training method that achieves 88.4\% top-1 accuracy on ImageNet, which is 2.0\% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0\% to 83.7\%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/45E7A7BK/Xie et al. - 2020 - Self-Training With Noisy Student Improves ImageNet.pdf}
}

@article{xie_smooth_2020,
  title = {Smooth {{Adversarial Training}}},
  author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Yuille, Alan and Le, Quoc V.},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.14536 [cs]},
  eprint = {2006.14536},
  primaryclass = {cs},
  urldate = {2020-07-07},
  abstract = {It is commonly believed that networks cannot be both accurate and robust, that gaining robustness means losing accuracy. It is also generally believed that, unless making networks larger, network architectural elements would otherwise matter little in improving adversarial robustness. Here we present evidence to challenge these common beliefs by a careful study about adversarial training. Our key observation is that the widely-used ReLU activation function significantly weakens adversarial training due to its non-smooth nature. Hence we propose smooth adversarial training (SAT), in which we replace ReLU with its smooth approximations to strengthen adversarial training. The purpose of smooth activation functions in SAT is to allow it to find harder adversarial examples and compute better gradient updates during adversarial training. Compared to standard adversarial training, SAT improves adversarial robustness for "free", i.e., no drop in accuracy and no increase in computational cost. For example, without introducing additional computations, SAT significantly enhances ResNet-50's robustness from 33.0\% to 42.3\%, while also improving accuracy by 0.9\% on ImageNet. SAT also works well with larger networks: it helps EfficientNet-L1 to achieve 82.2\% accuracy and 58.6\% robustness on ImageNet, outperforming the previous state-of-the-art defense by 9.5\% for accuracy and 11.6\% for robustness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/antoniohortaribeiro/Zotero/storage/TPPTQ3PB/Xie et al. - 2020 - Smooth Adversarial Training.pdf;/Users/antoniohortaribeiro/Zotero/storage/N9G8XWGX/2006.html}
}

@article{xie_squareroot_2018,
  title = {Square-{{Root LASSO}} for {{High-Dimensional Sparse Linear Systems}} with {{Weakly Dependent Errors}}: {{Square-root LASSO}} with Time Series Errors},
  shorttitle = {Square-{{Root LASSO}} for {{High-Dimensional Sparse Linear Systems}} with {{Weakly Dependent Errors}}},
  author = {Xie, Fang and Xiao, Zhijie},
  year = {2018},
  month = mar,
  journal = {Journal of Time Series Analysis},
  volume = {39},
  number = {2},
  pages = {212--238},
  issn = {01439782},
  doi = {10.1111/jtsa.12278},
  urldate = {2018-02-22},
  langid = {english},
  annotation = {00000},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3M6STJVA/xie_square-roo_2018.pdf}
}

@inproceedings{xing_adversarially_2021,
  title = {Adversarially Robust Estimate and Risk Analysis in Linear Regression},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Xing, Yue and Zhang, Ruizhi and Cheng, Guang},
  editor = {Banerjee, Arindam and Fukumizu, Kenji},
  year = {2021-04-13/2021-04-15},
  volume = {130},
  pages = {514--522},
  abstract = {Adversarial robust learning aims to design algorithms that are robust to small adversarial perturbations on input variables. Beyond the existing studies on the predictive performance to adversarial samples, our goal is to understand statistical properties of adversarial robust estimates and analyze adversarial risk in the setup of linear regression models. By discovering the statistical minimax rate of convergence of adversarial robust estimators, we emphasize the importance of incorporating model information, e.g., sparsity, in adversarial robust learning. Further, we reveal an explicit connection of adversarial and standard estimates, and propose a straightforward two-stage adversarial training framework, which facilitates to utilize model structure information to improve adversarial robustness. In theory, the consistency of the adversarial robust estimator is proven and its Bahadur representation is also developed for the statistical inference purpose. The proposed estimator converges in a sharp rate under either low-dimensional or sparse scenario. Moreover, our theory confirms two phenomena in adversarial robust learning: adversarial robustness hurts generalization, and unlabeled data help improve the generalization. In the end, we conduct numerical simulations to verify our theory.},
  pdf = {http://proceedings.mlr.press/v130/xing21c/xing21c.pdf},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ALAJIDH2/Xing et al. - 2021 - Adversarially robust estimate and risk analysis in.pdf}
}

@inproceedings{xing_generalization_2021,
  title = {On the {{Generalization Properties}} of {{Adversarial Training}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Xing, Yue and Song, Qifan and Cheng, Guang},
  year = {2021},
  month = mar,
  pages = {505--513},
  issn = {2640-3498},
  urldate = {2022-08-15},
  abstract = {Modern machine learning and deep learning models are shown to be vulnerable when testing data are slightly perturbed. Theoretical studies of adversarial training algorithms mostly focus on their adversarial training losses or local convergence properties. In contrast, this paper studies the generalization performance of a generic adversarial training algorithm. Specifically, we consider linear regression models and two-layer neural networks (with lazy training) using squared loss under low-dimensional regime and high-dimensional regime. In the former regime, after overcoming the non-smoothness of adversarial training, the adversarial risk of the trained models will converge to the minimal adversarial risk. In the latter regime, we discover that data interpolation prevents the adversarial robust estimator from being consistent (i.e. converge in probability). Therefore, inspired by successes of the least absolute shrinkage and selection operator (LASSO), we incorporate the îˆ¸1L1{\textbackslash}mathcal\{L\}\_1 penalty in the high dimensional adversarial learning, and show that it leads to consistent adversarial robust estimation. A series of numerical studies are conducted to demonstrate that how the smoothness and îˆ¸1L1{\textbackslash}mathcal\{L\}\_1 penalization help to improve the adversarial robustness of DNN models.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/54Q4XQ3K/Xing et al. - 2021 - On the Generalization Properties of Adversarial Tr.pdf;/Users/antoniohortaribeiro/Zotero/storage/LQK3BMMR/Xing et al. - 2021 - On the Generalization Properties of Adversarial Tr.pdf}
}

@article{xu_robust_2008,
  title = {Robust Regression and Lasso},
  author = {Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  year = {2008},
  journal = {Advances in Neural Information Processing Systems},
  volume = {21},
  file = {/Users/antoniohortaribeiro/Zotero/storage/9E7L7F4L/Xu et al. - 2008 - Robust regression and lasso.pdf}
}

@article{xu_robustness_2009,
  title = {Robustness and Regularization of Support Vector Machines},
  author = {Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  year = {2009},
  journal = {Journal of Machine Learning Research},
  volume = {10},
  number = {51},
  pages = {1485--1510}
}

@article{xu_robustness_2012,
  title = {Robustness and Generalization},
  author = {Xu, Huan and Mannor, Shie},
  year = {2012},
  month = mar,
  journal = {Machine Learning},
  volume = {86},
  number = {3},
  pages = {391--423},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-011-5268-1},
  urldate = {2020-07-14},
  abstract = {We derive generalization bounds for learning algorithms based on their robustness: the property that if a testing sample is ``similar'' to a training sample, then the testing error is close to the training error. This provides a novel approach, different from complexity or stability arguments, to study generalization of learning algorithms. One advantage of the robustness approach, compared to previous methods, is the geometric intuition it conveys. Consequently, robustness-based analysis is easy to extend to learning in non-standard setups such as Markovian samples or quantile loss. We further show that a weak notion of robustness is both sufficient and necessary for generalizability, which implies that robustness is a fundamental property that is required for learning algorithms to work.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/WEVI47J7/Xu and Mannor - 2012 - Robustness and generalization.pdf}
}

@article{yan_narmax_2015,
  title = {{{NARMAX}} Model Identification Using a Set-Theoretic Evolutionary Approach},
  author = {Yan, Jinyao and Deller, {\relax JR}},
  year = {2015},
  journal = {Signal Processing},
  keywords = {ðŸ”No DOI found},
  annotation = {00009}
}

@article{yang_feedback_2013,
  title = {Feedback {{Particle Filter}}},
  author = {Yang, T. and Mehta, P. G. and Meyn, S. P.},
  year = {2013},
  month = oct,
  journal = {IEEE Transactions on Automatic Control},
  volume = {58},
  number = {10},
  pages = {2465--2480},
  issn = {0018-9286},
  doi = {10/f5bvxs},
  abstract = {The feedback particle filter introduced in this paper is a new approach to approximate nonlinear filtering, motivated by techniques from mean-field game theory. The filter is defined by an ensemble of controlled stochastic systems (the particles). Each particle evolves under feedback control based on its own state, and features of the empirical distribution of the ensemble. The feedback control law is obtained as the solution to an optimal control problem, in which the optimization criterion is the Kullback-Leibler divergence between the actual posterior, and the common posterior of any particle. The following conclusions are obtained for diffusions with continuous observations: 1) The optimal control solution is exact: The two posteriors match exactly, provided they are initialized with identical priors. 2) The optimal filter admits an innovation error-based gain feedback structure. 3) The optimal feedback gain is obtained via a solution of an Euler-Lagrange boundary value problem; the feedback gain equals the Kalman gain in the linear Gaussian case. Numerical algorithms are introduced and implemented in two general examples, and a neuroscience application involving coupled oscillators. In some cases it is found that the filter exhibits significantly lower variance when compared to the bootstrap particle filter.},
  keywords = {Approximation methods,boundary-value problems,empirical ensemble distribution,Equations,error-based gain feedback structure,Euler-Lagrange boundary value problem,feedback,feedback control law,feedback particle filter,game theory,Gaussian processes,Kalman filters,Kalman gain,Kullback- Leibler divergence,linear Gaussian,Mathematical model,mean field game theory,Mean-field games,nonlinear filtering,nonlinear filters,numerical algorithm,optimal control,Optimal control,optimal control problem,optimal feedback gain,optimal transportation,optimisation,optimization criterion,particle filtering,particle filtering (numerical methods),Particle filters,stochastic control system,stochastic systems,Technological innovation},
  file = {/Users/antoniohortaribeiro/Zotero/storage/2MUFLVIQ/yang_feedback_2013.pdf;/Users/antoniohortaribeiro/Zotero/storage/NNY7JBAF/6530707.html}
}

@article{yang_nystrom_2012,
  title = {Nystr{\"o}m {{Method}} vs {{Random Fourier Features}}: {{A Theoretical}} and {{Empirical Comparison}}},
  shorttitle = {Nystr{\"o}m {{Method}} vs {{Random Fourier Features}}},
  author = {Yang, Tianbao and Li, Yu-feng and Mahdavi, Mehrdad and Jin, Rong and Zhou, Zhi-Hua},
  year = {2012},
  journal = {Advances in Neural Information Processing Systems},
  volume = {25},
  pages = {476--484},
  urldate = {2021-01-29},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PJQCFJEN/Yang et al. - 2012 - NystrÃ¶m Method vs Random Fourier Features A Theor.pdf}
}

@misc{yang_tensor_2020,
  title = {Tensor {{Programs II}}: {{Neural Tangent Kernel}} for {{Any Architecture}}},
  shorttitle = {Tensor {{Programs II}}},
  author = {Yang, Greg},
  year = {2020},
  month = nov,
  number = {arXiv:2006.14548},
  eprint = {2006.14548},
  primaryclass = {cond-mat, stat},
  publisher = {arXiv},
  urldate = {2022-07-27},
  abstract = {We prove that a randomly initialized neural network of any architecture has its Tangent Kernel (NTK) converge to a deterministic limit, as the network widths tend to infinity. We demonstrate how to calculate this limit. In prior literature, the heuristic study of neural network gradients often assumes every weight matrix used in forward propagation is independent from its transpose used in backpropagation [58]. This is known as the gradient independence assumption (GIA). We identify a commonly satisfied condition, which we call Simple GIA Check, such that the NTK limit calculation based on GIA is correct. Conversely, when Simple GIA Check fails, we show GIA can result in wrong answers. Our material here presents the NTK results of Yang [63] in a friendly manner and showcases the tensor programs technique for understanding wide neural networks. We provide reference implementations of infinite-width NTKs of recurrent neural network, transformer, and batch normalization at https://github.com/thegregyang/NTK4A.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/6SHAXAWG/Yang - 2020 - Tensor Programs II Neural Tangent Kernel for Any .pdf}
}

@inproceedings{yang_wide_2019,
  title = {Wide {{Feedforward}} or {{Recurrent Neural Networks}} of {{Any Architecture}} Are {{Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yang, Greg},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-07-26},
  abstract = {Wide neural networks with random weights and biases are Gaussian processes, as observed by Neal (1995) for shallow networks, and more recently by Lee et al.{\textasciitilde}(2018) and Matthews et al.{\textasciitilde}(2018) for deep fully-connected networks, as well as by Novak et al.{\textasciitilde}(2019) and Garriga-Alonso et al.{\textasciitilde}(2019) for deep convolutional networks. We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph) convolution, pooling, skip connection, attention, batch normalization, and/or layer normalization. More generally, we introduce a language for expressing neural network computations, and our result encompasses all such expressible neural networks. This work serves as a tutorial on the {\textbackslash}emph\{tensor programs\} technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there. We provide open-source implementations of the Gaussian Process kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at  github.com/thegregyang/GP4A. Please see our arxiv version for the complete and up-to-date version of this paper.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7JE76TWM/Yang - 2019 - Wide Feedforward or Recurrent Neural Networks of A.pdf}
}

@book{yansongwang_discrete_2011,
  title = {Discrete {{Wavelet Transfom}} for {{Nonstationary Signal Processing}}.},
  author = {{Yansong Wang} and {Gongqi Shen} and {Qiang Zhu} and {Weiwei Wu}},
  year = {2011},
  publisher = {INTECH Open Access Publisher},
  isbn = {978-953-307-185-5},
  langid = {english},
  annotation = {00006 \\
OCLC: 884041491},
  file = {/Users/antoniohortaribeiro/Zotero/storage/X8DUR5FG/yansong wang_discrete_2011.pdf}
}

@article{yeh_knowledge_2009,
  title = {Knowledge Discovery on {{RFM}} Model Using {{Bernoulli}} Sequence},
  author = {Yeh, I-Cheng and Yang, King-Jang and Ting, Tao-Ming},
  year = {2009},
  month = apr,
  journal = {Expert Systems with Applications},
  volume = {36},
  number = {3, Part 2},
  pages = {5866--5871},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2008.07.018},
  urldate = {2024-05-21},
  abstract = {The objective of this paper is to introduce a comprehensive methodology to discover the knowledge for selecting targets for direct marketing from a database. This study expanded RFM model by including two parameters, time since first purchase and churn probability. Using Bernoulli sequence in probability theory, we derive out the formula that can estimate the probability that one customer will buy at the next time, and the expected value of the total number of times that the customer will buy in the future. This study also proposed the methodology to estimate the unknown parameters in the formula. This methodology leads to more efficient and accurate selection procedures than the existing ones. In the empirical part we examine a case study, blood transfusion service, to show that our methodology has greater predictive accuracy than traditional RFM approaches.},
  keywords = {Bernoulli sequence,Knowledge discovery,Marketing,RFM model},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZNVHTMRF/S0957417408004508.html}
}

@article{yildiz_revisiting_2012,
  title = {Re-Visiting the Echo State Property},
  author = {Yildiz, Izzet B. and Jaeger, Herbert and Kiebel, Stefan J.},
  year = {2012},
  month = nov,
  journal = {Neural Networks},
  volume = {35},
  pages = {1--9},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.07.005},
  urldate = {2019-10-28},
  abstract = {An echo state network (ESN) consists of a large, randomly connected neural network, the reservoir, which is driven by an input signal and projects to output units. During training, only the connections from the reservoir to these output units are learned. A key requisite for output-only training is the echo state property (ESP), which means that the effect of initial conditions should vanish as time passes. In this paper, we use analytical examples to show that a widely used criterion for the ESP, the spectral radius of the weight matrix being smaller than unity, is not sufficient to satisfy the echo state property. We obtain these examples by investigating local bifurcation properties of the standard ESNs. Moreover, we provide new sufficient conditions for the echo state property of standard sigmoid and leaky integrator ESNs. We furthermore suggest an improved technical definition of the echo state property, and discuss what practicians should (and should not) observe when they optimize their reservoirs for specific tasks.},
  keywords = {Bifurcation,Diagonally Schur stable,Echo state network,Lyapunov,Spectral radius},
  file = {/Users/antoniohortaribeiro/Zotero/storage/F5TL3B8X/Yildiz et al. - 2012 - Re-visiting the echo state property.pdf;/Users/antoniohortaribeiro/Zotero/storage/HCL4KE69/S0893608012001852.html}
}

@article{yin_fourier_2019,
  title = {A {{Fourier Perspective}} on {{Model Robustness}} in {{Computer Vision}}},
  author = {Yin, Dong and Lopes, Raphael Gontijo and Shlens, Jonathon and Cubuk, Ekin D. and Gilmer, Justin},
  year = {2019},
  month = oct,
  journal = {arXiv:1906.08988 [cs, stat]},
  eprint = {1906.08988},
  primaryclass = {cs, stat},
  urldate = {2020-07-25},
  abstract = {Achieving robustness to distributional shift is a longstanding and challenging goal of computer vision. Data augmentation is a commonly used approach for improving robustness, however robustness gains are typically not uniform across corruption types. Indeed increasing performance in the presence of random noise is often met with reduced performance on other corruptions such as contrast change. Understanding when and why these sorts of trade-offs occur is a crucial step towards mitigating them. Towards this end, we investigate recently observed trade-offs caused by Gaussian data augmentation and adversarial training. We find that both methods improve robustness to corruptions that are concentrated in the high frequency domain while reducing robustness to corruptions that are concentrated in the low frequency domain. This suggests that one way to mitigate these trade-offs via data augmentation is to use a more diverse set of augmentations. Towards this end we observe that AutoAugment, a recently proposed data augmentation policy optimized for clean accuracy, achieves state-of-the-art robustness on the CIFAR-10-C benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/D8Y4RGKU/Yin et al. - 2019 - A Fourier Perspective on Model Robustness in Compu.pdf;/Users/antoniohortaribeiro/Zotero/storage/Q8T2E3N6/1906.html}
}

@inproceedings{yin_rademacher_2019,
  title = {Rademacher {{Complexity}} for {{Adversarially Robust Generalization}}},
  booktitle = {Proceeding of the {{International Conference}} on {{Machine Learning}}},
  author = {Yin, Dong and Kannan, Ramchandran and Bartlett, Peter},
  year = {2019},
  pages = {7085--7094},
  urldate = {2022-04-12},
  abstract = {Many machine learning models are vulnerable to adversarial attacks; for example, adding adversarial perturbations that are imperceptible to humans can often make machine learning models produce wrong predictions with high confidence; moreover, although we may obtain robust models on the training dataset via adversarial training, in some problems the learned models cannot generalize well to the test data. In this paper, we focus on {$\ell\infty\ell\infty\backslash$}ell\_{\textbackslash}infty attacks, and study the adversarially robust generalization problem through the lens of Rademacher complexity. For binary linear classifiers, we prove tight bounds for the adversarial Rademacher complexity, and show that the adversarial Rademacher complexity is never smaller than its natural counterpart, and it has an unavoidable dimension dependence, unless the weight vector has bounded {$\ell$}1{$\ell$}1{\textbackslash}ell\_1 norm, and our results also extend to multi-class linear classifiers; in addition, for (nonlinear) neural networks, we show that the dimension dependence in the adversarial Rademacher complexity also exists. We further consider a surrogate adversarial loss for one-hidden layer ReLU network and prove margin bounds for this setting. Our results indicate that having {$\ell$}1{$\ell$}1{\textbackslash}ell\_1 norm constraints on the weight matrices might be a potential way to improve generalization in the adversarial setting. We demonstrate experimental results that validate our theoretical findings.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/GW33AX7V/Yin et al. - 2019 - Rademacher Complexity for Adversarially Robust Gen.pdf;/Users/antoniohortaribeiro/Zotero/storage/ZTWYLTS5/Yin et al. - 2019 - Rademacher Complexity for Adversarially Robust Gen.pdf}
}

@incollection{yoon_bayesian_2018,
  title = {Bayesian {{Model-Agnostic Meta-Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Yoon, Jaesik and Kim, Taesup and Dia, Ousmane and Kim, Sungwoong and Bengio, Yoshua and Ahn, Sungjin},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {7342--7352},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-12-04},
  file = {/Users/antoniohortaribeiro/Zotero/storage/5Z9LQF2C/yoon_bayesian_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/76MRT3UD/Kim - Bayesian Model-Agnostic Meta-Learning.pdf;/Users/antoniohortaribeiro/Zotero/storage/7VE3869M/7963-bayesian-model-agnostic-meta-learning.html}
}

@article{yoon_penalized_2013,
  title = {Penalized Regression Models with Autoregressive Error Terms},
  author = {Yoon, Young Joo and Park, Cheolwoo and Lee, Taewook},
  year = {2013},
  month = sep,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {83},
  number = {9},
  pages = {1756--1772},
  issn = {0094-9655},
  doi = {10.1080/00949655.2012.669383},
  abstract = {Penalized regression methods have recently gained enormous attention in statistics and the field of machine learning due to their ability of reducing the prediction error and identifying important variables at the same time. Numerous studies have been conducted for penalized regression, but most of them are limited to the case when the data are independently observed. In this paper, we study a variable selection problem in penalized regression models with autoregressive (AR) error terms. We consider three estimators, adaptive least absolute shrinkage and selection operator, bridge, and smoothly clipped absolute deviation, and propose a computational algorithm that enables us to select a relevant set of variables and also the order of AR error terms simultaneously. In addition, we provide their asymptotic properties such as consistency, selection consistency, and asymptotic normality. The performances of the three estimators are compared with one another using simulated and real examples.},
  keywords = {asymptotic normality,autoregressive error models,consistency,oracle property,penalized regression,variable selection},
  annotation = {00022},
  file = {/Users/antoniohortaribeiro/Zotero/storage/N8BVAUW8/yoon_penalized_2013.pdf;/Users/antoniohortaribeiro/Zotero/storage/8CNNNWWI/00949655.2012.html}
}

@incollection{yosinski_how_2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {3320--3328},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-09-01},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZFTH2R7A/Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf;/Users/antoniohortaribeiro/Zotero/storage/6KQ32M78/5347-how-transferable-are-features-in-deep-neural-networks.html}
}

@article{you_drawing_2020,
  title = {Drawing Early-Bird Tickets: {{Towards}} More Efficient Training of Deep Networks},
  shorttitle = {Drawing Early-Bird Tickets},
  author = {You, Haoran and Li, Chaojian and Xu, Pengfei and Fu, Yonggan and Wang, Yue and Chen, Xiaohan and Baraniuk, Richard G. and Wang, Zhangyang and Lin, Yingyan},
  year = {2020},
  month = feb,
  journal = {arXiv:1909.11957 [cs, stat]},
  eprint = {1909.11957},
  primaryclass = {cs, stat},
  urldate = {2020-07-05},
  abstract = {(Frankle \& Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at the very early training stage, which we term as early-bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. Our finding of EB tickets is consistent with recently reported observations that the key connectivity patterns of neural networks emerge early. Furthermore, we propose a mask distance metric that can be used to identify EB tickets with low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, we leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets, and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient training via EB tickets can achieve up to 4.7x energy savings while maintaining comparable or even better accuracy, demonstrating a promising and easily adopted method for tackling cost-prohibitive deep network training. Code available at https://github.com/RICE-EIC/Early-Bird-Tickets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/Q226SMIZ/You et al. - 2020 - Drawing early-bird tickets Towards more efficient.pdf;/Users/antoniohortaribeiro/Zotero/storage/MK5GIQ87/1909.html}
}

@article{young_entropy_,
  title = {Entropy in {{Dynamical Systems}}},
  author = {Young, Lai-Sang},
  pages = {13},
  doi = {10/gfz8cs},
  abstract = {Both h{\textmu} and htop measure the exponential rates of growth of n-orbits: -- h{\textmu} counts the number of typical n-orbits, while -- htop counts all distinguishable n-orbits.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QIVZRUMV/Young - Entropy in Dynamical Systems.pdf}
}

@article{young_instrumental_1970,
  title = {An Instrumental Variable Method for Real-Time Identification of a Noisy Process},
  author = {Young, Peter C},
  year = {1970},
  journal = {Automatica},
  volume = {6},
  number = {2},
  pages = {271--287},
  doi = {10.1016/0005-1098(70)90098-1},
  annotation = {00477}
}

@book{yu_kernelbased_2013,
  title = {Kernel-Based Data Fusion for Machine Learning},
  author = {Yu, Shi and Tranchevent, L{\'e}on-Charles and De Moor, Bart and Moreau, Yves},
  year = {2013},
  publisher = {Springer},
  annotation = {00045}
}

@article{yuan_adversarial_2019,
  title = {Adversarial Examples: {{Attacks}} and Defenses for Deep Learning},
  author = {Yuan, Xiaoyong and He, Pan and Zhu, Qile and Li, Xiaolin},
  year = {2019},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {30},
  number = {9},
  pages = {2805--2824},
  issn = {2162-237X}
}

@article{yuan_model_2006,
  title = {Model {{Selection}} and {{Estimation}} in {{Regression}} with {{Grouped Variables}}},
  author = {Yuan, Ming and Lin, Yi},
  year = {2006},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {68},
  number = {1},
  eprint = {3647556},
  eprinttype = {jstor},
  pages = {49--67},
  issn = {1369-7412},
  doi = {10.1111/j.1467-9868.2005.00532.x},
  abstract = {We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multi-factor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
  annotation = {04899},
  file = {/Users/antoniohortaribeiro/Zotero/storage/67JD7JQP/yuan_model_2006.pdf}
}

@inproceedings{yuan_review_2000,
  title = {A Review of Trust Region Algorithms for Optimization},
  booktitle = {{{ICIAM}}},
  author = {Yuan, Ya-xiang},
  year = {2000},
  volume = {99},
  pages = {271--282},
  urldate = {2017-09-11},
  annotation = {00182},
  file = {/Users/antoniohortaribeiro/Zotero/storage/4B7ZQ2M7/yuan_a review_2000.pdf}
}

@inproceedings{yuwang_new_2017,
  title = {A New Concept Using {{LSTM Neural Networks}} for Dynamic System Identification},
  booktitle = {2017 {{American Control Conference}} ({{ACC}})},
  author = {{Yu Wang}},
  year = {2017-05-24/2017-05-26},
  pages = {5324--5329},
  doi = {10.23919/ACC.2017.7963782},
  isbn = {2378-5861}
}

@inproceedings{zabihi_detection_2017,
  title = {{Detection of atrial fibrillation in ECG hand-held devices using a random forest classifier}},
  booktitle = {{Computing in Cardiology}},
  author = {Zabihi, Morteza and Rad, Ali Bahrami and Katsaggelos, Aggelos K. and Kiranyaz, Serkan and Narkilahti, Susanna and Gabbouj, Moncef},
  year = {2017},
  month = jan,
  volume = {44},
  pages = {1--4},
  publisher = {IEEE Computer Society},
  doi = {10/gf2597},
  urldate = {2019-05-27},
  langid = {English (US)},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SENJHHFF/zabihi_detection_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/WCFRCUUF/detection-of-atrial-fibrillation-in-ecg-hand-held-devices-using-a.html}
}

@book{zaki_data_2014,
  title = {Data Mining and Analysis: Fundamental Concepts and Algorithms},
  shorttitle = {Data Mining and Analysis},
  author = {Zaki, Mohammed J. and Meira, Wagner},
  year = {2014},
  publisher = {Cambridge University Press},
  address = {New York, NY},
  isbn = {978-0-521-76633-3},
  lccn = {QA76.9.D343 Z36 2014},
  keywords = {Data mining},
  file = {/Users/antoniohortaribeiro/Zotero/storage/835KDU4R/zaki_data_2014.pdf}
}

@article{zech_variable_2018,
  title = {Variable Generalization Performance of a Deep Learning Model to Detect Pneumonia in Chest Radiographs: {{A}} Cross-Sectional Study},
  shorttitle = {Variable Generalization Performance of a Deep Learning Model to Detect Pneumonia in Chest Radiographs},
  author = {Zech, John R. and Badgeley, Marcus A. and Liu, Manway and Costa, Anthony B. and Titano, Joseph J. and Oermann, Eric Karl},
  year = {2018},
  month = jun,
  journal = {PLOS Medicine},
  volume = {15},
  number = {11},
  pages = {e1002683},
  issn = {1549-1676},
  doi = {10/gfj53h},
  urldate = {2018-11-19},
  abstract = {Background There is interest in using convolutional neural networks (CNNs) to analyze medical imaging to provide computer-aided diagnosis (CAD). Recent work has suggested that image classification CNNs may not generalize to new data as well as previously believed. We assessed how well CNNs generalized across three hospital systems for a simulated pneumonia screening task. Methods and findings A cross-sectional design with multiple model training cohorts was used to evaluate model generalizability to external sites using split-sample validation. A total of 158,323 chest radiographs were drawn from three institutions: National Institutes of Health Clinical Center (NIH; 112,120 from 30,805 patients), Mount Sinai Hospital (MSH; 42,396 from 12,904 patients), and Indiana University Network for Patient Care (IU; 3,807 from 3,683 patients). These patient populations had an age mean (SD) of 46.9 years (16.6), 63.2 years (16.5), and 49.6 years (17) with a female percentage of 43.5\%, 44.8\%, and 57.3\%, respectively. We assessed individual models using the area under the receiver operating characteristic curve (AUC) for radiographic findings consistent with pneumonia and compared performance on different test sets with DeLong's test. The prevalence of pneumonia was high enough at MSH (34.2\%) relative to NIH and IU (1.2\% and 1.0\%) that merely sorting by hospital system achieved an AUC of 0.861 (95\% CI 0.855--0.866) on the joint MSH--NIH dataset. Models trained on data from either NIH or MSH had equivalent performance on IU (P values 0.580 and 0.273, respectively) and inferior performance on data from each other relative to an internal test set (i.e., new data from within the hospital system used for training data; P values both {$<$}0.001). The highest internal performance was achieved by combining training and test data from MSH and NIH (AUC 0.931, 95\% CI 0.927--0.936), but this model demonstrated significantly lower external performance at IU (AUC 0.815, 95\% CI 0.745--0.885, P = 0.001). To test the effect of pooling data from sites with disparate pneumonia prevalence, we used stratified subsampling to generate MSH--NIH cohorts that only differed in disease prevalence between training data sites. When both training data sites had the same pneumonia prevalence, the model performed consistently on external IU data (P = 0.88). When a 10-fold difference in pneumonia rate was introduced between sites, internal test performance improved compared to the balanced model (10{\texttimes} MSH risk P {$<$} 0.001; 10{\texttimes} NIH P = 0.002), but this outperformance failed to generalize to IU (MSH 10{\texttimes} P {$<$} 0.001; NIH 10{\texttimes} P = 0.027). CNNs were able to directly detect hospital system of a radiograph for 99.95\% NIH (22,050/22,062) and 99.98\% MSH (8,386/8,388) radiographs. The primary limitation of our approach and the available public data is that we cannot fully assess what other factors might be contributing to hospital system--specific biases. Conclusion Pneumonia-screening CNNs achieved better internal than external performance in 3 out of 5 natural comparisons. When models were trained on pooled data from sites with different pneumonia prevalence, they performed better on new pooled data from these sites but not on external data. CNNs robustly identified hospital system and department within a hospital, which can have large differences in disease burden and may confound predictions.},
  langid = {english},
  keywords = {Critical care and emergency medicine,Deep learning,Indiana,Inpatients,Natural language processing,Nosocomial infections,Pneumonia,Radiology and imaging},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PDF5Z5DX/zech_variable_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/5RJYXHWG/article.html}
}

@inproceedings{zeiler_rectified_2013,
  title = {On Rectified Linear Units for Speech Processing},
  booktitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Zeiler, M. D. and Ranzato, M. and Monga, R. and Mao, M. and Yang, K. and {Q. V. Le} and {P. Nguyen} and {A. Senior} and {V. Vanhoucke} and {J. Dean} and {G. E. Hinton}},
  year = {2013-05-26/2013-05-31},
  pages = {3517--3521},
  doi = {10/gfv3hc},
  isbn = {1520-6149},
  keywords = {Accuracy,acoustic modeling,Deep Learning,deep neural networks,discriminative tasks,distributed environment,Encoding,Error analysis,gold standard,Hybrid System,key computational unit,large vocabulary speech recognition task,linear projection,logistic function,logistic units,Logistics,neural nets,Neural networks,Neural Networks,point-wise nonlinearity,rectified linear units,Rectified Linear units,sparse features,speech processing,speech recognition,supervised setting,Training,Unsupervised learning,Unsupervised Learning,vocabulary,word error rates}
}

@article{zeiler_visualizing_2013,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  year = {2013},
  month = nov,
  journal = {arXiv:1311.2901 [cs]},
  eprint = {1311.2901},
  primaryclass = {cs},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/antoniohortaribeiro/Zotero/storage/XGF9X26K/zeiler_visualizin_2013.pdf;/Users/antoniohortaribeiro/Zotero/storage/XPSIWZC8/1311.html}
}

@article{zeiler_visualizing_2013a,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  year = {2013},
  month = nov,
  journal = {arXiv:1311.2901 [cs]},
  eprint = {1311.2901},
  primaryclass = {cs},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {04774},
  file = {/Users/antoniohortaribeiro/Zotero/storage/QRSKI24B/zeiler_visualizin_2013.pdf;/Users/antoniohortaribeiro/Zotero/storage/M7KX7DKG/1311.html}
}

@article{zeiler_visualizing_2013b,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  year = {2013},
  month = nov,
  journal = {arXiv:1311.2901 [cs]},
  eprint = {1311.2901},
  primaryclass = {cs},
  urldate = {2020-05-06},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/antoniohortaribeiro/Zotero/storage/C35QYR9Y/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf;/Users/antoniohortaribeiro/Zotero/storage/6KJFAS3Y/1311.html}
}

@inproceedings{zeiler_visualizing_2014,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  year = {2014},
  month = sep,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {818--833},
  publisher = {Springer, Cham},
  doi = {10.1007/978-3-319-10590-1_53},
  urldate = {2018-01-27},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  isbn = {978-3-319-10589-5 978-3-319-10590-1},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {04774},
  file = {/Users/antoniohortaribeiro/Zotero/storage/3GQ23AKM/zeiler_visualizin_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/BPBCASIB/1311.html;/Users/antoniohortaribeiro/Zotero/storage/RT75B6AG/978-3-319-10590-1_53.html}
}

@article{zhang_association_2023,
  title = {Association of Lifestyle with Deep-Learning Based {{ECG-age}}},
  author = {Zhang, Cuili and Miao, Xiao and Wang, Biqi and Ribeiro, Ant{\^o}nio H and Brant, Luisa and Ribeiro, Antonio L P and Lin, Honghuang},
  year = {2023},
  journal = {Frontiers in Cardiovascular Medicine},
  volume = {10},
  doi = {10.3389/fcvm.2023.1160091},
  copyright = {All rights reserved}
}

@inproceedings{zhang_causal_2020,
  title = {A {{Causal View}} on {{Robustness}} of {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Cheng and Zhang, Kun and Li, Yingzhen},
  year = {2020},
  volume = {33},
  pages = {289--301},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-04-14},
  abstract = {We present a causal view on the robustness of neural networks against input manipulations, which applies not only to traditional classification tasks but also to general measurement data. Based on this view, we design a deep causal manipulation augmented model (deep CAMA) which explicitly models possible manipulations on certain causes leading to changes in the observed effect.  We further develop data augmentation and test-time fine-tuning methods to improve deep CAMA's robustness. When compared with discriminative deep neural networks, our proposed model shows superior robustness against unseen manipulations. As a by-product, our model achieves disentangled representation which separates the representation of manipulations from those of other latent causes.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/IS36BQZG/Zhang et al. - 2020 - A Causal View on Robustness of Neural Networks.pdf}
}

@article{zhang_depth_2020,
  title = {Depth Creates No More Spurious Local Minima},
  author = {Zhang, Li},
  year = {2020},
  month = jan,
  journal = {arXiv:1901.09827 [cs, stat]},
  eprint = {1901.09827},
  primaryclass = {cs, stat},
  urldate = {2020-08-27},
  abstract = {We show that for any convex differentiable loss, a deep linear network has no spurious local minima as long as it is true for the two layer case. This reduction greatly simplifies the study on the existence of spurious local minima in deep linear networks. When applied to the quadratic loss, our result immediately implies the powerful result in [Kawaguchi 2016]. Further, with the work in [Zhou and Liang 2018], we can remove all the assumptions in [Kawaguchi 2016]. This property holds for more general "multi-tower" linear networks too. Our proof builds on [Laurent and von Brecht 2018] and develops a new perturbation argument to show that any spurious local minimum must have full rank, a structural property which can be useful more generally.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/7YVSGL4C/1405.4980.pdf;/Users/antoniohortaribeiro/Zotero/storage/JJXXZB2P/Zhang - 2020 - Depth creates no more spurious local minima.pdf;/Users/antoniohortaribeiro/Zotero/storage/DTAMCTUU/1901.html}
}

@article{zhang_flexible_2000,
  title = {A Flexible New Technique for Camera Calibration},
  author = {Zhang, Zhengyou},
  year = {2000},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  volume = {22},
  number = {11},
  pages = {1330--1334},
  doi = {10.1109/34.888718},
  annotation = {11804}
}

@article{zhang_forward_2015,
  title = {Forward and Backward Least Angle Regression for Nonlinear System Identification},
  author = {Zhang, Long and Li, Kang},
  year = {2015},
  month = mar,
  journal = {Automatica},
  volume = {53},
  pages = {94--102},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2014.12.010},
  abstract = {A forward and backward least angle regression (LAR) algorithm is proposed to construct the nonlinear autoregressive model with exogenous inputs (NARX) that is widely used to describe a large class of nonlinear dynamic systems. The main objective of this paper is to improve model sparsity and generalization performance of the original forward LAR algorithm. This is achieved by introducing a replacement scheme using an additional backward LAR stage. The backward stage replaces insignificant model terms selected by forward LAR with more significant ones, leading to an improved model in terms of the model compactness and performance. A numerical example to construct four types of NARX models, namely polynomials, radial basis function (RBF) networks, neuro fuzzy and wavelet networks, is presented to illustrate the effectiveness of the proposed technique in comparison with some popular methods.},
  keywords = {Backward refinement,Forward selection,Least angle regression (LAR),Nonlinear system identification},
  annotation = {00013},
  file = {/Users/antoniohortaribeiro/Zotero/storage/FMQXAVDX/zhang_forward_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/QMJSPBUB/zhang_forward_2015.pdf;/Users/antoniohortaribeiro/Zotero/storage/JIZ5IXAG/S0005109814005895.html;/Users/antoniohortaribeiro/Zotero/storage/QMT3V6U6/S0005109814005895.html;/Users/antoniohortaribeiro/Zotero/storage/S7C8HR9K/S0005109814005895.html;/Users/antoniohortaribeiro/Zotero/storage/ZIDPPRDV/S0005109814005895.html}
}

@article{zhang_generalized_2018,
  title = {Generalized {{Cross Entropy Loss}} for {{Training Deep Neural Networks}} with {{Noisy Labels}}},
  author = {Zhang, Zhilu and Sabuncu, Mert R.},
  year = {2018},
  month = may,
  journal = {arXiv:1805.07836 [cs, stat]},
  eprint = {1805.07836},
  primaryclass = {cs, stat},
  urldate = {2018-12-06},
  abstract = {Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/874DSUXK/zhang_generalize_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/YUHJDQ4Q/1805.html}
}

@article{zhang_identification_2017,
  title = {Identification of Multivariable Dynamic Errors-in-Variables System with Arbitrary Inputs},
  author = {Zhang, Erliang and Pintelon, Rik},
  year = {2017},
  month = aug,
  journal = {Automatica},
  volume = {82},
  pages = {69--78},
  issn = {00051098},
  doi = {10.1016/j.automatica.2017.04.031},
  urldate = {2017-08-23},
  langid = {english},
  annotation = {00006},
  file = {/Users/antoniohortaribeiro/Zotero/storage/X2ZK3A6U/zhang_identifica_2017.pdf}
}

@inproceedings{zhang_making_2019,
  title = {Making {{Convolutional Networks Shift-Invariant Again}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Zhang, Richard},
  year = {2019},
  month = jun,
  eprint = {1904.11486},
  urldate = {2020-03-23},
  abstract = {Modern convolutional networks are not shift-invariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and average-pooling, ignore the sampling theorem. The well-known signal processing fix is anti-aliasing by low-pass filtering before downsampling. However, simply inserting this module into deep networks degrades performance; as a result, it is seldomly used today. We show that when integrated correctly, it is compatible with existing architectural components, such as max-pooling and strided-convolution. We observe {\textbackslash}textit\{increased accuracy\} in ImageNet classification, across several commonly-used architectures, such as ResNet, DenseNet, and MobileNet, indicating effective regularization. Furthermore, we observe {\textbackslash}textit\{better generalization\}, in terms of stability and robustness to input corruptions. Our results demonstrate that this classical signal processing technique has been undeservingly overlooked in modern deep networks. Code and anti-aliased versions of popular networks are available at https://richzhang.github.io/antialiased-cnns/ .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/MTZWNWFT/Zhang - 2019 - Making Convolutional Networks Shift-Invariant Agai.pdf;/Users/antoniohortaribeiro/Zotero/storage/WS5G4X79/1904.html}
}

@article{zhang_modeling_2006,
  title = {Modeling of Temperature-Humidity for Wood Drying Based on Time-Delay Neural Network},
  author = {Zhang, Dong-yan and Sun, Li-ping and Cao, Jun},
  year = {2006},
  journal = {Journal of Forestry Research},
  volume = {17},
  number = {2},
  pages = {141--144},
  doi = {10.1007/s11676-006-0033-1},
  annotation = {00028}
}

@inproceedings{zhang_new_2014,
  title = {A New Battery Modelling Method Based on Simulation Error Minimization},
  booktitle = {2014 {{IEEE PES General Meeting}}},
  author = {Zhang, Cheng and Li, Kang and Yang, Zhile and Pei, Lei and Zhu, Chunbo},
  year = {2014}
}

@article{zhang_stabilizing_2018,
  title = {Stabilizing {{Gradients}} for {{Deep Neural Networks}} via {{Efficient SVD Parameterization}}},
  author = {Zhang, Jiong and Lei, Qi and Dhillon, Inderjit S},
  year = {2018},
  journal = {Proceedings of the 35 th International Conference on Machine Learning},
  pages = {9},
  abstract = {Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed Spectral-RNN method allows us to provably solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it controls generalization of RNN for the classification task. Our extensive experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially in capturing long range dependencies, as shown on the synthetic addition and copy tasks, as well as on the MNIST and Penn Tree Bank data sets.},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/478PGUNR/Zhang et al. - Stabilizing Gradients for Deep Neural Networks via.pdf}
}

@article{zhang_trust_2010,
  title = {A Trust Region Method in Adaptive Finite Element Framework for Bioluminescence Tomography},
  author = {Zhang, Bo and Yang, Xin and Qin, Chenghu and Liu, Dan and Zhu, Shouping and Feng, Jinchao and Sun, Li and Liu, Kai and Han, Dong and Ma, Xibo and others},
  year = {2010},
  journal = {Optics express},
  volume = {18},
  number = {7},
  pages = {6477--6491},
  doi = {10.1364/OE.18.006477},
  annotation = {00029}
}

@inproceedings{zhang_understanding_2017,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2017},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/DSMSXG33/zhang_understand_2016.pdf;/Users/antoniohortaribeiro/Zotero/storage/XSN6KBTC/1611.html}
}

@article{zhangjeffrey_fully_2018,
  title = {Fully {{Automated Echocardiogram Interpretation}} in {{Clinical Practice}}},
  author = {{Zhang Jeffrey} and {Gajjala Sravani} and {Agrawal Pulkit} and {Tison Geoffrey H.} and {Hallock Laura A.} and {Beussink-Nelson Lauren} and {Lassen Mats H.} and {Fan Eugene} and {Aras Mandar A.} and {Jordan ChaRandle} and {Fleischmann Kirsten E.} and {Melisko Michelle} and {Qasim Atif} and {Shah Sanjiv J.} and {Bajcsy Ruzena} and {Deo Rahul C.}},
  year = {2018},
  month = oct,
  journal = {Circulation},
  volume = {138},
  number = {16},
  pages = {1623--1635},
  doi = {10/gfg9fn},
  urldate = {2018-11-27},
  abstract = {Background:Automated cardiac image interpretation has the potential to transform clinical practice in multiple ways, including enabling serial assessment of cardiac function by nonexperts in primary care and rural settings. We hypothesized that advances in computer vision could enable building a fully automated, scalable analysis pipeline for echocardiogram interpretation, including (1) view identification, (2) image segmentation, (3) quantification of structure and function, and (4) disease detection.Methods:Using 14\,035 echocardiograms spanning a 10-year period, we trained and evaluated convolutional neural network models for multiple tasks, including automated identification of 23 viewpoints and segmentation of cardiac chambers across 5 common views. The segmentation output was used to quantify chamber volumes and left ventricular mass, determine ejection fraction, and facilitate automated determination of longitudinal strain through speckle tracking. Results were evaluated through comparison to manual segmentation and measurements from 8666 echocardiograms obtained during the routine clinical workflow. Finally, we developed models to detect 3 diseases: hypertrophic cardiomyopathy, cardiac amyloid, and pulmonary arterial hypertension.Results:Convolutional neural networks accurately identified views (eg, 96\% for parasternal long axis), including flagging partially obscured cardiac chambers, and enabled the segmentation of individual cardiac chambers. The resulting cardiac structure measurements agreed with study report values (eg, median absolute deviations of 15\% to 17\% of observed values for left ventricular mass, left ventricular diastolic volume, and left atrial volume). In terms of function, we computed automated ejection fraction and longitudinal strain measurements (within 2 cohorts), which agreed with commercial software-derived values (for ejection fraction, median absolute deviation=9.7\% of observed, N=6407 studies; for strain, median absolute deviation=7.5\%, n=419, and 9.0\%, n=110) and demonstrated applicability to serial monitoring of patients with breast cancer for trastuzumab cardiotoxicity. Overall, we found automated measurements to be comparable or superior to manual measurements across 11 internal consistency metrics (eg, the correlation of left atrial and ventricular volumes). Finally, we trained convolutional neural networks to detect hypertrophic cardiomyopathy, cardiac amyloidosis, and pulmonary arterial hypertension with C statistics of 0.93, 0.87, and 0.85, respectively.Conclusions:Our pipeline lays the groundwork for using automated interpretation to support serial patient tracking and scalable analysis of millions of echocardiograms archived within healthcare systems.},
  file = {/Users/antoniohortaribeiro/Zotero/storage/E87WKJ53/zhang jeffrey_fully_2018.pdf;/Users/antoniohortaribeiro/Zotero/storage/FWNCU24Q/CIRCULATIONAHA.118.html}
}

@article{zhao_automated_2017,
  title = {Automated {{Model Construction}} for {{Combined Sewer Overflow Prediction Based}} on {{Efficient LASSO Algorithm}}},
  author = {Zhao, W. and Beach, T. H. and Rezgui, Y.},
  year = {2017},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {PP},
  number = {99},
  pages = {1--16},
  issn = {2168-2216},
  doi = {10.1109/TSMC.2017.2724440},
  abstract = {The prediction of combined sewer overflow (CSO) operation in urban environments presents a challenging task for water utilities. The operation of CSOs (most often in heavy rainfall conditions) prevents houses and businesses from flooding. However, sometimes, CSOs do not operate as they should, potentially bringing environmental pollution risks. Therefore, CSOs should be appropriately managed by water utilities, highlighting the need for adapted decision support systems. This paper proposes an automated CSO predictive model construction methodology using field monitoring data, as a substitute for the commonly established hydrological-hydraulic modeling approach for time-series prediction of CSO statuses. It is a systematic methodology factoring in all monitored field variables to construct time-series dependencies for CSO statuses. The model construction process is largely automated with little human intervention, and the pertinent variables together with their associated time lags for every CSO are holistically and automatically generated. A fast least absolute shrinkage and selection operator solution generating scheme is proposed to expedite the model construction process, where matrix inversions are effectively eliminated. The whole algorithm works in a stepwise manner, invoking either an incremental or decremental movement for including or excluding one model regressor into, or from, the predictive model at every step. The computational complexity is thereby analyzed with the pseudo code provided. Actual experimental results from both single-step ahead (i.e., 15 min) and multistep ahead predictions are finally produced and analyzed on a U.K. pilot area with various types of monitoring data made available, demonstrating the efficiency and effectiveness of the proposed approach.},
  keywords = {Adaptation models,Analytical models,Combined sewer overflows (CSOs),Computational modeling,Data models,efficient model construction,hydraulics,Mathematical model,Monitoring,prediction,Predictive models,wastewater},
  annotation = {00003},
  file = {/Users/antoniohortaribeiro/Zotero/storage/CTZ3FZMN/zhao_automated_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/NEW9KR9Q/zhao_automated_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/JXTIDU4E/8013727.html;/Users/antoniohortaribeiro/Zotero/storage/RWXHEP7Y/8013727.html;/Users/antoniohortaribeiro/Zotero/storage/S2W2V5LM/8013727.html;/Users/antoniohortaribeiro/Zotero/storage/SUUUBW5R/8013727.html}
}

@article{zhao_identification_2014,
  title = {Identification of K-Step-Ahead Prediction Error Model and {{MPC}} Control},
  author = {Zhao, Jun and Zhu, Yucai and Patwardhan, Rohit},
  year = {2014},
  month = jan,
  journal = {Journal of Process Control},
  volume = {24},
  number = {1},
  pages = {48--56},
  issn = {0959-1524},
  doi = {10/f5v69w},
  urldate = {2019-04-04},
  abstract = {This work studies k-step-ahead prediction error model identification and its relationship to MPC control. The use of error criteria in parameter estimation will be discussed, where the identified model is used in model predictive control (MPC). Assume that the model error is dominated by the variance part, it can be shown that a k-step-ahead prediction error model is not optimal for k-step-ahead prediction. A normal one-step-ahead prediction error criterion will be optimal for k-step-ahead prediction. Then it is argued that even when some bias exists, the result could still hold true. Therefore, for MPC identification of linear processes, one-step-ahead prediction error models fever k-step-ahead prediction models. Simulations and industrial testing data will be used to illustrate the idea.},
  keywords = {Error criteria,Identification,Industrial application,MPC},
  file = {/Users/antoniohortaribeiro/Zotero/storage/T89LXRT6/zhao_identifica_2014.pdf;/Users/antoniohortaribeiro/Zotero/storage/FJGUYGF5/S0959152413002187.html}
}

@article{zheng_state_2017,
  title = {State {{Space LSTM Models}} with {{Particle MCMC Inference}}},
  author = {Zheng, Xun and Zaheer, Manzil and Ahmed, Amr and Wang, Yuan and Xing, Eric P. and Smola, Alexander J.},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.11179 [cs, stat]},
  eprint = {1711.11179},
  primaryclass = {cs, stat},
  urldate = {2018-11-26},
  abstract = {Long Short-Term Memory (LSTM) is one of the most powerful sequence models. Despite the strong performance, however, it lacks the nice interpretability as in state space models. In this paper, we present a way to combine the best of both worlds by introducing State Space LSTM (SSL) models that generalizes the earlier work {\textbackslash}cite\{zaheer2017latent\} of combining topic models with LSTM. However, unlike {\textbackslash}cite\{zaheer2017latent\}, we do not make any factorization assumptions in our inference algorithm. We present an efficient sampler based on sequential Monte Carlo (SMC) method that draws from the joint posterior directly. Experimental results confirms the superiority and stability of this SMC inference algorithm on a variety of domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/SXML9CVT/zheng_state_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/AM8M3E46/1711.html}
}

@article{zhou_deconstructing_2020,
  title = {Deconstructing {{Lottery Tickets}}: {{Zeros}}, {{Signs}}, and the {{Supermask}}},
  shorttitle = {Deconstructing {{Lottery Tickets}}},
  author = {Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
  year = {2020},
  month = mar,
  journal = {arXiv:1905.01067 [cs, stat]},
  eprint = {1905.01067},
  primaryclass = {cs, stat},
  urldate = {2020-06-29},
  abstract = {The recent "Lottery Ticket Hypothesis" paper by Frankle \& Carbin showed that a simple approach to creating sparse networks (keeping the large weights) results in models that are trainable from scratch, but only when starting from the same initial weights. The performance of these networks often exceeds the performance of the non-sparse base model, but for reasons that were not well understood. In this paper we study the three critical components of the Lottery Ticket (LT) algorithm, showing that each may be varied significantly without impacting the overall results. Ablating these factors leads to new insights for why LT networks perform as well as they do. We show why setting weights to zero is important, how signs are all you need to make the reinitialized network train, and why masking behaves like training. Finally, we discover the existence of Supermasks, masks that can be applied to an untrained, randomly initialized network to produce a model with performance far better than chance (86\% on MNIST, 41\% on CIFAR-10).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NLRTHN6S/Zhou et al. - 2020 - Deconstructing Lottery Tickets Zeros, Signs, and .pdf;/Users/antoniohortaribeiro/Zotero/storage/VGWXT9AF/1905.html}
}

@book{zhou_essentials_1998,
  title = {Essentials of Robust Control},
  author = {Zhou, Kemin and Doyle, John Comstock},
  year = {1998},
  volume = {104},
  publisher = {Prentice Hall}
}

@article{zhou_state_2017,
  title = {State Estimation of a Compound Non-Smooth Sandwich System with Backlash and Dead Zone},
  author = {Zhou, Zupeng and Tan, Yonghong and Xie, Yangqiu and Dong, Ruili},
  year = {2017},
  journal = {Mechanical Systems and Signal Processing},
  volume = {83},
  pages = {439--449},
  issn = {0888-3270},
  doi = {10.1016/j.ymssp.2016.06.023},
  annotation = {00007}
}

@misc{zhou_uniform_2021,
  title = {On {{Uniform Convergence}} and {{Low-Norm Interpolation Learning}}},
  author = {Zhou, Lijia and Sutherland, Danica J. and Srebro, Nathan},
  year = {2021},
  month = jan,
  number = {arXiv:2006.05942},
  eprint = {2006.05942},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-12-06},
  abstract = {We consider an underdetermined noisy linear regression model where the minimum-norm interpolating predictor is known to be consistent, and ask: can uniform convergence in a norm ball, or at least (following Nagarajan and Kolter) the subset of a norm ball that the algorithm selects on a typical input set, explain this success? We show that uniformly bounding the difference between empirical and population errors cannot show any learning in the norm ball, and cannot show consistency for any set, even one depending on the exact algorithm and distribution. But we argue we can explain the consistency of the minimal-norm interpolator with a slightly weaker, yet standard, notion: uniform convergence of zero-error predictors in a norm ball. We use this to bound the generalization error of low- (but not minimal-) norm interpolating predictors.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NDBIBHTD/Zhou et al. - 2021 - On Uniform Convergence and Low-Norm Interpolation .pdf}
}

@article{zoph_learning_2017,
  title = {Learning {{Transferable Architectures}} for {{Scalable Image Recognition}}},
  author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
  year = {2017},
  month = jul,
  journal = {arXiv:1707.07012 [cs, stat]},
  eprint = {1707.07012},
  primaryclass = {cs, stat},
  abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4\% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
  archiveprefix = {arXiv},
  keywords = {ðŸ”No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning},
  annotation = {00171},
  file = {/Users/antoniohortaribeiro/Zotero/storage/ZIC377N3/zoph_learning_2017.pdf;/Users/antoniohortaribeiro/Zotero/storage/A3W67TPU/1707.html}
}

@inproceedings{zou_delving_2020,
  title = {Delving {{Deeper}} into {{Anti-aliasing}} in {{ConvNets}}},
  booktitle = {Proceedings of the 31st {{British Machine Vision Virtual Conference}} ({{BMVC}})},
  author = {Zou, Xueyan},
  year = {2020},
  langid = {english},
  file = {/Users/antoniohortaribeiro/Zotero/storage/PWQBWPZ8/Zou - Delving Deeper into Anti-aliasing in ConvNets.pdf}
}

@article{zou_regularization_2005,
  title = {Regularization and {{Variable Selection}} via the {{Elastic Net}}},
  author = {Zou, Hui and Hastie, Trevor},
  year = {2005},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {67},
  number = {2},
  eprint = {3647580},
  eprinttype = {jstor},
  pages = {301--320},
  issn = {1369-7412},
  doi = {10.1111/j.1467-9868.2005.00503.x},
  abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p {$\gg$} n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
  annotation = {08250},
  file = {/Users/antoniohortaribeiro/Zotero/storage/NRIBGQ52/zou_regulariza_2005.pdf}
}

@article{zou_stochastic_2018,
  title = {Stochastic {{Gradient Descent Optimizes Over-parameterized Deep ReLU Networks}}},
  author = {Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  year = {2018},
  month = dec,
  journal = {arXiv:1811.08888 [cs, math, stat]},
  eprint = {1811.08888},
  primaryclass = {cs, math, stat},
  urldate = {2020-08-10},
  abstract = {We study the problem of training deep neural networks with Rectified Linear Unit (ReLU) activation function using gradient descent and stochastic gradient descent. In particular, we study the binary classification problem and show that for a broad family of loss functions, with proper random weight initialization, both gradient descent and stochastic gradient descent can find the global minima of the training loss for an over-parameterized deep ReLU network, under mild assumption on the training data. The key idea of our proof is that Gaussian random initialization followed by (stochastic) gradient descent produces a sequence of iterates that stay inside a small perturbation region centering around the initial weights, in which the empirical loss function of deep ReLU networks enjoys nice local curvature properties that ensure the global convergence of (stochastic) gradient descent. Our theoretical results shed light on understanding the optimization for deep learning, and pave the way for studying the optimization dynamics of training modern deep neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/antoniohortaribeiro/Zotero/storage/AVY94BMX/Zou et al. - 2018 - Stochastic Gradient Descent Optimizes Over-paramet.pdf;/Users/antoniohortaribeiro/Zotero/storage/YGD4FNNY/1811.html}
}

@book{zurawski_industrial_2005,
  title = {The Industrial Communication Technology Handbook},
  editor = {Zurawski, Richard},
  year = {2005},
  series = {The Industrial Information Technology Series},
  number = {1},
  publisher = {Taylor \& Francis},
  address = {Boca Raton, Fla},
  isbn = {978-0-8493-3077-3},
  lccn = {TK5105.5 .I48 2005},
  keywords = {Computer networks,Data transmission systems,Wireless communication systems},
  file = {/Users/antoniohortaribeiro/Zotero/storage/BIXUCAMM/zurawski_the_2005.pdf}
}

@article{zvuloni_merging_2023,
  title = {On {{Merging Feature Engineering}} and {{Deep Learning}} for {{Diagnosis}}, {{Risk-Prediction}} and {{Age Estimation Based}} on the 12-{{Lead ECG}}},
  author = {Zvuloni, Eran and Read, Jesse and Ribeiro, Ant{\^o}nio H. and Ribeiro, Antonio Luiz P. and Behar, Joachim A.},
  year = {2023},
  month = jan,
  journal = {IEEE Transactions on Biomedical Engineering},
  eprint = {2207.06096},
  primaryclass = {cs, eess},
  doi = {10.1109/TBME.2023.3239527},
  urldate = {2022-07-22},
  abstract = {Objective: Over the past few years, deep learning (DL) has been used extensively in research for 12-lead electrocardiogram (ECG) analysis. However, it is unclear whether the explicit or implicit claims made on DL superiority to the more classical feature engineering (FE) approaches, based on domain knowledge, hold. In addition, it remains unclear whether combining DL with FE may improve performance over a single modality. Methods: To address these research gaps and in-line with recent major experiments, we revisited three tasks: cardiac arrhythmia diagnosis (multiclass-multilabel classification), atrial fibrillation risk prediction (binary classification), and age estimation (regression). We used an overall dataset of 2.3M 12-lead ECG recordings to train the following models for each task: i) a random forest taking FE as input; ii) an end-to-end DL model; and iii) a merged model of FE+DL. Results: FE yielded comparable results to DL while necessitating significantly less data for the two classification tasks. DL outperformed FE for the regression task. For all tasks, merging FE with DL did not improve performance over DL alone. These findings were confirmed on the additional PTB-XL dataset. Conclusion: We found that for traditional 12-lead ECG based diagnosis tasks, DL did not yield a meaningful improvement over FE, while it improved significantly the nontraditional regression task. We also found that combining FE with DL did not improve over DL alone, which suggests that the FE were redundant with the features learned by DL. Significance: Our findings provides important recommendations on 12-lead ECG based machine learning strategy and data regime to choose for a given task. When looking at maximizing performance as the end goal, if the task is nontraditional and a large dataset is available then DL is preferable. If the task is a classical one and/or a small dataset is available then a FE approach may be the better choice.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,I.2.0,J.3},
  file = {/Users/antoniohortaribeiro/Zotero/storage/VVZCGQJP/Zvuloni et al. - 2022 - On Merging Feature Engineering and Deep Learning f.pdf}
}
